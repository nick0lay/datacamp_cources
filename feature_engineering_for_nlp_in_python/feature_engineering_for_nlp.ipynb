{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering for NLP in Python\n",
    "## 1. Basic features and readability scores\n",
    "Learn to compute basic features such as number of words, number of characters, average word length and number of special characters (such as Twitter hashtags and mentions). You will also learn to compute readability scores and determine the amount of education required to comprehend a piece of text.\n",
    "\n",
    "### Introduction to NLP feature engineering\n",
    "Feature shoul be numerical\n",
    "Categorical feature \"One-hot encoding\"\n",
    "\n",
    "| Sex    | one-hot encoding | sex_female | sex_male |\n",
    "|--------|------------------|------------|----------|\n",
    "| female | ->               | 1          | 0        |\n",
    "| male   | ->               | 0          | 1        |\n",
    "| female | ->               | 1          | 0        |\n",
    "| ...    | ...              | ...        | ...      |\n",
    "\n",
    "#### One-hot encoding with pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age  sex_female  sex_male\n",
      "0   10           0         1\n",
      "1   15           1         0\n",
      "2   14           0         1\n"
     ]
    }
   ],
   "source": [
    "# Import the pandas library\n",
    "import pandas as pd\n",
    "# initialize list of lists \n",
    "data = [['male', 10], ['female', 15], ['male', 14]] \n",
    "  \n",
    "# Create the pandas DataFrame \n",
    "df = pd.DataFrame(data, columns = ['sex', 'age'])\n",
    "\n",
    "# Perform one-hot encoding on the 'sex' feature of df\n",
    "# Encode all non numerical features in numbers\n",
    "df = pd.get_dummies(df, columns = ['sex'])\n",
    "print(df.columns)\n",
    "print(df.head())\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Textual dataset\n",
    "Movie review dataset\n",
    "\n",
    "| review                    | class    |\n",
    "|---------------------------|----------|\n",
    "| This movie ...            | positive |\n",
    "| The movie is forgettable. | negative |\n",
    "| A truly amazing ...       | positive |\n",
    "| ...                       | ...      |\n",
    "\n",
    "1. Text pre-processing\n",
    "- Converting to lowercase\n",
    "  - **Example**: `Reduction` to `reduction`\n",
    "- Converting to base-form\n",
    "  - **Example**: `reduction` to `reduce`\n",
    "\n",
    "2. Vectorization\n",
    "After processing data looks like this:\n",
    "\n",
    "| 0    | 1    | ... | n    | class    |\n",
    "|------|------|-----|------|----------|\n",
    "| 0.03 | 0.71 | ... | 0.22 | positive |\n",
    "| 0.45 | 0.00 | ... | 0.19 | negative |\n",
    "| 0.14 | 0.03 | ... | 0.45 | positive |\n",
    "\n",
    "3. Basic features\n",
    "- Number of words\n",
    "- Number of characters\n",
    "- Average lenght of words\n",
    "\n",
    "Word features:\n",
    "- POS tagging (Pronoun, Verb, Article, Noun ...)\n",
    "- NER (Person, Organization ...)\n",
    "\n",
    "#### Course concept\n",
    "- Text processing\n",
    "- Basic features\n",
    "- Word Features\n",
    "- Vectorization\n",
    "\n",
    "### Basic feature extraction\n",
    "#### Number of characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "      sex  age  num_char\n",
      "0    male   10         4\n",
      "1  female   15         6\n",
      "2    male   14         4\n"
     ]
    }
   ],
   "source": [
    "# Compute the number of characters\n",
    "text = \"I don't know.\"\n",
    "num_char = len(text)\n",
    "print(num_char)\n",
    "\n",
    "data = [['male', 10], ['female', 15], ['male', 14]] \n",
    "df = pd.DataFrame(data, columns = ['sex', 'age'])\n",
    "\n",
    "df['num_char'] = df['sex'].apply(len)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mary', 'had', 'a', 'little', 'lamb.']\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# Split the string into words\n",
    "text = \"Mary had a little lamb.\"\n",
    "words = text.split()\n",
    "print(words)\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    review  age  num_words\n",
      "0  Mary had a little lamb.   10          5\n",
      "1  Mary had a little lamb.   15          5\n"
     ]
    }
   ],
   "source": [
    "# Function that returns number of words in string\n",
    "def word_count(string):\n",
    "    words = text.split()\n",
    "    return len(words)\n",
    "\n",
    "data = [[\"Mary had a little lamb.\", 10], [\"Mary had a little lamb.\", 15]] \n",
    "df = pd.DataFrame(data, columns = ['review', 'age'])\n",
    "\n",
    "df['num_words'] = df['review'].apply(word_count)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average word lenght"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    review  age  num_words\n",
      "0  Mary had a little lamb.   10        3.8\n",
      "1  Mary had a little lamb.   15        3.8\n"
     ]
    }
   ],
   "source": [
    "def avg_word_lenght(x):\n",
    "    words = text.split()\n",
    "    word_lenghts = [len(word) for word in words]\n",
    "    avg_word_length = sum(word_lenghts)/len(words)\n",
    "    return avg_word_length\n",
    "\n",
    "data = [[\"Mary had a little lamb.\", 10], [\"Mary had a little lamb.\", 15]] \n",
    "df = pd.DataFrame(data, columns = ['review', 'age'])\n",
    "\n",
    "df['num_words'] = df['review'].apply(avg_word_lenght)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hash tags and mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "def hashtag_count(string):\n",
    "    # Split the string into words\n",
    "    words = string.split()\n",
    "    # Create a list of hashtags\n",
    "    hashtags = [word for word in words if word.startswith('#')]\n",
    "    return len(hashtags)\n",
    "\n",
    "print(hashtag_count(\"My first tweet! #FirstTweet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other features\n",
    "- Number of sentences\n",
    "- Number of paragraphs\n",
    "- Word starting with an uppercase\n",
    "- All-capital words\n",
    "- Numeric quantities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.0\n"
     ]
    }
   ],
   "source": [
    "data = [[\"Mary had a little lamb.\"], [\"Nick had a big dog.\"]] \n",
    "df = pd.DataFrame(data, columns = ['content'])\n",
    "\n",
    "df['char_count'] = df['content'].apply(len)\n",
    "print(df['char_count'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redability tests\n",
    "- Determine readability of an English passage\n",
    "- Scale ranging from primary school up to college graduate level\n",
    "- A mathematical formula utilizing word, syllable and sentence count\n",
    "- Used in fake news and opinion spam detection\n",
    "\n",
    "#### Redability test examples (English text only)\n",
    "- **Flesch reading ease**\n",
    "- **Gunning fog index**\n",
    "- Simple Measure of Gobbledygook (SMOG)\n",
    "- Dale-Chall score\n",
    "\n",
    "#### Flesch reading ease\n",
    "- One of the oldest and most widely used test\n",
    "- Dependent on two factors:\n",
    "  - **Greater the average sentence length, harder the text is to read**\n",
    "    - \"This is short sentence.\"\n",
    "    - \"This is linger sentence with more words an it is harder to follow than the first sentence.\"\n",
    "  - **Greated the average mumber of syllables in a word, harder the text is to read**\n",
    "    - \"I live in my home.\"\n",
    "    - \"I reside in my domicile.\"\n",
    "- Higher the score, greated the readability\n",
    "\n",
    "#### Gunning for index\n",
    "- Developed in 1954\n",
    "- Also dependent on average sentence length\n",
    "- Great the percentage of complex words (3 or more syllables), hader the text is to read\n",
    "- Higher the index, lesser the readability\n",
    "\n",
    "#### The textatistic library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81.67466335836913\n",
      "7.913698140200286\n"
     ]
    }
   ],
   "source": [
    "from textatistic import Textatistic\n",
    "\n",
    "text = '\\nThe gods had condemned Sisyphus to ceaselessly rolling a rock to the top of a mountain, whence the stone would fall back of its own weight. They had thought with some reason that there is no more dreadful punishment than futile and hopeless labor. If one believes Homer, Sisyphus was the wisest and most prudent of mortals. According to another tradition, however, he was disposed to practice the profession of highwayman. I see no contradiction in this. Opinions differ as to the reasons why he became the futile laborer of the underworld. To begin with, he is accused of a certain levity in regard to the gods. He stole their secrets. Egina, the daughter of Esopus, was carried off by Jupiter. The father was shocked by that disappearance and complained to Sisyphus. He, who knew of the abduction, offered to tell about it on condition that Esopus would give water to the citadel of Corinth. To the celestial thunderbolts he preferred the benediction of water. He was punished for this in the underworld. Homer tells us also that Sisyphus had put Death in chains. Pluto could not endure the sight of his deserted, silent empire. He dispatched the god of war, who liberated Death from the hands of her conqueror. It is said that Sisyphus, being near to death, rashly wanted to test his wife\\'s love. He ordered her to cast his unburied body into the middle of the public square. Sisyphus woke up in the underworld. And there, annoyed by an obedience so contrary to human love, he obtained from Pluto permission to return to earth in order to chastise his wife. But when he had seen again the face of this world, enjoyed water and sun, warm stones and the sea, he no longer wanted to go back to the infernal darkness. Recalls, signs of anger, warnings were of no avail. Many years more he lived facing the curve of the gulf, the sparkling sea, and the smiles of earth. A decree of the gods was necessary. Mercury came and seized the impudent man by the collar and, snatching him from his joys, lead him forcibly back to the underworld, where his rock was ready for him. You have already grasped that Sisyphus is the absurd hero. He is, as much through his passions as through his torture. His scorn of the gods, his hatred of death, and his passion for life won him that unspeakable penalty in which the whole being is exerted toward accomplishing nothing. This is the price that must be paid for the passions of this earth. Nothing is told us about Sisyphus in the underworld. Myths are made for the imagination to breathe life into them. As for this myth, one sees merely the whole effort of a body straining to raise the huge stone, to roll it, and push it up a slope a hundred times over; one sees the face screwed up, the cheek tight against the stone, the shoulder bracing the clay-covered mass, the foot wedging it, the fresh start with arms outstretched, the wholly human security of two earth-clotted hands. At the very end of his long effort measured by skyless space and time without depth, the purpose is achieved. Then Sisyphus watches the stone rush down in a few moments toward tlower world whence he will have to push it up again toward the summit. He goes back down to the plain. It is during that return, that pause, that Sisyphus interests me. A face that toils so close to stones is already stone itself! I see that man going back down with a heavy yet measured step toward the torment of which he will never know the end. That hour like a breathing-space which returns as surely as his suffering, that is the hour of consciousness. At each of those moments when he leaves the heights and gradually sinks toward the lairs of the gods, he is superior to his fate. He is stronger than his rock. If this myth is tragic, that is because its hero is conscious. Where would his torture be, indeed, if at every step the hope of succeeding upheld him? The workman of today works everyday in his life at the same tasks, and his fate is no less absurd. But it is tragic only at the rare moments when it becomes conscious. Sisyphus, proletarian of the gods, powerless and rebellious, knows the whole extent of his wretched condition: it is what he thinks of during his descent. The lucidity that was to constitute his torture at the same time crowns his victory. There is no fate that can not be surmounted by scorn. If the descent is thus sometimes performed in sorrow, it can also take place in joy. This word is not too much. Again I fancy Sisyphus returning toward his rock, and the sorrow was in the beginning. When the images of earth cling too tightly to memory, when the call of happiness becomes too insistent, it happens that melancholy arises in man\\'s heart: this is the rock\\'s victory, this is the rock itself. The boundless grief is too heavy to bear. These are our nights of Gethsemane. But crushing truths perish from being acknowledged. Thus, Edipus at the outset obeys fate without knowing it. But from the moment he knows, his tragedy begins. Yet at the same moment, blind and desperate, he realizes that the only bond linking him to the world is the cool hand of a girl. Then a tremendous remark rings out: \"Despite so many ordeals, my advanced age and the nobility of my soul make me conclude that all is well.\" Sophocles\\' Edipus, like Dostoevsky\\'s Kirilov, thus gives the recipe for the absurd victory. Ancient wisdom confirms modern heroism. One does not discover the absurd without being tempted to write a manual of happiness. \"What!---by such narrow ways--?\" There is but one world, however. Happiness and the absurd are two sons of the same earth. They are inseparable. It would be a mistake to say that happiness necessarily springs from the absurd. Discovery. It happens as well that the felling of the absurd springs from happiness. \"I conclude that all is well,\" says Edipus, and that remark is sacred. It echoes in the wild and limited universe of man. It teaches that all is not, has not been, exhausted. It drives out of this world a god who had come into it with dissatisfaction and a preference for futile suffering. It makes of fate a human matter, which must be settled among men. All Sisyphus\\' silent joy is contained therein. His fate belongs to him. His rock is a thing. Likewise, the absurd man, when he contemplates his torment, silences all the idols. In the universe suddenly restored to its silence, the myriad wondering little voices of the earth rise up. Unconscious, secret calls, invitations from all the faces, they are the necessary reverse and price of victory. There is no sun without shadow, and it is essential to know the night. The absurd man says yes and his efforts will henceforth be unceasing. If there is a personal fate, there is no higher destiny, or at least there is, but one which he concludes is inevitable and despicable. For the rest, he knows himself to be the master of his days. At that subtle moment when man glances backward over his life, Sisyphus returning toward his rock, in that slight pivoting he contemplates that series of unrelated actions which become his fate, created by him, combined under his memory\\'s eye and soon sealed by his death. Thus, convinced of the wholly human origin of all that is human, a blind man eager to see who knows that the night has no end, he is still on the go. The rock is still rolling. I leave Sisyphus at the foot of the mountain! One always finds one\\'s burden again. But Sisyphus teaches the higher fidelity that negates the gods and raises rocks. He too concludes that all is well. This universe henceforth without a master seems to him neither sterile nor futile. Each atom of that stone, each mineral flake of that night filled mountain, in itself forms a world. The struggle itself toward the heights is enough to fill a man\\'s heart. One must imagine Sisyphus happy.\\n'\n",
    "readability_scores = Textatistic(text).scores\n",
    "\n",
    "# Generate scores\n",
    "print(readability_scores['flesch_score'])\n",
    "print(readability_scores['gunningfog_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Readability of various publications\n",
    "In this exercise, you have been given excerpts of articles from four publications. Your task is to compute the readability of these excerpts using the Gunning fog index and consequently, determine the relative difficulty of reading these publications.\n",
    "\n",
    "The excerpts are available as the following strings:\n",
    "\n",
    "forbes- An excerpt from an article from Forbes magazine on the Chinese social credit score system.\n",
    "harvard_law- An excerpt from a book review published in Harvard Law Review.\n",
    "r_digest- An excerpt from a Reader's Digest article on flight turbulence.\n",
    "time_kids - An excerpt from an article on the ill effects of salt consumption published in TIME for Kids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14.436002482929858, 20.735401069518716, 11.085587583148559, 5.926785009861934]\n"
     ]
    }
   ],
   "source": [
    "from textatistic import Textatistic\n",
    "\n",
    "forbes = '\\nThe idea is to create more transparency about companies and individuals that are breaking the law or are non-compliant with official obligations and incentivize the right behaviors with the overall goal of improving governance and market order. The Chinese Communist Party intends the social credit score system to “allow the trustworthy to roam freely under heaven while making it hard for the discredited to take a single step.” Even though the system is still under development it currently plays out in real life in myriad ways for private citizens, businesses and government officials. Generally, higher credit scores give people a variety of advantages. Individuals are often given perks such as discounted energy bills and access or better visibility on dating websites. Often, those with higher social credit scores are able to forgo deposits on rental properties, bicycles, and umbrellas. They can even get better travel deals. In addition, Chinese hospitals are currently experimenting with social credit scores. A social credit score above 650 at one hospital allows an individual to see a doctor without lining up to pay.\\n'\n",
    "harvard_law = '\\nIn his important new book, The Schoolhouse Gate: Public Education, the Supreme Court, and the Battle for the American Mind, Professor Justin Driver reminds us that private controversies that arise within the confines of public schools are part of a broader historical arc — one that tracks a range of cultural and intellectual flashpoints in U.S. history. Moreover, Driver explains, these tensions are reflected in constitutional law, and indeed in the history and jurisprudence of the Supreme Court. As such, debates that arise in the context of public education are not simply about the conflict between academic freedom, public safety, and student rights. They mirror our persistent struggle to reconcile our interest in fostering a pluralistic society, rooted in the ideal of individual autonomy, with our desire to cultivate a sense of national unity and shared identity (or, put differently, our effort to reconcile our desire to forge common norms of citizenship with our fear of state indoctrination and overencroachment). In this regard, these debates reflect the unique role that both the school and the courts have played in defining and enforcing the boundaries of American citizenship. \\n'\n",
    "r_digest = '\\nThis week 30 passengers were reportedly injured when a Turkish Airlines flight landing at John F. Kennedy International Airport encountered turbulent conditions. Injuries included bruises, bloody noses, and broken bones. In mid-February, a Delta Airlines flight made an emergency landing to assist three passengers in getting to the nearest hospital after some sudden and unexpected turbulence. Doctors treated 15 passengers after a flight from Miami to Buenos Aires last October for everything from severe bruising to nosebleeds after the plane caught some rough winds over Brazil. In 2016, 23 passengers were injured on a United Airlines flight after severe turbulence threw people into the cabin ceiling. The list goes on. Turbulence has been become increasingly common, with painful outcomes for those on board. And more costly to the airlines, too. Forbes estimates that the cost of turbulence has risen to over $500 million each year in damages and delays. And there are no signs the increase in turbulence will be stopping anytime soon.\\n'\n",
    "time_kids = '\\nThat, of course, is easier said than done. The more you eat salty foods, the more you develop a taste for them. The key to changing your diet is to start small. “Small changes in sodium in foods are not usually noticed,” Quader says. Eventually, she adds, the effort will reset a kid’s taste buds so the salt cravings stop. Bridget Murphy is a dietitian at New York University’s Langone Medical Center. She suggests kids try adding spices to their food instead of salt. Eating fruits and veggies and cutting back on packaged foods will also help. Need a little inspiration? Murphy offers this tip: Focus on the immediate effects of a diet that is high in sodium. High blood pressure can make it difficult to be active. “Do you want to be able to think clearly and perform well in school?” she asks. “If you’re an athlete, do you want to run faster?” If you answered yes to these questions, then it’s time to shake the salt habit.\\n'\n",
    "\n",
    "# List of excerpts\n",
    "excerpts = [forbes, harvard_law, r_digest, time_kids]\n",
    "\n",
    "# Loop through excerpts and compute gunning fog index\n",
    "gunning_fog_scores = []\n",
    "for excerpt in excerpts:\n",
    "  readability_scores = Textatistic(excerpt).scores\n",
    "  gunning_fog = readability_scores['gunningfog_score']\n",
    "  gunning_fog_scores.append(gunning_fog)\n",
    "\n",
    "# Print the gunning fog indices\n",
    "print(gunning_fog_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text preprocessing, POS tagging and NER\n",
    "In this chapter, you will learn about tokenization and lemmatization. You will then learn how to perform text cleaning, part-of-speech tagging, and named entity recognition using the spaCy library. Upon mastering these concepts, you will proceed to make the Gettysburg address machine-friendly, analyze noun usage in fake news, and identify people mentioned in a TechCrunch article.\n",
    "\n",
    "### Tokenization and Lemmatization\n",
    "#### Making text machine friendly\n",
    "- Dogs, dog\n",
    "- reduction, REDUCING, Reduce\n",
    "- don't, do not\n",
    "\n",
    "#### Text preprocessing techniques\n",
    "- Converting words into lowercase\n",
    "- Removing leading and trailing whitespaces\n",
    "- Removing punctuations\n",
    "- Removing stopwords\n",
    "- Expanding contractions\n",
    "- Removing spectial characters (numbers, emojis, etc.)\n",
    "\n",
    "#### Tokenization\n",
    "\"I have a dog. His name is Hachi\"\n",
    "**Tokens:**\n",
    "`[\"I\", \"have\", \"a\", \"dog\", \".\", \"His\", \"name\", \"is\", \"Hachi\"]`\n",
    "\n",
    "\"Don't do this.\"\n",
    "**Tokens:**\n",
    "`[\"Do\", \"n't\", \"do\", \"this\", \".\"]`\n",
    "\n",
    "#### Tokenization using spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', '!', 'I', 'do', \"n't\", 'know', 'what', 'I', \"'m\", 'doing', 'here']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "string = \"Hello! I don't know what I'm doing here\"\n",
    "doc = nlp(string)\n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization\n",
    "- Convert word into its base form\n",
    "  - redicing, reduces, reduced, reduction -> reduce\n",
    "  - am, are, is -> be\n",
    "  - n't -> not\n",
    "  - 've -> have\n",
    "\n",
    "#### Lemmatization using spaCy\n",
    "spaCy generate lemmas by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', '!', '-PRON-', 'do', 'not', 'know', 'what', '-PRON-', 'be', 'do', 'here']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "string = \"Hello! I don't know what I'm doing here\"\n",
    "doc = nlp(string)\n",
    "\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Tokenizing the Gettysburg Address\n",
    "In this exercise, you will be tokenizing one of the most famous speeches of all time: the Gettysburg Address delivered by American President Abraham Lincoln during the American Civil War.\n",
    "\n",
    "The entire speech is available as a string named gettysburg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Four', 'score', 'and', 'seven', 'years', 'ago', 'our', 'fathers', 'brought', 'forth', 'on', 'this', 'continent', ',', 'a', 'new', 'nation', ',', 'conceived', 'in', 'Liberty', ',', 'and', 'dedicated', 'to', 'the', 'proposition', 'that', 'all', 'men', 'are', 'created', 'equal', '.', 'Now', 'we', \"'re\", 'engaged', 'in', 'a', 'great', 'civil', 'war', ',', 'testing', 'whether', 'that', 'nation', ',', 'or', 'any', 'nation', 'so', 'conceived', 'and', 'so', 'dedicated', ',', 'can', 'long', 'endure', '.', 'We', \"'re\", 'met', 'on', 'a', 'great', 'battlefield', 'of', 'that', 'war', '.', 'We', \"'ve\", 'come', 'to', 'dedicate', 'a', 'portion', 'of', 'that', 'field', ',', 'as', 'a', 'final', 'resting', 'place', 'for', 'those', 'who', 'here', 'gave', 'their', 'lives', 'that', 'that', 'nation', 'might', 'live', '.', 'It', \"'s\", 'altogether', 'fitting', 'and', 'proper', 'that', 'we', 'should', 'do', 'this', '.', 'But', ',', 'in', 'a', 'larger', 'sense', ',', 'we', 'ca', \"n't\", 'dedicate', '-', 'we', 'can', 'not', 'consecrate', '-', 'we', 'can', 'not', 'hallow', '-', 'this', 'ground', '.', 'The', 'brave', 'men', ',', 'living', 'and', 'dead', ',', 'who', 'struggled', 'here', ',', 'have', 'consecrated', 'it', ',', 'far', 'above', 'our', 'poor', 'power', 'to', 'add', 'or', 'detract', '.', 'The', 'world', 'will', 'little', 'note', ',', 'nor', 'long', 'remember', 'what', 'we', 'say', 'here', ',', 'but', 'it', 'can', 'never', 'forget', 'what', 'they', 'did', 'here', '.', 'It', 'is', 'for', 'us', 'the', 'living', ',', 'rather', ',', 'to', 'be', 'dedicated', 'here', 'to', 'the', 'unfinished', 'work', 'which', 'they', 'who', 'fought', 'here', 'have', 'thus', 'far', 'so', 'nobly', 'advanced', '.', 'It', \"'s\", 'rather', 'for', 'us', 'to', 'be', 'here', 'dedicated', 'to', 'the', 'great', 'task', 'remaining', 'before', 'us', '-', 'that', 'from', 'these', 'honored', 'dead', 'we', 'take', 'increased', 'devotion', 'to', 'that', 'cause', 'for', 'which', 'they', 'gave', 'the', 'last', 'full', 'measure', 'of', 'devotion', '-', 'that', 'we', 'here', 'highly', 'resolve', 'that', 'these', 'dead', 'shall', 'not', 'have', 'died', 'in', 'vain', '-', 'that', 'this', 'nation', ',', 'under', 'God', ',', 'shall', 'have', 'a', 'new', 'birth', 'of', 'freedom', '-', 'and', 'that', 'government', 'of', 'the', 'people', ',', 'by', 'the', 'people', ',', 'for', 'the', 'people', ',', 'shall', 'not', 'perish', 'from', 'the', 'earth', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "gettysburg = \"Four score and seven years ago our fathers brought forth on this continent, a new nation, conceived in Liberty, and dedicated to the proposition that all men are created equal. Now we're engaged in a great civil war, testing whether that nation, or any nation so conceived and so dedicated, can long endure. We're met on a great battlefield of that war. We've come to dedicate a portion of that field, as a final resting place for those who here gave their lives that that nation might live. It's altogether fitting and proper that we should do this. But, in a larger sense, we can't dedicate - we can not consecrate - we can not hallow - this ground. The brave men, living and dead, who struggled here, have consecrated it, far above our poor power to add or detract. The world will little note, nor long remember what we say here, but it can never forget what they did here. It is for us the living, rather, to be dedicated here to the unfinished work which they who fought here have thus far so nobly advanced. It's rather for us to be here dedicated to the great task remaining before us - that from these honored dead we take increased devotion to that cause for which they gave the last full measure of devotion - that we here highly resolve that these dead shall not have died in vain - that this nation, under God, shall have a new birth of freedom - and that government of the people, by the people, for the people, shall not perish from the earth.\"\n",
    "\n",
    "# Load the en_core_web_sm model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Create a Doc object\n",
    "doc = nlp(gettysburg)\n",
    "\n",
    "# Generate the tokens\n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "four score and seven year ago -PRON- father bring forth on this continent , a new nation , conceive in Liberty , and dedicate to the proposition that all man be create equal . now -PRON- be engage in a great civil war , test whether that nation , or any nation so conceived and so dedicated , can long endure . -PRON- be meet on a great battlefield of that war . -PRON- have come to dedicate a portion of that field , as a final resting place for those who here give -PRON- life that that nation might live . -PRON- be altogether fitting and proper that -PRON- should do this . but , in a large sense , -PRON- can not dedicate - -PRON- can not consecrate - -PRON- can not hallow - this ground . the brave man , living and dead , who struggle here , have consecrate -PRON- , far above -PRON- poor power to add or detract . the world will little note , nor long remember what -PRON- say here , but -PRON- can never forget what -PRON- do here . -PRON- be for -PRON- the living , rather , to be dedicate here to the unfinished work which -PRON- who fight here have thus far so nobly advanced . -PRON- be rather for -PRON- to be here dedicate to the great task remain before -PRON- - that from these honor dead -PRON- take increase devotion to that cause for which -PRON- give the last full measure of devotion - that -PRON- here highly resolve that these dead shall not have die in vain - that this nation , under God , shall have a new birth of freedom - and that government of the people , by the people , for the people , shall not perish from the earth .\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the en_core_web_sm model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Create a Doc object\n",
    "doc = nlp(gettysburg)\n",
    "\n",
    "# Generate lemmas\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "\n",
    "# Convert lemmas into a string\n",
    "print(' '.join(lemmas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text cleaning\n",
    "#### Text cleaning techniques\n",
    "- Unnecessary whitespaces and escape sequences\n",
    "- Punctuations\n",
    "- Special characters (numbers, emojis, etc.)\n",
    "- Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(\"Dog\".isalpha())\n",
    "print(\"3dogs\".isalpha())\n",
    "print(\"1233456\".isalpha())\n",
    "print(\"U.S.A\".isalpha())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A word of caution\n",
    "- Abreviation: U.S.A, U.K., etc\n",
    "- Proper Nouns: word2vec and xto10x\n",
    "- Write your own custom function (using regex) for the more nuanced cases.\n",
    "\n",
    "#### Removing non-alphabetical characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['OMG', '!', '!', '!', 'this', 'be', 'like', '     ', 'the', 'good', 'thing', 'ever', '\\t\\n', '.', '\\n', 'wow', ',', 'such', 'an', 'amazing', 'song', '!', '-PRON-', 'be', 'hook', '.', 'Top', '5', 'definitely', '.', '?']\n",
      "OMG this be like the good thing ever wow such an amazing song -PRON- be hook Top definitely\n"
     ]
    }
   ],
   "source": [
    "string = \"\"\"OMG!!! This is like      the best thing ever \\t\\n.\n",
    "Wow, such an amazing song! I'm hooked. Top 5 definitely. ?\"\"\"\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(string)\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "print(lemmas)\n",
    "\n",
    "# Remove tokens that are not alphabetic\n",
    "a_lemmas = [lemma for lemma in lemmas if lemma.isalpha() or lemma == '-PRON-']\n",
    "print(' '.join(a_lemmas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stopwords\n",
    "- Words that occur extemely commonly\n",
    "- Eg. articles, be verbs, pronouns, etc.\n",
    "\n",
    "#### Removing stop words using spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OMG like good thing wow amazing song hook Top definitely\n"
     ]
    }
   ],
   "source": [
    "# Get list of stopwords\n",
    "stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "string = \"\"\"OMG!!! This is like      the best thing ever \\t\\n.\n",
    "Wow, such an amazing song! I'm hooked. Top 5 definitely. ?\"\"\"\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(string)\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "\n",
    "# Removing stopwords and non-alphabetic tokens\n",
    "a_lemmas = [lemma for lemma in lemmas if lemma.isalpha() and lemma not in stopwords]\n",
    "print(' '.join(a_lemmas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other text preprocessing techniques\n",
    "- Removing HTML/XML tags\n",
    "- Replacing accented characters\n",
    "- Correcting spelling errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Cleaning a blog post\n",
    "In this exercise, you have been given an excerpt from a blog post. Your task is to clean this text into a more machine friendly format. This will involve converting to lowercase, lemmatization and removing stopwords, punctuations and non-alphabetic characters.\n",
    "\n",
    "The excerpt is available as a string blog and has been printed to the console. The list of stopwords are available as stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "century politic witness alarming rise populism Europe warning sign come UK Brexit Referendum vote swinge way Leave follow stupendous victory billionaire Donald Trump President United States November Europe steady rise populist far right party capitalize Europe Immigration Crisis raise nationalist anti europe sentiment instance include Alternative Germany AfD win seat enter Bundestag upset Germany political order time Second World War success Five Star Movement Italy surge popularity neo nazism neo fascism country Hungary Czech Republic Poland Austria\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "blog = \"Twenty-first-century politics has witnessed an alarming rise of populism in the U.S. and Europe. The first warning signs came with the UK Brexit Referendum vote in 2016 swinging in the way of Leave. This was followed by a stupendous victory by billionaire Donald Trump to become the 45th President of the United States in November 2016. Since then, Europe has seen a steady rise in populist and far-right parties that have capitalized on Europe’s Immigration Crisis to raise nationalist and anti-Europe sentiments. Some instances include Alternative for Germany (AfD) winning 12.6% of all seats and entering the Bundestag, thus upsetting Germany’s political order for the first time since the Second World War, the success of the Five Star Movement in Italy and the surge in popularity of neo-nazism and neo-fascism in countries such as Hungary, Czech Republic, Poland and Austria.\"\n",
    "\n",
    "# Load model and create Doc object\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(blog)\n",
    "\n",
    "# Generate lemmatized tokens\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "\n",
    "# Remove stopwords and non-alphabetic tokens\n",
    "a_lemmas = [lemma for lemma in lemmas \n",
    "            if lemma.isalpha() and lemma not in stopwords]\n",
    "\n",
    "# Print string after text cleaning\n",
    "print(' '.join(a_lemmas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examples: Cleaning TED talks in a dataframe\n",
    "In this exercise, we will revisit the TED Talks from the first chapter. You have been a given a dataframe ted consisting of 5 TED Talks. Your task is to clean these talks using techniques discussed earlier by writing a function preprocess and applying it to the transcript feature of the dataframe.\n",
    "\n",
    "The stopwords list is available as stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Mary little lamb\n",
      "1        Nick big dog\n",
      "Name: transcript, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "data = [[\"Mary had a little lamb.\"], [\"Nick had a big dog.\"]] \n",
    "ted = pd.DataFrame(data, columns = ['transcript'])\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess(text):\n",
    "  \t# Create Doc object\n",
    "    doc = nlp(text, disable=['ner', 'parser'])\n",
    "    # Generate lemmas\n",
    "    lemmas = [token.lemma_ for token in doc]\n",
    "    # Remove stopwords and non-alphabetic characters\n",
    "    a_lemmas = [lemma for lemma in lemmas \n",
    "            if lemma.isalpha() and lemma not in stopwords]\n",
    "    \n",
    "    return ' '.join(a_lemmas)\n",
    "  \n",
    "# Apply preprocess to ted['transcript']\n",
    "ted['transcript'] = ted['transcript'].apply(preprocess)\n",
    "print(ted['transcript'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-of-speach tagging\n",
    "#### Applications\n",
    "- Word-sense disambiguation\n",
    "  - `\"The bear is a majestic animal\"`\n",
    "  - `\"Please bear with me\"`\n",
    "- Sentiment analysis\n",
    "- Question answering\n",
    "- Fake news and opinion spam detection\n",
    "\n",
    "#### POS tagging\n",
    "- Assigning every word, its corresponding part of speech.\n",
    "\n",
    "\"Jane is an amazing gutarist.\"\n",
    "- **POS Tagging:**\n",
    "  - Jane -> proper noun\n",
    "  - is -> verb\n",
    "  - an -> determiner\n",
    "  - amazing -> adjective\n",
    "  - guitarist -> noun\n",
    "  \n",
    "#### POS tagging using spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Jane', 'PROPN'), ('is', 'AUX'), ('an', 'DET'), ('amazing', 'ADJ'), ('gutarist', 'NOUN'), ('.', 'PUNCT')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "string = \"Jane is an amazing gutarist.\"\n",
    "doc = nlp(string)\n",
    "\n",
    "# Generate list of tokens and pos tags\n",
    "pos = [(token.text, token.pos_) for token in doc]\n",
    "print(pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy pos tagging depends on data which it was trained on and on data on for which it was used for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Counting nouns in a piece of text\n",
    "In this exercise, we will write two functions, nouns() and proper_nouns() that will count the number of other nouns and proper nouns in a piece of text respectively.\n",
    "\n",
    "These functions will take in a piece of text and generate a list containing the POS tags for each word. It will then return the number of proper nouns/other nouns that the text contains. We will use these functions in the next exercise to generate interesting insights about fake news.\n",
    "\n",
    "The en_core_web_sm model has already been loaded as nlp in this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Returns number of proper nouns\n",
    "def proper_nouns(text, model=nlp):\n",
    "  \t# Create doc object\n",
    "    doc = model(text)\n",
    "    # Generate list of POS tags\n",
    "    pos = [token.pos_ for token in doc]\n",
    "    \n",
    "    # Return number of proper nouns\n",
    "    return pos.count('PROPN')\n",
    "\n",
    "print(proper_nouns(\"Abdul, Bill and Cathy went to the market to buy apples.\", nlp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Noun usage in fake news\n",
    "In this exercise, you have been given a dataframe headlines that contains news headlines that are either fake or real. Your task is to generate two new features num_propn and num_noun that represent the number of proper nouns and other nouns contained in the title feature of headlines.\n",
    "\n",
    "Next, we will compute the mean number of proper nouns and other nouns used in fake and real news headlines and compare the values. If there is a remarkable difference, then there is a good chance that using the num_propn and num_noun features in fake news detectors will improve its performance.\n",
    "\n",
    "To accomplish this task, the functions proper_nouns and nouns that you had built in the previous exercise have already been made available to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean no. of proper nouns in real and fake headlines are 1.00 and 1.00 respectively\n",
      "Mean no. of other nouns in real and fake headlines are 1.00 and 1.00 respectively\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def proper_nouns(text, model=nlp):\n",
    "  \t# Create doc object\n",
    "    doc = model(text)\n",
    "    # Generate list of POS tags\n",
    "    pos = [token.pos_ for token in doc]\n",
    "    \n",
    "    # Return number of proper nouns\n",
    "    return pos.count('PROPN')\n",
    "\n",
    "def nouns(text, model=nlp):\n",
    "  \t# Create doc object\n",
    "    doc = model(text)\n",
    "    # Generate list of POS tags\n",
    "    pos = [token.pos_ for token in doc]\n",
    "    \n",
    "    # Return number of proper nouns\n",
    "    return pos.count('NOUN')\n",
    "\n",
    "data = [[\"Mary had a little lamb.\", \"REAL\"], [\"Nick had a big dog.\", \"FAKE\"]] \n",
    "headlines = pd.DataFrame(data, columns = ['title', 'label'])\n",
    "\n",
    "headlines['num_propn'] = headlines['title'].apply(proper_nouns)\n",
    "headlines['num_noun'] = headlines['title'].apply(nouns)\n",
    "\n",
    "# Compute mean of proper nouns\n",
    "real_propn = headlines[headlines['label'] == 'REAL']['num_propn'].mean()\n",
    "fake_propn = headlines[headlines['label'] == 'FAKE']['num_propn'].mean()\n",
    "\n",
    "# Compute mean of other nouns\n",
    "real_noun = headlines[headlines['label'] == 'REAL']['num_noun'].mean()\n",
    "fake_noun = headlines[headlines['label'] == 'FAKE']['num_noun'].mean()\n",
    "\n",
    "# Print results\n",
    "print(\"Mean no. of proper nouns in real and fake headlines are %.2f and %.2f respectively\"%(real_propn, fake_propn))\n",
    "print(\"Mean no. of other nouns in real and fake headlines are %.2f and %.2f respectively\"%(real_noun, fake_noun))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named entity recognition\n",
    "#### Applications\n",
    "- Efficient search algorithm\n",
    "- Question answering\n",
    "- News aarticles classification\n",
    "- Customer service\n",
    "\n",
    "#### Named entity recognition\n",
    "- Identifying and classifying named entities into predefined categories.\n",
    "- Categoories include person, organization, country, etc.\n",
    "\n",
    "\"John Doe is a software engineer working at Google. He lives in France.\"\n",
    "\n",
    "- **Named Entities**\n",
    "- John Doe -> person\n",
    "- Google -> organization\n",
    "- France -> country\n",
    "\n",
    "#### NER using spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('John Doe', 'PERSON'), ('Google', 'ORG'), ('France', 'GPE')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "string = \"John Doe is a software engineer working at Google. He lives in France.\"\n",
    "doc = nlp(string)\n",
    "\n",
    "# generate named entities\n",
    "ne = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "print(ne)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A word of caution\n",
    "- Not perfect\n",
    "- Performance dependent on training and test data\n",
    "- Train models with specialized data foe nuanced cases\n",
    "- Language specific (model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sheryl Sandberg', 'Mark Zuckerberg']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "tc = \"\\nIt’s' been a busy day for Facebook  exec op-eds. Earlier this morning, Sheryl Sandberg broke the site’s silence around the Christchurch massacre, and now Mark Zuckerberg is calling on governments and other bodies to increase regulation around the sorts of data Facebook traffics in. He’s hoping to get out in front of heavy-handed regulation and get a seat at the table shaping it.\\n\"\n",
    "\n",
    "def find_persons(text):\n",
    "  # Create Doc object\n",
    "  doc = nlp(text)\n",
    "  \n",
    "  # Identify the persons\n",
    "  persons = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\n",
    "  \n",
    "  # Return persons\n",
    "  return persons\n",
    "\n",
    "print(find_persons(tc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. N-Gram models\n",
    "Learn about n-gram modeling and use it to perform sentiment analysis on movie reviews.\n",
    "\n",
    "### Building a bag of words model\n",
    "For any ML algorithm:\n",
    "- Data must be in tabular form\n",
    "- Training features must be numerical\n",
    "\n",
    "#### Bag of words model\n",
    "- Extract word tokens\n",
    "- Compute frequency of tokens\n",
    "- Construct a word vector out of these frequences and vocabulary of corpus\n",
    "\n",
    "**Corpus**\n",
    "`\"The lion is the king of the jungle\"`\n",
    "`\"Lions have lifespans of a decade\"`\n",
    "`\"The lion is an endangered species\"`\n",
    "\n",
    "**Vocabulary of model** -> `a`, `an`, `decade`, `endangered`, `have`, `is`, `jungle`, `king`, `lifespans`, `lion`, `Lions`, `of`, `species`, `the`, `The`\n",
    "\n",
    "Convert to word vector\n",
    "`The lion is the king of the jungle`\n",
    "`[0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 2, 1]`\n",
    "\n",
    "Each value in the vector corresponds to the frequency of the corresponding word in the vocabulary.\n",
    "\n",
    "#### Text preprocessing\n",
    "By doing text preprocessing model might be improved significantly\n",
    "- Lions, lion -> lion\n",
    "- The, the -> the\n",
    "- No punctuations\n",
    "- No stopwords\n",
    "- Preprocessing leads to smaller vacabulaies\n",
    "- Reducing mumber of dimensions helps improve performance\n",
    "\n",
    "#### Bag of words model using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 1 0 0 3]\n",
      " [0 0 0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 0]\n",
      " [1 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 1 1]\n",
      " [0 1 1 1 0 0 1 2 0 0 0 0 0 0 0 2 2 0 1 0 0]]\n",
      "   an  and  but  come  decade  endangered  forever  go  have  is  ...  king  \\\n",
      "0   0    0    0     0       0           0        0   0     0   1  ...     1   \n",
      "1   0    0    0     0       1           0        0   0     1   0  ...     0   \n",
      "2   1    0    0     0       0           1        0   0     0   1  ...     0   \n",
      "3   0    1    1     1       0           0        1   2     0   0  ...     0   \n",
      "\n",
      "   lifespans  lion  lions  may  men  of  on  species  the  \n",
      "0          0     1      0    0    0   1   0        0    3  \n",
      "1          1     0      1    0    0   1   0        0    0  \n",
      "2          0     1      0    0    0   0   0        1    1  \n",
      "3          0     0      0    2    2   0   1        0    0  \n",
      "\n",
      "[4 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = pd.Series([\n",
    "    'The lion is the king of the jungle',\n",
    "    'Lions have lifespans of a decade',\n",
    "    'The lion is an endangered species',\n",
    "    'men may come and men may go but i go on forever'\n",
    "])\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Generate matrix of word vectors\n",
    "# Lowercase all words and ignore single character tokens such as 'a'\n",
    "# Doesn't index in alphabetical order\n",
    "bow_matrix = vectorizer.fit_transform(corpus)\n",
    "print(bow_matrix.toarray())\n",
    "\n",
    "# Convert bow_matrix into a DataFrame\n",
    "bow_df = pd.DataFrame(bow_matrix.toarray())\n",
    "# Map the column names to vocabulary \n",
    "bow_df.columns = vectorizer.get_feature_names()\n",
    "print(bow_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         lion king jungle\n",
      "1     lion lifespan decade\n",
      "2    lion endanger species\n",
      "3     man come man forever\n",
      "Name: data, dtype: object\n",
      "[[0 0 0 0 1 1 0 1 0 0]\n",
      " [0 1 0 0 0 0 1 1 0 0]\n",
      " [0 0 1 0 0 0 0 1 0 1]\n",
      " [1 0 0 1 0 0 0 0 2 0]]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "corpus = pd.Series([\n",
    "    'The lion is the king of the jungle',\n",
    "    'Lions have lifespans of a decade',\n",
    "    'The lion is an endangered species',\n",
    "    'men may come and men may go but i go on forever'\n",
    "]) \n",
    "lemmas = pd.DataFrame(corpus, columns = ['data'])\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess(text):\n",
    "  \t# Create Doc object\n",
    "    doc = nlp(text, disable=['ner', 'parser'])\n",
    "    # Generate lemmas\n",
    "    lemmas = [token.lemma_ for token in doc]\n",
    "    # Remove stopwords and non-alphabetic characters\n",
    "    a_lemmas = [lemma for lemma in lemmas \n",
    "            if lemma.isalpha() and lemma not in stopwords]\n",
    "    \n",
    "    return ' '.join(a_lemmas)\n",
    "  \n",
    "# Apply preprocess to ted['transcript']\n",
    "lemmas['data'] = lemmas['data'].apply(preprocess)\n",
    "print(lemmas['data'])\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Generate matrix of word vectors\n",
    "# Lowercase all words and ignore single character tokens such as 'a'\n",
    "# Doesn't index in alphabetical order\n",
    "bow_matrix = vectorizer.fit_transform(corpus)\n",
    "print(bow_matrix.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a BoW Naive Bayes classifier\n",
    "#### Spam filtering\n",
    "\n",
    "**Steps**\n",
    "1. Text processing\n",
    "2. Building a bag-of-words model (or representations)\n",
    "3. Machine learning\n",
    "\n",
    "#### Text presprocessing using CountVectorizer\n",
    "CountVectorizer arguments\n",
    "- `lowercase` : `False`, `True`\n",
    "- `strip_accents` : `unicode`, `ascii`, `None`\n",
    "- `stop_words` : `english`, `list`, `None`\n",
    "- `token_pattern` : `regexp`\n",
    "- `tokenizer` : `function`\n",
    "\n",
    "#### Building the BoW model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 9)\n",
      "(1, 9)\n"
     ]
    }
   ],
   "source": [
    "review = [\n",
    "    'The lion is the king of the jungle',\n",
    "    'Lions have lifespans of a decade',\n",
    "    'The lion is an endangered species',\n",
    "    'men may come and men may go but i go on forever'\n",
    "]\n",
    "label = [1, 1, 1, 0]\n",
    "data = list(zip(review, label))\n",
    "\n",
    "# Create test spam dataframe\n",
    "df = pd.DataFrame(data, columns = ['message', 'label'])\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create CountVectorizer object\n",
    "vectorizer = CountVectorizer(strip_accents='ascii', stop_words='english', lowercase=False)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['message'], df['label'], test_size=0.25)\n",
    "X_train_bow = vectorizer.fit_transform(X_train)\n",
    "X_test_bow = vectorizer.transform(X_test)\n",
    "\n",
    "# Print shape of X_train_bow and X_test_bow\n",
    "print(X_train_bow.shape)\n",
    "print(X_test_bow.shape)\n",
    "\n",
    "# Words in test data which not part for count vectorizer ignored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the Naive Bayes classifier\n",
    "In the previous exercise, you generated the bag-of-words representations for the training and test movie review data. In this exercise, we will use this model to train a Naive Bayes classifier that can detect the sentiment of a movie review and compute its accuracy. Note that since this is a binary classification problem, the model is only capable of classifying a review as either positive (1) or negative (0). It is incapable of detecting neutral reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# !!! Depends on previous task\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "clf = MultinomialNB()\n",
    "\n",
    "# Train classifier\n",
    "clf.fit(X_train_bow, y_train)\n",
    "\n",
    "# Compute accuracy on test set\n",
    "accuracy = clf.score(X_test_bow, y_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building n-gram models\n",
    "BoW shortcomings\n",
    "\n",
    "| review                                | label    |\n",
    "|---------------------------------------|----------|\n",
    "| `'The movie was good and not boring'` | positive |\n",
    "| `'The movie was not good and boring'` | negative |\n",
    "\n",
    "- Exactly the same BoW representation!\n",
    "- Context of the words is lost\n",
    "- Sentiment depends on the position of 'not'\n",
    "\n",
    "**n-gram**\n",
    "- Contiguous sequence of n elements (or words) in given document.\n",
    "- n=1 -> bag-of-words\n",
    "\n",
    "`'for you a thousand times over'`\n",
    "\n",
    "- n=2, n-grams:\n",
    "\n",
    "`[\n",
    "'for you',\n",
    "'you a',\n",
    "'a thousand',\n",
    "'thousand times',\n",
    "'time over'\n",
    "]`\n",
    "\n",
    "- n=3, n-grams:\n",
    "\n",
    "`[\n",
    "'for you a',\n",
    "'you a thousand',\n",
    "'a thousand times',\n",
    "'thousand time over'\n",
    "]`\n",
    "\n",
    "- Capture more context\n",
    "\n",
    "#### Applications\n",
    "- sentence completion\n",
    "- spelling correction\n",
    "- machine translation correction\n",
    "\n",
    "#### Building n-gram models using scikit-learn\n",
    "Generate only bigrams.\n",
    "\n",
    "`bigrams = CountVectorizer(ngram_range=(2,2))`\n",
    "\n",
    "Generate unigrams, bigrams and trigrams.\n",
    "\n",
    "`ngram = CountVectorizer(ngram_range=(1,3))`\n",
    "\n",
    "#### Shortcomings\n",
    "- Curse of demensionality\n",
    "- Higher order n-grams are rare (more than 3)\n",
    "- Keep n is small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: n-gram models for movie tag lines\n",
    "In this exercise, we have been provided with a corpus of more than 9000 movie tag lines. Our job is to generate n-gram models up to n equal to 1, n equal to 2 and n equal to 3 for this data and discover the number of features for each model.\n",
    "\n",
    "We will then compare the number of features generated for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adult</th>\n",
       "      <th>belongs_to_collection</th>\n",
       "      <th>budget</th>\n",
       "      <th>genres</th>\n",
       "      <th>homepage</th>\n",
       "      <th>id</th>\n",
       "      <th>imdb_id</th>\n",
       "      <th>original_language</th>\n",
       "      <th>original_title</th>\n",
       "      <th>overview</th>\n",
       "      <th>...</th>\n",
       "      <th>release_date</th>\n",
       "      <th>revenue</th>\n",
       "      <th>runtime</th>\n",
       "      <th>spoken_languages</th>\n",
       "      <th>status</th>\n",
       "      <th>tagline</th>\n",
       "      <th>title</th>\n",
       "      <th>video</th>\n",
       "      <th>vote_average</th>\n",
       "      <th>vote_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 10194, 'name': 'Toy Story Collection', ...</td>\n",
       "      <td>30000000</td>\n",
       "      <td>[{'id': 16, 'name': 'Animation'}, {'id': 35, '...</td>\n",
       "      <td>http://toystory.disney.com/toy-story</td>\n",
       "      <td>862</td>\n",
       "      <td>tt0114709</td>\n",
       "      <td>en</td>\n",
       "      <td>Toy Story</td>\n",
       "      <td>Led by Woody, Andy's toys live happily in his ...</td>\n",
       "      <td>...</td>\n",
       "      <td>1995-10-30</td>\n",
       "      <td>373554033.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>[{'iso_639_1': 'en', 'name': 'English'}]</td>\n",
       "      <td>Released</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Toy Story</td>\n",
       "      <td>False</td>\n",
       "      <td>7.7</td>\n",
       "      <td>5415.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65000000</td>\n",
       "      <td>[{'id': 12, 'name': 'Adventure'}, {'id': 14, '...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8844</td>\n",
       "      <td>tt0113497</td>\n",
       "      <td>en</td>\n",
       "      <td>Jumanji</td>\n",
       "      <td>When siblings Judy and Peter discover an encha...</td>\n",
       "      <td>...</td>\n",
       "      <td>1995-12-15</td>\n",
       "      <td>262797249.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>[{'iso_639_1': 'en', 'name': 'English'}, {'iso...</td>\n",
       "      <td>Released</td>\n",
       "      <td>Roll the dice and unleash the excitement!</td>\n",
       "      <td>Jumanji</td>\n",
       "      <td>False</td>\n",
       "      <td>6.9</td>\n",
       "      <td>2413.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 119050, 'name': 'Grumpy Old Men Collect...</td>\n",
       "      <td>0</td>\n",
       "      <td>[{'id': 10749, 'name': 'Romance'}, {'id': 35, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15602</td>\n",
       "      <td>tt0113228</td>\n",
       "      <td>en</td>\n",
       "      <td>Grumpier Old Men</td>\n",
       "      <td>A family wedding reignites the ancient feud be...</td>\n",
       "      <td>...</td>\n",
       "      <td>1995-12-22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>[{'iso_639_1': 'en', 'name': 'English'}]</td>\n",
       "      <td>Released</td>\n",
       "      <td>Still Yelling. Still Fighting. Still Ready for...</td>\n",
       "      <td>Grumpier Old Men</td>\n",
       "      <td>False</td>\n",
       "      <td>6.5</td>\n",
       "      <td>92.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16000000</td>\n",
       "      <td>[{'id': 35, 'name': 'Comedy'}, {'id': 18, 'nam...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31357</td>\n",
       "      <td>tt0114885</td>\n",
       "      <td>en</td>\n",
       "      <td>Waiting to Exhale</td>\n",
       "      <td>Cheated on, mistreated and stepped on, the wom...</td>\n",
       "      <td>...</td>\n",
       "      <td>1995-12-22</td>\n",
       "      <td>81452156.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>[{'iso_639_1': 'en', 'name': 'English'}]</td>\n",
       "      <td>Released</td>\n",
       "      <td>Friends are the people who let you be yourself...</td>\n",
       "      <td>Waiting to Exhale</td>\n",
       "      <td>False</td>\n",
       "      <td>6.1</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 96871, 'name': 'Father of the Bride Col...</td>\n",
       "      <td>0</td>\n",
       "      <td>[{'id': 35, 'name': 'Comedy'}]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11862</td>\n",
       "      <td>tt0113041</td>\n",
       "      <td>en</td>\n",
       "      <td>Father of the Bride Part II</td>\n",
       "      <td>Just when George Banks has recovered from his ...</td>\n",
       "      <td>...</td>\n",
       "      <td>1995-02-10</td>\n",
       "      <td>76578911.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>[{'iso_639_1': 'en', 'name': 'English'}]</td>\n",
       "      <td>Released</td>\n",
       "      <td>Just When His World Is Back To Normal... He's ...</td>\n",
       "      <td>Father of the Bride Part II</td>\n",
       "      <td>False</td>\n",
       "      <td>5.7</td>\n",
       "      <td>173.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   adult                              belongs_to_collection    budget  \\\n",
       "0  False  {'id': 10194, 'name': 'Toy Story Collection', ...  30000000   \n",
       "1  False                                                NaN  65000000   \n",
       "2  False  {'id': 119050, 'name': 'Grumpy Old Men Collect...         0   \n",
       "3  False                                                NaN  16000000   \n",
       "4  False  {'id': 96871, 'name': 'Father of the Bride Col...         0   \n",
       "\n",
       "                                              genres  \\\n",
       "0  [{'id': 16, 'name': 'Animation'}, {'id': 35, '...   \n",
       "1  [{'id': 12, 'name': 'Adventure'}, {'id': 14, '...   \n",
       "2  [{'id': 10749, 'name': 'Romance'}, {'id': 35, ...   \n",
       "3  [{'id': 35, 'name': 'Comedy'}, {'id': 18, 'nam...   \n",
       "4                     [{'id': 35, 'name': 'Comedy'}]   \n",
       "\n",
       "                               homepage     id    imdb_id original_language  \\\n",
       "0  http://toystory.disney.com/toy-story    862  tt0114709                en   \n",
       "1                                   NaN   8844  tt0113497                en   \n",
       "2                                   NaN  15602  tt0113228                en   \n",
       "3                                   NaN  31357  tt0114885                en   \n",
       "4                                   NaN  11862  tt0113041                en   \n",
       "\n",
       "                original_title  \\\n",
       "0                    Toy Story   \n",
       "1                      Jumanji   \n",
       "2             Grumpier Old Men   \n",
       "3            Waiting to Exhale   \n",
       "4  Father of the Bride Part II   \n",
       "\n",
       "                                            overview  ... release_date  \\\n",
       "0  Led by Woody, Andy's toys live happily in his ...  ...   1995-10-30   \n",
       "1  When siblings Judy and Peter discover an encha...  ...   1995-12-15   \n",
       "2  A family wedding reignites the ancient feud be...  ...   1995-12-22   \n",
       "3  Cheated on, mistreated and stepped on, the wom...  ...   1995-12-22   \n",
       "4  Just when George Banks has recovered from his ...  ...   1995-02-10   \n",
       "\n",
       "       revenue runtime                                   spoken_languages  \\\n",
       "0  373554033.0    81.0           [{'iso_639_1': 'en', 'name': 'English'}]   \n",
       "1  262797249.0   104.0  [{'iso_639_1': 'en', 'name': 'English'}, {'iso...   \n",
       "2          0.0   101.0           [{'iso_639_1': 'en', 'name': 'English'}]   \n",
       "3   81452156.0   127.0           [{'iso_639_1': 'en', 'name': 'English'}]   \n",
       "4   76578911.0   106.0           [{'iso_639_1': 'en', 'name': 'English'}]   \n",
       "\n",
       "     status                                            tagline  \\\n",
       "0  Released                                                NaN   \n",
       "1  Released          Roll the dice and unleash the excitement!   \n",
       "2  Released  Still Yelling. Still Fighting. Still Ready for...   \n",
       "3  Released  Friends are the people who let you be yourself...   \n",
       "4  Released  Just When His World Is Back To Normal... He's ...   \n",
       "\n",
       "                         title  video vote_average vote_count  \n",
       "0                    Toy Story  False          7.7     5415.0  \n",
       "1                      Jumanji  False          6.9     2413.0  \n",
       "2             Grumpier Old Men  False          6.5       92.0  \n",
       "3            Waiting to Exhale  False          6.1       34.0  \n",
       "4  Father of the Bride Part II  False          5.7      173.0  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../dataset/movie/movies_metadata.csv', header=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             tagline vote_average\n",
      "0                                                             7.7\n",
      "1          Roll the dice and unleash the excitement!          6.9\n",
      "2  Still Yelling. Still Fighting. Still Ready for...          6.5\n",
      "3  Friends are the people who let you be yourself...          6.1\n",
      "4  Just When His World Is Back To Normal... He's ...          5.7\n",
      "ng1, ng2 and ng3 have 12930, 86665 and 192984 features respectively\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "df = pd.read_csv('../dataset/movie/movies_metadata.csv', header=0, keep_default_na = False, usecols=[\"tagline\", \"vote_average\"])\n",
    "print(df.head())\n",
    "\n",
    "corpus = df['tagline']\n",
    "\n",
    "# Generate n-grams upto n=1\n",
    "vectorizer_ng1 = CountVectorizer(ngram_range=(1,1))\n",
    "ng1 = vectorizer_ng1.fit_transform(corpus)\n",
    "\n",
    "# Generate n-grams upto n=2\n",
    "vectorizer_ng2 = CountVectorizer(ngram_range=(1,2))\n",
    "ng2 = vectorizer_ng2.fit_transform(corpus)\n",
    "\n",
    "# Generate n-grams upto n=3\n",
    "vectorizer_ng3 = CountVectorizer(ngram_range=(1, 3))\n",
    "ng3 = vectorizer_ng3.fit_transform(corpus)\n",
    "\n",
    "# Print the number of features for each model\n",
    "print(\"ng1, ng2 and ng3 have %i, %i and %i features respectively\" % (ng1.shape[1], ng2.shape[1], ng3.shape[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Higher order n-grams for sentiment analysis\n",
    "Similar to a previous exercise, we are going to build a classifier that can detect if the review of a particular movie is positive or negative. However, this time, we will use n-grams up to n=2 for the task.\n",
    "\n",
    "The n-gram training reviews are available as X_train_ng. The corresponding test reviews are available as X_test_ng. Finally, use y_train and y_test to access the training and test sentiment classes respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the classifier on the test set is 0.073\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "%i format: a number is required, not numpy.str_",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-1cd0e9d470d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mreview\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"The movie was not good. The plot had several holes and the acting lacked panache.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf_ng\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mng_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The sentiment predicted by the classifier is %i\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: %i format: a number is required, not numpy.str_"
     ]
    }
   ],
   "source": [
    "# !!! Depends on previous exercise\n",
    "# Split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['tagline'], df['vote_average'], test_size=0.25)\n",
    "\n",
    "ng_vectorizer = CountVectorizer(ngram_range=(1,2))\n",
    "X_train_ng = ng_vectorizer.fit_transform(X_train)\n",
    "X_test_ng = ng_vectorizer.transform(X_test)\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Define an instance of MultinomialNB \n",
    "clf_ng = MultinomialNB()\n",
    "\n",
    "# Fit the classifier \n",
    "clf_ng.fit(X_train_ng, y_train)\n",
    "\n",
    "# Measure the accuracy \n",
    "accuracy = clf_ng.score(X_test_ng, y_test)\n",
    "print(\"The accuracy of the classifier on the test set is %.3f\" % accuracy)\n",
    "\n",
    "# Predict the sentiment of a negative review\n",
    "review = \"The movie was not good. The plot had several holes and the acting lacked panache.\"\n",
    "prediction = clf_ng.predict(ng_vectorizer.transform([review]))[0]\n",
    "print(\"The sentiment predicted by the classifier is %i\" % (prediction))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Comparing performance of n-gram models\n",
    "You now know how to conduct sentiment analysis by converting text into various n-gram representations and feeding them to a classifier. In this exercise, we will conduct sentiment analysis for the same movie reviews from before using two n-gram models: unigrams and n-grams upto n equal to 3.\n",
    "\n",
    "We will then compare the performance using three criteria: accuracy of the model on the test set, time taken to execute the program and the number of features created when generating the n-gram representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-d748d69b0e8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# Splitting the data into training and test sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'review'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentiment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentiment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Generating ngrams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'time' is not defined"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# Splitting the data into training and test sets\n",
    "train_X, test_X, train_y, test_y = train_test_split(df['review'], df['sentiment'], test_size=0.5, random_state=42, stratify=df['sentiment'])\n",
    "\n",
    "# Generating ngrams\n",
    "vectorizer = CountVectorizer(ngram_range=(1,3))\n",
    "train_X = vectorizer.fit_transform(train_X)\n",
    "test_X = vectorizer.transform(test_X)\n",
    "\n",
    "# Fit classifier\n",
    "clf = MultinomialNB()\n",
    "clf.fit(train_X, train_y)\n",
    "\n",
    "# Print accuracy, time and number of dimensions\n",
    "print(\"The program took %.3f seconds to complete. The accuracy on the test set is %.2f. The ngram representation had %i features.\" % (time.time() - start_time, clf.score(test_X, test_y), train_X.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result**\n",
    "Unigram:\n",
    "The program took 0.194 seconds to complete. The accuracy on the test set is 0.75. The ngram representation had 12347 features.\n",
    "\n",
    "N-Gram (up to 3)\n",
    "The program took 3.605 seconds to complete. The accuracy on the test set is 0.77. The ngram representation had 178240 features.\n",
    "\n",
    "**Summary**\n",
    "Amazing work! The program took around 0.2 seconds in the case of the unigram model and more than 10 times longer for the higher order n-gram model. The unigram model had over 12,000 features whereas the n-gram model for upto n=3 had over 178,000! Despite taking higher computation time and generating more features, the classifier only performs marginally better in the latter case, producing an accuracy of 77% in comparison to the 75% for the unigram model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. TF-IDF and similarity scores\n",
    "Learn how to compute tf-idf weights and the cosine similarity score between two vectors. You will use these concepts to build a movie and a TED Talk recommender. Finally, you will also learn about word embeddings and using word vector representations, you will compute similarities between various Pink Floyd songs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buildin tf-idf document vectors\n",
    "#### n-gram modeling\n",
    "- weight of dimension dependent on the frequency of the word corresponding to the dimension.\n",
    "  - Document contain the word `human` in five places.\n",
    "  - Dimension corresponding to `human` has weight `5`\n",
    "- Some words occur very commonly across all documents\n",
    "- Corpus of documents on the univese\n",
    "  - One document has `jupiter` and `universe` occuring 20 times each.\n",
    "  - `jupiter` rarely occure in the other documents, `universe` is common\n",
    "  - Give more weight to `jupiter` on account of exclusivity (jupiter characterize document more than universe).\n",
    "\n",
    "#### Applications\n",
    "- automatically detect stopwords\n",
    "- search algorithm\n",
    "- recommender systems\n",
    "- better performance in predictive modeling for some cases\n",
    "\n",
    "#### Term frequency-inverse document frequency (td-idf)\n",
    "- Proportional to term frequency\n",
    "- Inverse function of the number of documents in which it occurs\n",
    "- Higher td-idf - more importan word characterize it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![id](images/td-idf-formula.png \"Tf-Idf mathematical formula\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf-idf using scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.25634672 0.3251433  0.3251433\n",
      "  0.         0.25634672 0.         0.         0.         0.25634672\n",
      "  0.         0.         0.76904015]\n",
      " [0.         0.         0.         0.         0.46516193 0.\n",
      "  0.         0.         0.46516193 0.         0.         0.\n",
      "  0.46516193 0.         0.46516193 0.         0.         0.36673901\n",
      "  0.         0.         0.        ]\n",
      " [0.4533864  0.         0.         0.         0.         0.4533864\n",
      "  0.         0.         0.         0.35745504 0.         0.\n",
      "  0.         0.35745504 0.         0.         0.         0.\n",
      "  0.         0.4533864  0.35745504]\n",
      " [0.         0.24253563 0.24253563 0.24253563 0.         0.\n",
      "  0.24253563 0.48507125 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.48507125 0.48507125 0.\n",
      "  0.24253563 0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = [\n",
    "    'The lion is the king of the jungle',\n",
    "    'Lions have lifespans of a decade',\n",
    "    'The lion is an endangered species',\n",
    "    'men may come and men may go but i go on forever'\n",
    "]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "print(tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: tf-idf weight of commonly occurring words\n",
    "The word bottle occurs 5 times in a particular document D and also occurs in every document of the corpus. What is the tf-idf weight of bottle in D?\n",
    "\n",
    "Answer:\n",
    "In fact, the tf-idf weight for bottle in every document will be 0. This is because the inverse document frequency is constant across documents in a corpus and since bottle occurs in every document, its value is log(1), which is 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exersise: tf-idf vectors for TED talks\n",
    "In this exercise, you have been given a corpus ted which contains the transcripts of 500 TED Talks. Your task is to generate the tf-idf vectors for these talks.\n",
    "\n",
    "In a later lesson, we will use these vectors to generate recommendations of similar talks based on the transcript. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Good morning. How are you?(Laughter)It's been ...</td>\n",
       "      <td>https://www.ted.com/talks/ken_robinson_says_sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Thank you so much, Chris. And it's truly a gre...</td>\n",
       "      <td>https://www.ted.com/talks/al_gore_on_averting_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>(Music: \"The Sound of Silence,\" Simon &amp; Garfun...</td>\n",
       "      <td>https://www.ted.com/talks/david_pogue_says_sim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>If you're here today — and I'm very happy that...</td>\n",
       "      <td>https://www.ted.com/talks/majora_carter_s_tale...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>About 10 years ago, I took on the task to teac...</td>\n",
       "      <td>https://www.ted.com/talks/hans_rosling_shows_t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          transcript  \\\n",
       "0  Good morning. How are you?(Laughter)It's been ...   \n",
       "1  Thank you so much, Chris. And it's truly a gre...   \n",
       "2  (Music: \"The Sound of Silence,\" Simon & Garfun...   \n",
       "3  If you're here today — and I'm very happy that...   \n",
       "4  About 10 years ago, I took on the task to teac...   \n",
       "\n",
       "                                                 url  \n",
       "0  https://www.ted.com/talks/ken_robinson_says_sc...  \n",
       "1  https://www.ted.com/talks/al_gore_on_averting_...  \n",
       "2  https://www.ted.com/talks/david_pogue_says_sim...  \n",
       "3  https://www.ted.com/talks/majora_carter_s_tale...  \n",
       "4  https://www.ted.com/talks/hans_rosling_shows_t...  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../dataset/ted_talks/transcripts.csv', header=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2467, 58795)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../dataset/ted_talks/transcripts.csv', header=0)\n",
    "\n",
    "ted = df['transcript']\n",
    "\n",
    "# Import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create TfidfVectorizer object\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Generate matrix of word vectors\n",
    "tfidf_matrix = vectorizer.fit_transform(ted)\n",
    "\n",
    "# Print the shape of tfidf_matrix\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![id](images/cosine-similarity.png \"Cosine similarity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The dot product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![id](images/dot-product.png \"Cosine similarity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![id](images/magnitude-of-vector.png \"Magnitude of vector\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![id](images/cosine-score.png \"Cosine score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cosine Score: points to rememeber\n",
    "- Value between -1 and 1\n",
    "- In NLP document vector use value between 0 and 1 (0 - no similarity, 1 - identical)\n",
    "- Robust to document length bcs ignore vector magnitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation using scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.73881883]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# Define two 3- dimensional vectors A and B\n",
    "A = (4, 7, 1)\n",
    "B = (5, 2, 3)\n",
    "\n",
    "# Compute the cosine score of A and B\n",
    "score = cosine_similarity([A], [B])\n",
    "\n",
    "# Print the cosine score\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Computing dot product\n",
    "In this exercise, we will learn to compute the dot product between two vectors, A = (1, 3) and B = (-2, 2), using the numpy library. More specifically, we will use the np.dot() function to compute the dot product of two numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "# Initialize numpy vectors\n",
    "A = np.array([1, 3])\n",
    "B = np.array([-2, 2])\n",
    "\n",
    "# Compute dot product\n",
    "dot_prod = np.dot(A, B)\n",
    "\n",
    "# Print dot product\n",
    "print(dot_prod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dot product of the two vectors is 1 * -2 + 3 * 2 = 4, which is indeed the output produced. We will not be using np.dot() too much in this course but it can prove to be a helpful function while computing dot products between two standalone vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Cosine similarity matrix of a corpus\n",
    "In this exercise, you have been given a corpus, which is a list containing five sentences. The corpus is printed in the console. You have to compute the cosine similarity matrix which contains the pairwise cosine similarity score for every pair of sentences (vectorized using tf-idf).\n",
    "\n",
    "Remember, the value corresponding to the ith row and jth column of a similarity matrix denotes the similarity score for the ith and jth vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.36413198 0.18314713 0.18435251 0.16336438]\n",
      " [0.36413198 1.         0.15054075 0.21704584 0.11203887]\n",
      " [0.18314713 0.15054075 1.         0.21318602 0.07763512]\n",
      " [0.18435251 0.21704584 0.21318602 1.         0.12960089]\n",
      " [0.16336438 0.11203887 0.07763512 0.12960089 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = ['The sun is the largest celestial body in the solar system',\n",
    "          'The solar system consists of the sun and eight revolving planets',\n",
    "          'Ra was the Egyptian Sun God',\n",
    "          'The Pyramids were the pinnacle of Egyptian architecture',\n",
    "          'The quick brown fox jumps over the lazy dog'\n",
    "         ]\n",
    "# Initialize an instance of tf-idf Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Generate the tf-idf vectors for the corpus\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Compute and print the cosine similarity matrix\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "print(cosine_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you will see in a subsequent lesson, computing the cosine similarity matrix lies at the heart of many practical systems such as recommenders. From our similarity matrix, we see that the first and the second sentence are the most similar. Also the fifth sentence has, on average, the lowest pairwise cosine scores. This is intuitive as it contains entities that are not present in the other sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a plot line based recommender\n",
    "#### Movie recommender\n",
    "\n",
    "| Title        | Overview                                                                                                                                                                                                                                                                                        |\n",
    "|--------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| Plot         | Astronauts who have seen the Earth from space have often described the 'Overview Effect',  an experience that has transformed their perspective of the planet and mankind's place  upon it, and enabled them to perceive it as our shared home, without boundaries between  nations or species. |\n",
    "| Interstellar | A team of explorers travel through a wormhole in space in an attempt to ensure humanity's survival.                                                                                                                                                                                             |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task\n",
    "Build a system which takes a movie title and outputs the list of moviews which has similar plot line.\n",
    "\n",
    "#### Steps\n",
    "1. Preprocess movie overview\n",
    "2. Generate tf-idf vectors\n",
    "3. Generate cosine similarity matrix\n",
    "\n",
    "#### Recommender function\n",
    "1. Takes a movie title, cosine similarity matrix and indices series as arguments.\n",
    "2. Extract pairwise cosine similarity scores for the movie.\n",
    "3. Sort the score in descending order.\n",
    "4. Output title corresponding to the highest scores.\n",
    "5. Ignore the highest similarity score (of 1 bcs move which more similar is movie itself)\n",
    "\n",
    "#### Generating tf-idf vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.36413198 0.18314713 0.18435251 0.16336438]\n",
      " [0.36413198 1.         0.15054075 0.21704584 0.11203887]\n",
      " [0.18314713 0.15054075 1.         0.21318602 0.07763512]\n",
      " [0.18435251 0.21704584 0.21318602 1.         0.12960089]\n",
      " [0.16336438 0.11203887 0.07763512 0.12960089 1.        ]]\n",
      "[[1.         0.36413198 0.18314713 0.18435251 0.16336438]\n",
      " [0.36413198 1.         0.15054075 0.21704584 0.11203887]\n",
      " [0.18314713 0.15054075 1.         0.21318602 0.07763512]\n",
      " [0.18435251 0.21704584 0.21318602 1.         0.12960089]\n",
      " [0.16336438 0.11203887 0.07763512 0.12960089 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "movie_plots = ['The sun is the largest celestial body in the solar system',\n",
    "          'The solar system consists of the sun and eight revolving planets',\n",
    "          'Ra was the Egyptian Sun God',\n",
    "          'The Pyramids were the pinnacle of Egyptian architecture',\n",
    "          'The quick brown fox jumps over the lazy dog'\n",
    "         ]\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(movie_plots)\n",
    "\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "print(cosine_sim)\n",
    "\n",
    "# Result the same but should take less time to compute\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "print(cosine_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The linear_kernel function\n",
    "- Magnitude of tf-idf vector is 1\n",
    "- Consine score between two tf-idf vectors is their dot product.\n",
    "- Can significantly improve computation time\n",
    "- There for we can use `linear_kernel` instead of `cosine_similarity`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Comparing linear_kernel and cosine_similarity\n",
    "In this exercise, you have been given tfidf_matrix which contains the tf-idf vectors of a thousand documents. Your task is to generate the cosine similarity matrix for these vectors first using cosine_similarity and then, using linear_kernel.\n",
    "\n",
    "We will then compare the computation times for both functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.36413198 0.18314713 0.18435251 0.16336438]\n",
      " [0.36413198 1.         0.15054075 0.21704584 0.11203887]\n",
      " [0.18314713 0.15054075 1.         0.21318602 0.07763512]\n",
      " [0.18435251 0.21704584 0.21318602 1.         0.12960089]\n",
      " [0.16336438 0.11203887 0.07763512 0.12960089 1.        ]]\n",
      "Time taken: 0.014595508575439453 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "movie_plots = ['The sun is the largest celestial body in the solar system',\n",
    "          'The solar system consists of the sun and eight revolving planets',\n",
    "          'Ra was the Egyptian Sun God',\n",
    "          'The Pyramids were the pinnacle of Egyptian architecture',\n",
    "          'The quick brown fox jumps over the lazy dog'\n",
    "         ]\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(movie_plots)\n",
    "\n",
    "# Record start time\n",
    "start = time.time()\n",
    "\n",
    "# Compute cosine similarity matrix\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Print cosine similarity matrix\n",
    "print(cosine_sim)\n",
    "\n",
    "# Print time taken\n",
    "print(\"Time taken: %s seconds\" %(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.36413198 0.18314713 0.18435251 0.16336438]\n",
      " [0.36413198 1.         0.15054075 0.21704584 0.11203887]\n",
      " [0.18314713 0.15054075 1.         0.21318602 0.07763512]\n",
      " [0.18435251 0.21704584 0.21318602 1.         0.12960089]\n",
      " [0.16336438 0.11203887 0.07763512 0.12960089 1.        ]]\n",
      "Time taken: 0.003167390823364258 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "movie_plots = ['The sun is the largest celestial body in the solar system',\n",
    "          'The solar system consists of the sun and eight revolving planets',\n",
    "          'Ra was the Egyptian Sun God',\n",
    "          'The Pyramids were the pinnacle of Egyptian architecture',\n",
    "          'The quick brown fox jumps over the lazy dog'\n",
    "         ]\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(movie_plots)\n",
    "\n",
    "# Record start time\n",
    "start = time.time()\n",
    "\n",
    "# Compute cosine similarity matrix\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Print cosine similarity matrix\n",
    "print(cosine_sim)\n",
    "\n",
    "# Print time taken\n",
    "print(\"Time taken: %s seconds\" %(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result**\n",
    "Notice how both linear_kernel and cosine_similarity produced the same result. However, linear_kernel took a smaller amount of time to execute. When you're working with a very large amount of data and your vectors are in the tf-idf representation, it is good practice to default to linear_kernel to improve performance. (NOTE: In case, you see linear_kernel taking more time, it's because the dataset we're dealing with is extremely small and Python's time module is incapable of capture such minute time differences accurately)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Plot recommendation engine\n",
    "In this exercise, we will build a recommendation engine that suggests movies based on similarity of plot lines. You have been given a get_recommendations() function that takes in the title of a movie, a similarity matrix and an indices series as its arguments and outputs a list of most similar movies. indices has already been provided to you.\n",
    "\n",
    "You have also been given a movie_plots Series that contains the plot lines of several movies. Your task is to generate a cosine similarity matrix for the tf-idf vectors of these plots.\n",
    "\n",
    "Consequently, we will check the potency of our engine by generating recommendations for one of my favorite movies, The Dark Knight Rises.\n",
    "\n",
    "Data [\"Movie Data Kaggle\"](https://www.kaggle.com/kokur123/analysis-of-movie-data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>budget</th>\n",
       "      <th>genres</th>\n",
       "      <th>homepage</th>\n",
       "      <th>id</th>\n",
       "      <th>keywords</th>\n",
       "      <th>original_language</th>\n",
       "      <th>original_title</th>\n",
       "      <th>overview</th>\n",
       "      <th>popularity</th>\n",
       "      <th>production_companies</th>\n",
       "      <th>production_countries</th>\n",
       "      <th>release_date</th>\n",
       "      <th>revenue</th>\n",
       "      <th>runtime</th>\n",
       "      <th>spoken_languages</th>\n",
       "      <th>status</th>\n",
       "      <th>tagline</th>\n",
       "      <th>title</th>\n",
       "      <th>vote_average</th>\n",
       "      <th>vote_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>237000000</td>\n",
       "      <td>[{\"id\": 28, \"name\": \"Action\"}, {\"id\": 12, \"nam...</td>\n",
       "      <td>http://www.avatarmovie.com/</td>\n",
       "      <td>19995</td>\n",
       "      <td>[{\"id\": 1463, \"name\": \"culture clash\"}, {\"id\":...</td>\n",
       "      <td>en</td>\n",
       "      <td>Avatar</td>\n",
       "      <td>In the 22nd century, a paraplegic Marine is di...</td>\n",
       "      <td>150.437577</td>\n",
       "      <td>[{\"name\": \"Ingenious Film Partners\", \"id\": 289...</td>\n",
       "      <td>[{\"iso_3166_1\": \"US\", \"name\": \"United States o...</td>\n",
       "      <td>2009-12-10</td>\n",
       "      <td>2787965087</td>\n",
       "      <td>162.0</td>\n",
       "      <td>[{\"iso_639_1\": \"en\", \"name\": \"English\"}, {\"iso...</td>\n",
       "      <td>Released</td>\n",
       "      <td>Enter the World of Pandora.</td>\n",
       "      <td>Avatar</td>\n",
       "      <td>7.2</td>\n",
       "      <td>11800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>300000000</td>\n",
       "      <td>[{\"id\": 12, \"name\": \"Adventure\"}, {\"id\": 14, \"...</td>\n",
       "      <td>http://disney.go.com/disneypictures/pirates/</td>\n",
       "      <td>285</td>\n",
       "      <td>[{\"id\": 270, \"name\": \"ocean\"}, {\"id\": 726, \"na...</td>\n",
       "      <td>en</td>\n",
       "      <td>Pirates of the Caribbean: At World's End</td>\n",
       "      <td>Captain Barbossa, long believed to be dead, ha...</td>\n",
       "      <td>139.082615</td>\n",
       "      <td>[{\"name\": \"Walt Disney Pictures\", \"id\": 2}, {\"...</td>\n",
       "      <td>[{\"iso_3166_1\": \"US\", \"name\": \"United States o...</td>\n",
       "      <td>2007-05-19</td>\n",
       "      <td>961000000</td>\n",
       "      <td>169.0</td>\n",
       "      <td>[{\"iso_639_1\": \"en\", \"name\": \"English\"}]</td>\n",
       "      <td>Released</td>\n",
       "      <td>At the end of the world, the adventure begins.</td>\n",
       "      <td>Pirates of the Caribbean: At World's End</td>\n",
       "      <td>6.9</td>\n",
       "      <td>4500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>245000000</td>\n",
       "      <td>[{\"id\": 28, \"name\": \"Action\"}, {\"id\": 12, \"nam...</td>\n",
       "      <td>http://www.sonypictures.com/movies/spectre/</td>\n",
       "      <td>206647</td>\n",
       "      <td>[{\"id\": 470, \"name\": \"spy\"}, {\"id\": 818, \"name...</td>\n",
       "      <td>en</td>\n",
       "      <td>Spectre</td>\n",
       "      <td>A cryptic message from Bond’s past sends him o...</td>\n",
       "      <td>107.376788</td>\n",
       "      <td>[{\"name\": \"Columbia Pictures\", \"id\": 5}, {\"nam...</td>\n",
       "      <td>[{\"iso_3166_1\": \"GB\", \"name\": \"United Kingdom\"...</td>\n",
       "      <td>2015-10-26</td>\n",
       "      <td>880674609</td>\n",
       "      <td>148.0</td>\n",
       "      <td>[{\"iso_639_1\": \"fr\", \"name\": \"Fran\\u00e7ais\"},...</td>\n",
       "      <td>Released</td>\n",
       "      <td>A Plan No One Escapes</td>\n",
       "      <td>Spectre</td>\n",
       "      <td>6.3</td>\n",
       "      <td>4466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>250000000</td>\n",
       "      <td>[{\"id\": 28, \"name\": \"Action\"}, {\"id\": 80, \"nam...</td>\n",
       "      <td>http://www.thedarkknightrises.com/</td>\n",
       "      <td>49026</td>\n",
       "      <td>[{\"id\": 849, \"name\": \"dc comics\"}, {\"id\": 853,...</td>\n",
       "      <td>en</td>\n",
       "      <td>The Dark Knight Rises</td>\n",
       "      <td>Following the death of District Attorney Harve...</td>\n",
       "      <td>112.312950</td>\n",
       "      <td>[{\"name\": \"Legendary Pictures\", \"id\": 923}, {\"...</td>\n",
       "      <td>[{\"iso_3166_1\": \"US\", \"name\": \"United States o...</td>\n",
       "      <td>2012-07-16</td>\n",
       "      <td>1084939099</td>\n",
       "      <td>165.0</td>\n",
       "      <td>[{\"iso_639_1\": \"en\", \"name\": \"English\"}]</td>\n",
       "      <td>Released</td>\n",
       "      <td>The Legend Ends</td>\n",
       "      <td>The Dark Knight Rises</td>\n",
       "      <td>7.6</td>\n",
       "      <td>9106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>260000000</td>\n",
       "      <td>[{\"id\": 28, \"name\": \"Action\"}, {\"id\": 12, \"nam...</td>\n",
       "      <td>http://movies.disney.com/john-carter</td>\n",
       "      <td>49529</td>\n",
       "      <td>[{\"id\": 818, \"name\": \"based on novel\"}, {\"id\":...</td>\n",
       "      <td>en</td>\n",
       "      <td>John Carter</td>\n",
       "      <td>John Carter is a war-weary, former military ca...</td>\n",
       "      <td>43.926995</td>\n",
       "      <td>[{\"name\": \"Walt Disney Pictures\", \"id\": 2}]</td>\n",
       "      <td>[{\"iso_3166_1\": \"US\", \"name\": \"United States o...</td>\n",
       "      <td>2012-03-07</td>\n",
       "      <td>284139100</td>\n",
       "      <td>132.0</td>\n",
       "      <td>[{\"iso_639_1\": \"en\", \"name\": \"English\"}]</td>\n",
       "      <td>Released</td>\n",
       "      <td>Lost in our world, found in another.</td>\n",
       "      <td>John Carter</td>\n",
       "      <td>6.1</td>\n",
       "      <td>2124</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      budget                                             genres  \\\n",
       "0  237000000  [{\"id\": 28, \"name\": \"Action\"}, {\"id\": 12, \"nam...   \n",
       "1  300000000  [{\"id\": 12, \"name\": \"Adventure\"}, {\"id\": 14, \"...   \n",
       "2  245000000  [{\"id\": 28, \"name\": \"Action\"}, {\"id\": 12, \"nam...   \n",
       "3  250000000  [{\"id\": 28, \"name\": \"Action\"}, {\"id\": 80, \"nam...   \n",
       "4  260000000  [{\"id\": 28, \"name\": \"Action\"}, {\"id\": 12, \"nam...   \n",
       "\n",
       "                                       homepage      id  \\\n",
       "0                   http://www.avatarmovie.com/   19995   \n",
       "1  http://disney.go.com/disneypictures/pirates/     285   \n",
       "2   http://www.sonypictures.com/movies/spectre/  206647   \n",
       "3            http://www.thedarkknightrises.com/   49026   \n",
       "4          http://movies.disney.com/john-carter   49529   \n",
       "\n",
       "                                            keywords original_language  \\\n",
       "0  [{\"id\": 1463, \"name\": \"culture clash\"}, {\"id\":...                en   \n",
       "1  [{\"id\": 270, \"name\": \"ocean\"}, {\"id\": 726, \"na...                en   \n",
       "2  [{\"id\": 470, \"name\": \"spy\"}, {\"id\": 818, \"name...                en   \n",
       "3  [{\"id\": 849, \"name\": \"dc comics\"}, {\"id\": 853,...                en   \n",
       "4  [{\"id\": 818, \"name\": \"based on novel\"}, {\"id\":...                en   \n",
       "\n",
       "                             original_title  \\\n",
       "0                                    Avatar   \n",
       "1  Pirates of the Caribbean: At World's End   \n",
       "2                                   Spectre   \n",
       "3                     The Dark Knight Rises   \n",
       "4                               John Carter   \n",
       "\n",
       "                                            overview  popularity  \\\n",
       "0  In the 22nd century, a paraplegic Marine is di...  150.437577   \n",
       "1  Captain Barbossa, long believed to be dead, ha...  139.082615   \n",
       "2  A cryptic message from Bond’s past sends him o...  107.376788   \n",
       "3  Following the death of District Attorney Harve...  112.312950   \n",
       "4  John Carter is a war-weary, former military ca...   43.926995   \n",
       "\n",
       "                                production_companies  \\\n",
       "0  [{\"name\": \"Ingenious Film Partners\", \"id\": 289...   \n",
       "1  [{\"name\": \"Walt Disney Pictures\", \"id\": 2}, {\"...   \n",
       "2  [{\"name\": \"Columbia Pictures\", \"id\": 5}, {\"nam...   \n",
       "3  [{\"name\": \"Legendary Pictures\", \"id\": 923}, {\"...   \n",
       "4        [{\"name\": \"Walt Disney Pictures\", \"id\": 2}]   \n",
       "\n",
       "                                production_countries release_date     revenue  \\\n",
       "0  [{\"iso_3166_1\": \"US\", \"name\": \"United States o...   2009-12-10  2787965087   \n",
       "1  [{\"iso_3166_1\": \"US\", \"name\": \"United States o...   2007-05-19   961000000   \n",
       "2  [{\"iso_3166_1\": \"GB\", \"name\": \"United Kingdom\"...   2015-10-26   880674609   \n",
       "3  [{\"iso_3166_1\": \"US\", \"name\": \"United States o...   2012-07-16  1084939099   \n",
       "4  [{\"iso_3166_1\": \"US\", \"name\": \"United States o...   2012-03-07   284139100   \n",
       "\n",
       "   runtime                                   spoken_languages    status  \\\n",
       "0    162.0  [{\"iso_639_1\": \"en\", \"name\": \"English\"}, {\"iso...  Released   \n",
       "1    169.0           [{\"iso_639_1\": \"en\", \"name\": \"English\"}]  Released   \n",
       "2    148.0  [{\"iso_639_1\": \"fr\", \"name\": \"Fran\\u00e7ais\"},...  Released   \n",
       "3    165.0           [{\"iso_639_1\": \"en\", \"name\": \"English\"}]  Released   \n",
       "4    132.0           [{\"iso_639_1\": \"en\", \"name\": \"English\"}]  Released   \n",
       "\n",
       "                                          tagline  \\\n",
       "0                     Enter the World of Pandora.   \n",
       "1  At the end of the world, the adventure begins.   \n",
       "2                           A Plan No One Escapes   \n",
       "3                                 The Legend Ends   \n",
       "4            Lost in our world, found in another.   \n",
       "\n",
       "                                      title  vote_average  vote_count  \n",
       "0                                    Avatar           7.2       11800  \n",
       "1  Pirates of the Caribbean: At World's End           6.9        4500  \n",
       "2                                   Spectre           6.3        4466  \n",
       "3                     The Dark Knight Rises           7.6        9106  \n",
       "4                               John Carter           6.1        2124  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../dataset/tmdb_movie/tmdb_5000_movies.csv', header=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65                              The Dark Knight\n",
      "299                              Batman Forever\n",
      "428                              Batman Returns\n",
      "1359                                     Batman\n",
      "3854    Batman: The Dark Knight Returns, Part 2\n",
      "119                               Batman Begins\n",
      "2507                                  Slow Burn\n",
      "9            Batman v Superman: Dawn of Justice\n",
      "1181                                        JFK\n",
      "210                              Batman & Robin\n",
      "Name: title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../dataset/tmdb_movie/tmdb_5000_movies.csv', header=0, keep_default_na = False)\n",
    "\n",
    "# Generate mapping between titles and index\n",
    "indices = pd.Series(df.index, index=df['title']).drop_duplicates()\n",
    "\n",
    "def get_recommendations(title, cosine_sim, indices):\n",
    "    # Get index of movie that matches title\n",
    "    idx = indices[title]\n",
    "    # Sort the movies based on the similarity scores\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    # Get the scores for 10 most similar movies\n",
    "    sim_scores = sim_scores[1:11]\n",
    "    # Get the movie indices\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "    # Return the top 10 most similar movies\n",
    "    return df['title'].iloc[movie_indices]\n",
    "\n",
    "movie_plots = df['overview']\n",
    "\n",
    "# Initialize the TfidfVectorizer \n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# Construct the TF-IDF matrix\n",
    "tfidf_matrix = tfidf.fit_transform(movie_plots)\n",
    "\n",
    "# Generate the cosine similarity matrix\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    " \n",
    "# Generate recommendations \n",
    "print(get_recommendations('The Dark Knight Rises', cosine_sim, indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: TED talk recommender\n",
    "In this exercise, we will build a recommendation system that suggests TED Talks based on their transcripts. You have been given a get_recommendations() function that takes in the title of a talk, a similarity matrix and an indices series as its arguments, and outputs a list of most similar talks. indices has already been provided to you.\n",
    "\n",
    "You have also been given a transcripts series that contains the transcripts of around 500 TED talks. Your task is to generate a cosine similarity matrix for the tf-idf vectors of the talk transcripts.\n",
    "\n",
    "Consequently, we will generate recommendations for a talk titled '5 ways to kill your dreams' by Brazilian entrepreneur Bel Pesce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>description</th>\n",
       "      <th>duration</th>\n",
       "      <th>event</th>\n",
       "      <th>film_date</th>\n",
       "      <th>languages</th>\n",
       "      <th>main_speaker</th>\n",
       "      <th>name</th>\n",
       "      <th>num_speaker</th>\n",
       "      <th>published_date</th>\n",
       "      <th>ratings</th>\n",
       "      <th>related_talks</th>\n",
       "      <th>speaker_occupation</th>\n",
       "      <th>tags</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>views</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4553</td>\n",
       "      <td>Sir Ken Robinson makes an entertaining and pro...</td>\n",
       "      <td>1164</td>\n",
       "      <td>TED2006</td>\n",
       "      <td>1140825600</td>\n",
       "      <td>60</td>\n",
       "      <td>Ken Robinson</td>\n",
       "      <td>Ken Robinson: Do schools kill creativity?</td>\n",
       "      <td>1</td>\n",
       "      <td>1151367060</td>\n",
       "      <td>[{'id': 7, 'name': 'Funny', 'count': 19645}, {...</td>\n",
       "      <td>[{'id': 865, 'hero': 'https://pe.tedcdn.com/im...</td>\n",
       "      <td>Author/educator</td>\n",
       "      <td>['children', 'creativity', 'culture', 'dance',...</td>\n",
       "      <td>Do schools kill creativity?</td>\n",
       "      <td>https://www.ted.com/talks/ken_robinson_says_sc...</td>\n",
       "      <td>47227110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>265</td>\n",
       "      <td>With the same humor and humanity he exuded in ...</td>\n",
       "      <td>977</td>\n",
       "      <td>TED2006</td>\n",
       "      <td>1140825600</td>\n",
       "      <td>43</td>\n",
       "      <td>Al Gore</td>\n",
       "      <td>Al Gore: Averting the climate crisis</td>\n",
       "      <td>1</td>\n",
       "      <td>1151367060</td>\n",
       "      <td>[{'id': 7, 'name': 'Funny', 'count': 544}, {'i...</td>\n",
       "      <td>[{'id': 243, 'hero': 'https://pe.tedcdn.com/im...</td>\n",
       "      <td>Climate advocate</td>\n",
       "      <td>['alternative energy', 'cars', 'climate change...</td>\n",
       "      <td>Averting the climate crisis</td>\n",
       "      <td>https://www.ted.com/talks/al_gore_on_averting_...</td>\n",
       "      <td>3200520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>124</td>\n",
       "      <td>New York Times columnist David Pogue takes aim...</td>\n",
       "      <td>1286</td>\n",
       "      <td>TED2006</td>\n",
       "      <td>1140739200</td>\n",
       "      <td>26</td>\n",
       "      <td>David Pogue</td>\n",
       "      <td>David Pogue: Simplicity sells</td>\n",
       "      <td>1</td>\n",
       "      <td>1151367060</td>\n",
       "      <td>[{'id': 7, 'name': 'Funny', 'count': 964}, {'i...</td>\n",
       "      <td>[{'id': 1725, 'hero': 'https://pe.tedcdn.com/i...</td>\n",
       "      <td>Technology columnist</td>\n",
       "      <td>['computers', 'entertainment', 'interface desi...</td>\n",
       "      <td>Simplicity sells</td>\n",
       "      <td>https://www.ted.com/talks/david_pogue_says_sim...</td>\n",
       "      <td>1636292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>In an emotionally charged talk, MacArthur-winn...</td>\n",
       "      <td>1116</td>\n",
       "      <td>TED2006</td>\n",
       "      <td>1140912000</td>\n",
       "      <td>35</td>\n",
       "      <td>Majora Carter</td>\n",
       "      <td>Majora Carter: Greening the ghetto</td>\n",
       "      <td>1</td>\n",
       "      <td>1151367060</td>\n",
       "      <td>[{'id': 3, 'name': 'Courageous', 'count': 760}...</td>\n",
       "      <td>[{'id': 1041, 'hero': 'https://pe.tedcdn.com/i...</td>\n",
       "      <td>Activist for environmental justice</td>\n",
       "      <td>['MacArthur grant', 'activism', 'business', 'c...</td>\n",
       "      <td>Greening the ghetto</td>\n",
       "      <td>https://www.ted.com/talks/majora_carter_s_tale...</td>\n",
       "      <td>1697550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>593</td>\n",
       "      <td>You've never seen data presented like this. Wi...</td>\n",
       "      <td>1190</td>\n",
       "      <td>TED2006</td>\n",
       "      <td>1140566400</td>\n",
       "      <td>48</td>\n",
       "      <td>Hans Rosling</td>\n",
       "      <td>Hans Rosling: The best stats you've ever seen</td>\n",
       "      <td>1</td>\n",
       "      <td>1151440680</td>\n",
       "      <td>[{'id': 9, 'name': 'Ingenious', 'count': 3202}...</td>\n",
       "      <td>[{'id': 2056, 'hero': 'https://pe.tedcdn.com/i...</td>\n",
       "      <td>Global health expert; data visionary</td>\n",
       "      <td>['Africa', 'Asia', 'Google', 'demo', 'economic...</td>\n",
       "      <td>The best stats you've ever seen</td>\n",
       "      <td>https://www.ted.com/talks/hans_rosling_shows_t...</td>\n",
       "      <td>12005869</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   comments                                        description  duration  \\\n",
       "0      4553  Sir Ken Robinson makes an entertaining and pro...      1164   \n",
       "1       265  With the same humor and humanity he exuded in ...       977   \n",
       "2       124  New York Times columnist David Pogue takes aim...      1286   \n",
       "3       200  In an emotionally charged talk, MacArthur-winn...      1116   \n",
       "4       593  You've never seen data presented like this. Wi...      1190   \n",
       "\n",
       "     event   film_date  languages   main_speaker  \\\n",
       "0  TED2006  1140825600         60   Ken Robinson   \n",
       "1  TED2006  1140825600         43        Al Gore   \n",
       "2  TED2006  1140739200         26    David Pogue   \n",
       "3  TED2006  1140912000         35  Majora Carter   \n",
       "4  TED2006  1140566400         48   Hans Rosling   \n",
       "\n",
       "                                            name  num_speaker  published_date  \\\n",
       "0      Ken Robinson: Do schools kill creativity?            1      1151367060   \n",
       "1           Al Gore: Averting the climate crisis            1      1151367060   \n",
       "2                  David Pogue: Simplicity sells            1      1151367060   \n",
       "3             Majora Carter: Greening the ghetto            1      1151367060   \n",
       "4  Hans Rosling: The best stats you've ever seen            1      1151440680   \n",
       "\n",
       "                                             ratings  \\\n",
       "0  [{'id': 7, 'name': 'Funny', 'count': 19645}, {...   \n",
       "1  [{'id': 7, 'name': 'Funny', 'count': 544}, {'i...   \n",
       "2  [{'id': 7, 'name': 'Funny', 'count': 964}, {'i...   \n",
       "3  [{'id': 3, 'name': 'Courageous', 'count': 760}...   \n",
       "4  [{'id': 9, 'name': 'Ingenious', 'count': 3202}...   \n",
       "\n",
       "                                       related_talks  \\\n",
       "0  [{'id': 865, 'hero': 'https://pe.tedcdn.com/im...   \n",
       "1  [{'id': 243, 'hero': 'https://pe.tedcdn.com/im...   \n",
       "2  [{'id': 1725, 'hero': 'https://pe.tedcdn.com/i...   \n",
       "3  [{'id': 1041, 'hero': 'https://pe.tedcdn.com/i...   \n",
       "4  [{'id': 2056, 'hero': 'https://pe.tedcdn.com/i...   \n",
       "\n",
       "                     speaker_occupation  \\\n",
       "0                       Author/educator   \n",
       "1                      Climate advocate   \n",
       "2                  Technology columnist   \n",
       "3    Activist for environmental justice   \n",
       "4  Global health expert; data visionary   \n",
       "\n",
       "                                                tags  \\\n",
       "0  ['children', 'creativity', 'culture', 'dance',...   \n",
       "1  ['alternative energy', 'cars', 'climate change...   \n",
       "2  ['computers', 'entertainment', 'interface desi...   \n",
       "3  ['MacArthur grant', 'activism', 'business', 'c...   \n",
       "4  ['Africa', 'Asia', 'Google', 'demo', 'economic...   \n",
       "\n",
       "                             title  \\\n",
       "0      Do schools kill creativity?   \n",
       "1      Averting the climate crisis   \n",
       "2                 Simplicity sells   \n",
       "3              Greening the ghetto   \n",
       "4  The best stats you've ever seen   \n",
       "\n",
       "                                                 url     views  \n",
       "0  https://www.ted.com/talks/ken_robinson_says_sc...  47227110  \n",
       "1  https://www.ted.com/talks/al_gore_on_averting_...   3200520  \n",
       "2  https://www.ted.com/talks/david_pogue_says_sim...   1636292  \n",
       "3  https://www.ted.com/talks/majora_carter_s_tale...   1697550  \n",
       "4  https://www.ted.com/talks/hans_rosling_shows_t...  12005869  "
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../dataset/ted_talks/ted_main.csv', header=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>description</th>\n",
       "      <th>duration</th>\n",
       "      <th>event</th>\n",
       "      <th>film_date</th>\n",
       "      <th>languages</th>\n",
       "      <th>main_speaker</th>\n",
       "      <th>name</th>\n",
       "      <th>num_speaker</th>\n",
       "      <th>published_date</th>\n",
       "      <th>ratings</th>\n",
       "      <th>related_talks</th>\n",
       "      <th>speaker_occupation</th>\n",
       "      <th>tags</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>views</th>\n",
       "      <th>transcript</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4553</td>\n",
       "      <td>Sir Ken Robinson makes an entertaining and pro...</td>\n",
       "      <td>1164</td>\n",
       "      <td>TED2006</td>\n",
       "      <td>1140825600</td>\n",
       "      <td>60</td>\n",
       "      <td>Ken Robinson</td>\n",
       "      <td>Ken Robinson: Do schools kill creativity?</td>\n",
       "      <td>1</td>\n",
       "      <td>1151367060</td>\n",
       "      <td>[{'id': 7, 'name': 'Funny', 'count': 19645}, {...</td>\n",
       "      <td>[{'id': 865, 'hero': 'https://pe.tedcdn.com/im...</td>\n",
       "      <td>Author/educator</td>\n",
       "      <td>['children', 'creativity', 'culture', 'dance',...</td>\n",
       "      <td>Do schools kill creativity?</td>\n",
       "      <td>https://www.ted.com/talks/ken_robinson_says_sc...</td>\n",
       "      <td>47227110</td>\n",
       "      <td>Good morning. How are you?(Laughter)It's been ...</td>\n",
       "      <td>https://www.ted.com/talks/ken_robinson_says_sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>265</td>\n",
       "      <td>With the same humor and humanity he exuded in ...</td>\n",
       "      <td>977</td>\n",
       "      <td>TED2006</td>\n",
       "      <td>1140825600</td>\n",
       "      <td>43</td>\n",
       "      <td>Al Gore</td>\n",
       "      <td>Al Gore: Averting the climate crisis</td>\n",
       "      <td>1</td>\n",
       "      <td>1151367060</td>\n",
       "      <td>[{'id': 7, 'name': 'Funny', 'count': 544}, {'i...</td>\n",
       "      <td>[{'id': 243, 'hero': 'https://pe.tedcdn.com/im...</td>\n",
       "      <td>Climate advocate</td>\n",
       "      <td>['alternative energy', 'cars', 'climate change...</td>\n",
       "      <td>Averting the climate crisis</td>\n",
       "      <td>https://www.ted.com/talks/al_gore_on_averting_...</td>\n",
       "      <td>3200520</td>\n",
       "      <td>Thank you so much, Chris. And it's truly a gre...</td>\n",
       "      <td>https://www.ted.com/talks/al_gore_on_averting_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>124</td>\n",
       "      <td>New York Times columnist David Pogue takes aim...</td>\n",
       "      <td>1286</td>\n",
       "      <td>TED2006</td>\n",
       "      <td>1140739200</td>\n",
       "      <td>26</td>\n",
       "      <td>David Pogue</td>\n",
       "      <td>David Pogue: Simplicity sells</td>\n",
       "      <td>1</td>\n",
       "      <td>1151367060</td>\n",
       "      <td>[{'id': 7, 'name': 'Funny', 'count': 964}, {'i...</td>\n",
       "      <td>[{'id': 1725, 'hero': 'https://pe.tedcdn.com/i...</td>\n",
       "      <td>Technology columnist</td>\n",
       "      <td>['computers', 'entertainment', 'interface desi...</td>\n",
       "      <td>Simplicity sells</td>\n",
       "      <td>https://www.ted.com/talks/david_pogue_says_sim...</td>\n",
       "      <td>1636292</td>\n",
       "      <td>(Music: \"The Sound of Silence,\" Simon &amp; Garfun...</td>\n",
       "      <td>https://www.ted.com/talks/david_pogue_says_sim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>In an emotionally charged talk, MacArthur-winn...</td>\n",
       "      <td>1116</td>\n",
       "      <td>TED2006</td>\n",
       "      <td>1140912000</td>\n",
       "      <td>35</td>\n",
       "      <td>Majora Carter</td>\n",
       "      <td>Majora Carter: Greening the ghetto</td>\n",
       "      <td>1</td>\n",
       "      <td>1151367060</td>\n",
       "      <td>[{'id': 3, 'name': 'Courageous', 'count': 760}...</td>\n",
       "      <td>[{'id': 1041, 'hero': 'https://pe.tedcdn.com/i...</td>\n",
       "      <td>Activist for environmental justice</td>\n",
       "      <td>['MacArthur grant', 'activism', 'business', 'c...</td>\n",
       "      <td>Greening the ghetto</td>\n",
       "      <td>https://www.ted.com/talks/majora_carter_s_tale...</td>\n",
       "      <td>1697550</td>\n",
       "      <td>If you're here today — and I'm very happy that...</td>\n",
       "      <td>https://www.ted.com/talks/majora_carter_s_tale...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>593</td>\n",
       "      <td>You've never seen data presented like this. Wi...</td>\n",
       "      <td>1190</td>\n",
       "      <td>TED2006</td>\n",
       "      <td>1140566400</td>\n",
       "      <td>48</td>\n",
       "      <td>Hans Rosling</td>\n",
       "      <td>Hans Rosling: The best stats you've ever seen</td>\n",
       "      <td>1</td>\n",
       "      <td>1151440680</td>\n",
       "      <td>[{'id': 9, 'name': 'Ingenious', 'count': 3202}...</td>\n",
       "      <td>[{'id': 2056, 'hero': 'https://pe.tedcdn.com/i...</td>\n",
       "      <td>Global health expert; data visionary</td>\n",
       "      <td>['Africa', 'Asia', 'Google', 'demo', 'economic...</td>\n",
       "      <td>The best stats you've ever seen</td>\n",
       "      <td>https://www.ted.com/talks/hans_rosling_shows_t...</td>\n",
       "      <td>12005869</td>\n",
       "      <td>About 10 years ago, I took on the task to teac...</td>\n",
       "      <td>https://www.ted.com/talks/hans_rosling_shows_t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   comments                                        description  duration  \\\n",
       "0      4553  Sir Ken Robinson makes an entertaining and pro...      1164   \n",
       "1       265  With the same humor and humanity he exuded in ...       977   \n",
       "2       124  New York Times columnist David Pogue takes aim...      1286   \n",
       "3       200  In an emotionally charged talk, MacArthur-winn...      1116   \n",
       "4       593  You've never seen data presented like this. Wi...      1190   \n",
       "\n",
       "     event   film_date  languages   main_speaker  \\\n",
       "0  TED2006  1140825600         60   Ken Robinson   \n",
       "1  TED2006  1140825600         43        Al Gore   \n",
       "2  TED2006  1140739200         26    David Pogue   \n",
       "3  TED2006  1140912000         35  Majora Carter   \n",
       "4  TED2006  1140566400         48   Hans Rosling   \n",
       "\n",
       "                                            name  num_speaker  published_date  \\\n",
       "0      Ken Robinson: Do schools kill creativity?            1      1151367060   \n",
       "1           Al Gore: Averting the climate crisis            1      1151367060   \n",
       "2                  David Pogue: Simplicity sells            1      1151367060   \n",
       "3             Majora Carter: Greening the ghetto            1      1151367060   \n",
       "4  Hans Rosling: The best stats you've ever seen            1      1151440680   \n",
       "\n",
       "                                             ratings  \\\n",
       "0  [{'id': 7, 'name': 'Funny', 'count': 19645}, {...   \n",
       "1  [{'id': 7, 'name': 'Funny', 'count': 544}, {'i...   \n",
       "2  [{'id': 7, 'name': 'Funny', 'count': 964}, {'i...   \n",
       "3  [{'id': 3, 'name': 'Courageous', 'count': 760}...   \n",
       "4  [{'id': 9, 'name': 'Ingenious', 'count': 3202}...   \n",
       "\n",
       "                                       related_talks  \\\n",
       "0  [{'id': 865, 'hero': 'https://pe.tedcdn.com/im...   \n",
       "1  [{'id': 243, 'hero': 'https://pe.tedcdn.com/im...   \n",
       "2  [{'id': 1725, 'hero': 'https://pe.tedcdn.com/i...   \n",
       "3  [{'id': 1041, 'hero': 'https://pe.tedcdn.com/i...   \n",
       "4  [{'id': 2056, 'hero': 'https://pe.tedcdn.com/i...   \n",
       "\n",
       "                     speaker_occupation  \\\n",
       "0                       Author/educator   \n",
       "1                      Climate advocate   \n",
       "2                  Technology columnist   \n",
       "3    Activist for environmental justice   \n",
       "4  Global health expert; data visionary   \n",
       "\n",
       "                                                tags  \\\n",
       "0  ['children', 'creativity', 'culture', 'dance',...   \n",
       "1  ['alternative energy', 'cars', 'climate change...   \n",
       "2  ['computers', 'entertainment', 'interface desi...   \n",
       "3  ['MacArthur grant', 'activism', 'business', 'c...   \n",
       "4  ['Africa', 'Asia', 'Google', 'demo', 'economic...   \n",
       "\n",
       "                             title  \\\n",
       "0      Do schools kill creativity?   \n",
       "1      Averting the climate crisis   \n",
       "2                 Simplicity sells   \n",
       "3              Greening the ghetto   \n",
       "4  The best stats you've ever seen   \n",
       "\n",
       "                                                 url     views  \\\n",
       "0  https://www.ted.com/talks/ken_robinson_says_sc...  47227110   \n",
       "1  https://www.ted.com/talks/al_gore_on_averting_...   3200520   \n",
       "2  https://www.ted.com/talks/david_pogue_says_sim...   1636292   \n",
       "3  https://www.ted.com/talks/majora_carter_s_tale...   1697550   \n",
       "4  https://www.ted.com/talks/hans_rosling_shows_t...  12005869   \n",
       "\n",
       "                                          transcript  \\\n",
       "0  Good morning. How are you?(Laughter)It's been ...   \n",
       "1  Thank you so much, Chris. And it's truly a gre...   \n",
       "2  (Music: \"The Sound of Silence,\" Simon & Garfun...   \n",
       "3  If you're here today — and I'm very happy that...   \n",
       "4  About 10 years ago, I took on the task to teac...   \n",
       "\n",
       "                                                 url  \n",
       "0  https://www.ted.com/talks/ken_robinson_says_sc...  \n",
       "1  https://www.ted.com/talks/al_gore_on_averting_...  \n",
       "2  https://www.ted.com/talks/david_pogue_says_sim...  \n",
       "3  https://www.ted.com/talks/majora_carter_s_tale...  \n",
       "4  https://www.ted.com/talks/hans_rosling_shows_t...  "
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "ted_main = pd.read_csv('../dataset/ted_talks/ted_main.csv', header=0)\n",
    "ted_transcript = pd.read_csv('../dataset/ted_talks/transcripts.csv', header=0)\n",
    "ted_data = pd.concat([ted_main,ted_transcript], axis=1)\n",
    "ted_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2197    How my son's short life made a lasting difference\n",
      "1473              4 pillars of college success in science\n",
      "1148                                 The true cost of oil\n",
      "2186      The inside story of the Paris climate agreement\n",
      "1781           Why I love a country that once betrayed me\n",
      "131                  How to educate leaders? Liberal arts\n",
      "1848      What I learned from spending 31 days underwater\n",
      "1634                              Invest in social change\n",
      "1379                     How to \"sketch\" with electronics\n",
      "973                         Try something new for 30 days\n",
      "Name: title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import pandas as pd\n",
    "\n",
    "ted_main = pd.read_csv('../dataset/ted_talks/ted_main.csv', header=0)\n",
    "ted_transcript = pd.read_csv('../dataset/ted_talks/transcripts.csv', header=0)\n",
    "ted_data = pd.concat([ted_main,ted_transcript], axis=1).dropna()\n",
    "\n",
    "transcripts = ted_data['transcript']\n",
    "# Generate mapping between titles and index\n",
    "indices = pd.Series(df.index, index=df['title']).drop_duplicates()\n",
    "\n",
    "def get_recommendations(title, cosine_sim, indices):\n",
    "    # Get index of movie that matches title\n",
    "    idx = indices[title]\n",
    "    # Sort the movies based on the similarity scores\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    # Get the scores for 10 most similar movies\n",
    "    sim_scores = sim_scores[1:11]\n",
    "    # Get the movie indices\n",
    "    ted_indices = [i[0] for i in sim_scores]\n",
    "    # Return the top 10 most similar movies\n",
    "    return ted_data['title'].iloc[ted_indices]\n",
    "\n",
    "# Initialize the TfidfVectorizer \n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# Construct the TF-IDF matrix\n",
    "tfidf_matrix = tfidf.fit_transform(transcripts)\n",
    "\n",
    "# Generate the cosine similarity matrix\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    " \n",
    "# Generate recommendations \n",
    "print(get_recommendations('5 ways to kill your dreams', cosine_sim, indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beyond n-grams: word embeddings\n",
    "#### The problem with BoW and tf-idf\n",
    "`I am happy`\n",
    "`I am joyous`\n",
    "`I am sad`\n",
    "\n",
    "#### Word embeddings\n",
    "- Mapping words into an n-dimensional vector space\n",
    "- Produced using deep learning and huge amount of data\n",
    "- Discern how similar two words are to each other\n",
    "- Used to detect synonyms and antonyms\n",
    "- Capture complex relationships\n",
    "  - `King`-`Queen` relates same way as `Man`-`Woman`\n",
    "  - `France`-`Paris` -> `Russia`-`Moscow`\n",
    "- Dependent on spacy model; independent of dataset you use\n",
    "\n",
    "#### Word embeddings using spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.8733e-01  4.0595e-01 -5.1174e-01 -5.5482e-01  3.9716e-02  1.2887e-01\n",
      "  4.5137e-01 -5.9149e-01  1.5591e-01  1.5137e+00 -8.7020e-01  5.0672e-02\n",
      "  1.5211e-01 -1.9183e-01  1.1181e-01  1.2131e-01 -2.7212e-01  1.6203e+00\n",
      " -2.4884e-01  1.4060e-01  3.3099e-01 -1.8061e-02  1.5244e-01 -2.6943e-01\n",
      " -2.7833e-01 -5.2123e-02 -4.8149e-01 -5.1839e-01  8.6262e-02  3.0818e-02\n",
      " -2.1253e-01 -1.1378e-01 -2.2384e-01  1.8262e-01 -3.4541e-01  8.2611e-02\n",
      "  1.0024e-01 -7.9550e-02 -8.1721e-01  6.5621e-03  8.0134e-02 -3.9976e-01\n",
      " -6.3131e-02  3.2260e-01 -3.1625e-02  4.3056e-01 -2.7270e-01 -7.6020e-02\n",
      "  1.0293e-01 -8.8653e-02 -2.9087e-01 -4.7214e-02  4.6036e-02 -1.7788e-02\n",
      "  6.4990e-02  8.8451e-02 -3.1574e-01 -5.8522e-01  2.2295e-01 -5.2785e-02\n",
      " -5.5981e-01 -3.9580e-01 -7.9849e-02 -1.0933e-02 -4.1722e-02 -5.5576e-01\n",
      "  8.8707e-02  1.3710e-01 -2.9873e-03 -2.6256e-02  7.7330e-02  3.9199e-01\n",
      "  3.4507e-01 -8.0130e-02  3.3451e-01  2.7063e-01 -2.4544e-02  7.2576e-02\n",
      " -1.8120e-01  2.3693e-01  3.9977e-01  4.5012e-01  2.7179e-02  2.7400e-01\n",
      "  1.4791e-01 -5.8324e-03  9.5910e-01 -1.0129e+00  2.0699e-01  1.8237e-01\n",
      " -2.5234e-01 -2.6261e-01 -3.4799e-01 -2.4051e-02  4.4470e-01  5.9226e-02\n",
      "  4.5561e-01  1.9700e-01 -4.8327e-01  8.9523e-02 -2.2373e-01 -1.5654e-01\n",
      "  2.1578e-01  1.1673e-01  8.2006e-02 -8.0735e-01  2.3903e-01 -5.1304e-01\n",
      " -3.3888e-01 -3.1499e-01 -1.7272e-01 -6.7020e-01  2.7096e-01 -4.3241e-01\n",
      "  4.3103e-02  2.1233e-02  1.3350e-02 -6.3938e-02 -2.4957e-01 -2.4938e-01\n",
      "  3.4812e-01 -7.1321e-02  2.3375e-01 -9.5384e-02  5.2488e-01  6.8175e-01\n",
      " -1.0214e-01 -1.4914e-01 -7.5697e-02  1.7248e-01  2.5440e-01  1.5760e-01\n",
      " -5.9125e-01  2.4300e-01  6.3962e-01 -9.3280e-02 -2.7914e-01 -6.6262e-02\n",
      " -6.7170e-02 -4.0929e-01 -3.0300e+00  1.8250e-01  2.0113e-01  6.0628e-02\n",
      " -2.4769e-01  5.5324e-02 -4.9106e-01  3.1544e-01 -3.4231e-01 -6.3766e-01\n",
      " -3.6129e-01 -5.9029e-02  1.5510e-01  4.4577e-02  2.3572e-01 -1.7095e-01\n",
      " -2.2749e-01 -2.3184e-02  2.3868e-01  2.8170e-02  4.2965e-01 -1.2458e-01\n",
      " -3.6972e-02  2.0061e-01 -3.1405e-01 -8.5287e-02 -3.3496e-01 -9.7047e-02\n",
      " -1.4388e-01  1.1147e-01 -4.5232e-01 -2.4217e-01 -1.8245e-01 -6.7292e-01\n",
      "  2.1933e-02 -5.4816e-02 -4.6508e-01  4.7767e-01 -2.4752e-01 -1.5790e-01\n",
      "  1.1817e-01  5.6851e-02 -4.9151e-01  1.5496e-01  1.6425e-02  4.1650e-02\n",
      " -3.4990e-01 -1.5979e-01  3.9705e-01  2.2963e-01  2.4688e-01  1.9567e-02\n",
      " -2.8802e-01 -6.9983e-01  3.2744e-01  1.0833e-01  2.4945e-01 -7.8653e-01\n",
      " -6.1379e-02 -3.7359e-01 -1.1603e-01 -2.4950e-01  1.0161e-01  3.3994e-02\n",
      "  1.5650e-01  2.1344e-01 -1.1094e-01 -5.7687e-03  1.7869e-01 -1.0127e-01\n",
      " -1.6891e-02  3.0001e-01 -3.4116e-01 -3.2390e-02  4.2514e-02  1.1850e-01\n",
      " -1.8337e-01 -6.2865e-01 -2.8021e-01  4.2351e-01  1.1277e-01  1.2121e-03\n",
      "  1.5710e-01 -3.6321e-01 -4.9251e-01  1.1653e-01  2.4024e-01  1.7712e-01\n",
      "  6.8700e-02 -4.4137e-01 -2.9877e-01 -1.2071e-02  2.8325e-01  1.0668e-01\n",
      " -1.8859e-01 -4.1345e-01 -3.4090e-01  4.7236e-02 -3.8309e-01  4.3572e-01\n",
      "  2.4505e-01  2.7337e-01 -7.3038e-02  4.2514e-01 -3.2455e-02 -3.5211e-01\n",
      "  4.5691e-01  1.9433e-01 -1.5230e-01  4.2675e-01  2.8795e-01 -5.5969e-01\n",
      " -1.3031e-01  8.9844e-02  4.2605e-01 -1.9632e-01 -7.1989e-02 -8.0189e-02\n",
      " -3.0425e-01 -4.6190e-01  2.8178e-01 -9.9872e-02  3.5097e-01  1.6123e-01\n",
      " -3.6548e-02 -3.6739e-01 -1.9819e-02  3.2130e-01  1.7479e-01  2.5175e-01\n",
      " -7.6439e-03 -9.3786e-02 -3.7852e-01  4.3725e-01  2.1288e-01  2.5096e-01\n",
      " -1.9613e-01 -2.8865e-01 -5.6726e-03  4.2795e-01  2.0625e-01 -3.7701e-02\n",
      " -1.2200e-01 -7.9253e-02 -1.0290e-01  1.0558e-02  4.9880e-01  2.5382e-01\n",
      "  1.5526e-01  1.7951e-03  1.1633e-01  7.9300e-02 -3.9142e-01 -3.2483e-01\n",
      "  6.3451e-01 -1.8910e-01  5.4050e-02  1.6495e-01  1.8757e-01  5.3874e-01]\n",
      "[-8.1760e-02  9.5050e-01 -1.9627e-01  3.2322e-01 -1.1475e-01  7.2491e-01\n",
      "  1.9991e-01 -4.1873e-01 -2.1523e-01  2.2864e+00 -3.7111e-01  2.0624e-01\n",
      " -3.4831e-01 -2.4620e-01  1.9677e-01 -7.9295e-02 -3.6580e-01  1.1856e+00\n",
      " -3.5559e-03  5.7556e-01 -2.3356e-01 -4.5721e-01  1.8607e-01  5.3613e-01\n",
      " -5.5108e-02  2.6244e-01 -1.9040e-02 -5.3399e-02  9.9329e-01 -4.6326e-02\n",
      " -5.8343e-01  2.4831e-01  2.3628e-01  3.6735e-01 -4.1087e-01 -2.4192e-02\n",
      " -3.8454e-01  3.3113e-01 -2.5281e-01 -2.1088e-02 -2.9030e-01 -3.1195e-01\n",
      " -3.7847e-01  7.9862e-01  3.3756e-01  6.0462e-01  2.1106e-01  1.7161e-02\n",
      "  1.6436e-01 -3.4147e-01  3.2015e-01  3.4738e-01 -6.0944e-01  2.8853e-01\n",
      "  4.7932e-01  5.8479e-01 -7.5082e-01 -4.4402e-01  6.5224e-03 -1.0748e-01\n",
      "  9.1903e-02 -1.1566e+00 -7.5945e-02 -1.7315e-01 -6.5896e-01 -7.6838e-01\n",
      "  6.9863e-01 -3.9328e-02  3.5764e-01  5.6520e-01  2.0184e-02 -4.9523e-01\n",
      " -1.9349e-01 -1.6587e-01  8.3815e-01  5.0432e-01  5.3014e-01  2.2458e-01\n",
      " -3.8416e-01  6.2961e-01  1.9621e-01  5.1145e-02  8.5893e-02 -3.1473e-01\n",
      "  3.4123e-01 -1.9530e-01  5.7403e-01 -8.9242e-01  6.0832e-01 -2.1274e-02\n",
      "  2.0518e-01 -3.1363e-01 -8.1625e-01  2.9493e-01  3.4002e-01  2.3381e-01\n",
      "  4.7663e-01  6.4454e-02 -1.7138e-01 -1.0685e-01 -1.1225e-02 -5.8588e-02\n",
      " -2.0039e-01 -2.4616e-01  3.6297e-01  5.6525e-01 -5.1624e-01 -8.0988e-01\n",
      "  2.7651e-02  1.0138e-01 -2.4744e-01 -3.0343e-01  3.0367e-01 -4.2305e-01\n",
      "  5.6549e-01 -1.1670e-01 -3.1203e-01 -5.6972e-01 -8.0784e-02 -9.2206e-02\n",
      "  1.1807e-01 -3.7990e-01  4.1985e-01 -2.6395e-01  5.8506e-01 -3.6453e-02\n",
      " -3.0516e-02 -1.4047e-01 -4.9372e-01  5.1860e-02  1.0483e-01 -4.4887e-01\n",
      " -6.6936e-01 -8.9778e-02  1.3845e-01 -6.9786e-01 -4.3077e-01  2.3854e-01\n",
      " -1.3252e-01  2.1398e-01 -1.3762e+00  1.1814e-01  1.6111e-01  1.2531e-01\n",
      "  5.4347e-02  5.4878e-01 -3.7092e-01 -1.0099e-01 -7.2403e-02 -6.6552e-01\n",
      " -3.4027e-01  3.2232e-02 -7.6320e-02  3.7552e-01 -4.1588e-01  2.2676e-01\n",
      "  3.4596e-01 -3.3434e-01 -5.3742e-01  9.4397e-02 -1.7104e-02 -2.6160e-01\n",
      "  2.7804e-01 -6.5511e-02  7.3646e-02  2.5776e-01 -3.5709e-02 -1.4276e-01\n",
      " -2.0539e-01 -1.8760e-01 -5.5609e-01 -3.2273e-01  2.9594e-01 -3.9765e-01\n",
      " -2.5016e-02 -4.8199e-02 -3.4339e-01 -1.7692e-01  2.5867e-01  6.8487e-01\n",
      " -3.3053e-01 -5.7565e-02 -3.3737e-01 -1.2563e-01 -2.9366e-01  1.9139e-01\n",
      "  5.3887e-02 -2.7826e-02  1.9512e-01  1.1829e-01 -5.8045e-01  3.5679e-01\n",
      " -6.1477e-01 -5.1816e-01  1.4871e-01  8.7451e-01 -2.3132e-01  4.0470e-01\n",
      " -7.8567e-02 -2.0145e-01  4.3885e-01 -1.0504e-01  6.6496e-02  1.5818e-01\n",
      "  3.2686e-01  7.7705e-02 -1.3774e-01 -2.8313e-01  5.6313e-02  3.0749e-02\n",
      " -1.6830e-01 -2.4820e-01  1.6009e-01 -2.7654e-01  2.6247e-01 -7.1188e-03\n",
      " -3.4712e-01  2.9753e-01 -3.5213e-01 -3.3462e-02  3.2864e-01 -4.1862e-02\n",
      "  3.7404e-01  2.5576e-01  1.6339e-01 -7.5334e-02  3.4678e-01  1.8152e-01\n",
      " -3.8499e-01 -3.7584e-01 -9.0580e-02  1.9120e-01 -3.0927e-01  1.3177e-01\n",
      " -2.7881e-01 -5.6982e-01 -6.6286e-01 -3.0949e-01 -2.0002e-01  4.9368e-01\n",
      "  3.4719e-01  2.3746e-02  1.0352e-01  2.8529e-01 -4.0391e-01 -4.0647e-01\n",
      " -3.9268e-02 -1.4621e-01  3.4379e-01  8.5500e-01 -7.4400e-02  1.5851e-01\n",
      " -7.3204e-02 -7.2655e-02  6.4226e-01 -2.3160e-01  7.3683e-03 -9.2140e-02\n",
      "  6.3533e-02 -7.4703e-02  3.4700e-01  4.2235e-01  4.8242e-01  1.7781e-01\n",
      " -1.4147e-01 -1.8437e-01 -2.4470e-01  8.4425e-02 -1.0549e-03  1.0658e-01\n",
      " -2.5153e-01 -1.7086e-01 -6.8509e-01 -1.2899e-01  8.3986e-02  1.5304e-01\n",
      "  1.1152e-01 -4.2578e-01 -3.4582e-01  7.9028e-01  2.5903e-01 -5.1056e-01\n",
      "  7.9888e-02 -2.5027e-02  1.8446e-01  2.6668e-02  2.2356e-01 -1.5973e-01\n",
      "  1.0135e-01  8.0687e-01  6.3786e-02  2.2335e-01 -2.6171e-01 -4.6166e-01\n",
      "  3.7709e-02 -7.7545e-02 -1.7156e-01 -1.5184e-01 -2.3864e-01  4.1293e-01]\n",
      "[ 0.036775   0.40917   -0.52141   -0.067184   0.087702  -0.048564\n",
      "  0.40947   -0.42818    0.19304    2.3925    -0.11441   -0.22952\n",
      " -0.16061    0.035533  -0.53179    0.19764   -0.48827    0.57439\n",
      " -0.064301   0.47053   -0.29647   -0.15927   -0.052798   0.10121\n",
      " -0.054461   0.036129  -0.16118   -0.34139    0.45834   -0.20144\n",
      " -0.29067   -0.51888   -0.062106   0.14084    0.016413   0.050826\n",
      "  0.13243   -0.033663  -0.42228   -0.30086    0.06202    0.26338\n",
      "  0.077223   0.27307    0.13392    0.30183   -0.16546    0.057011\n",
      " -0.0034585 -0.071113  -0.27287   -0.10297    0.07457   -0.32104\n",
      "  0.36696    0.27051   -0.15776    0.2978    -0.18988    0.097477\n",
      "  0.035665  -0.49749   -0.52759   -0.046148   0.021715  -0.11047\n",
      " -0.18007    0.20295    0.15254   -0.045976  -0.21846   -0.066865\n",
      " -0.21355    0.017509   0.66474    0.25527    0.24864   -0.094851\n",
      " -0.012857   0.46896    0.052031   0.62488   -0.12662    0.063972\n",
      " -0.15719   -0.45907    0.32286   -0.17502    0.64181    0.091587\n",
      " -0.075871   0.11718   -0.13864    0.24951   -0.40664    0.08845\n",
      " -0.29196   -0.51624   -0.074847  -0.012822  -0.088844  -0.19935\n",
      "  0.052734  -0.13588    0.231     -0.34368    0.30607   -0.21223\n",
      "  0.08178    0.10097    0.33585   -0.17491    0.019115   0.15998\n",
      "  0.38803   -0.35932    0.31682   -0.18614    0.11732   -0.068517\n",
      "  0.50785   -0.0035486  0.20069    0.25218    0.38309    0.19359\n",
      "  0.43857   -0.29954   -0.14219    0.087962  -0.14229    0.10075\n",
      " -0.58986   -0.12672    0.036944  -0.050421  -0.19875   -0.051368\n",
      " -0.023402   0.08744   -2.4938     0.15427    0.12373   -0.0086429\n",
      " -0.17007   -0.519     -0.29962    0.24369   -0.20535   -0.24942\n",
      " -0.079362   0.40986   -0.10753    0.098907  -0.063449   0.05373\n",
      "  0.26206    0.13207   -0.067694  -0.56168   -0.18867    0.14453\n",
      " -0.22469   -0.28404    0.20909   -0.46989    0.30992   -0.13283\n",
      "  0.041392   0.11146    0.17015   -0.059407  -0.16098   -0.2211\n",
      " -0.0035877 -0.22357   -0.01852   -0.23026   -0.18824    0.32997\n",
      "  0.16287   -0.52067    0.17308   -0.024264  -0.041321  -0.3241\n",
      " -0.44122   -0.11114    0.22684   -0.10883   -0.1278    -0.16696\n",
      "  0.051048  -0.12131    0.18038    0.19793    0.134     -0.37113\n",
      "  0.36008    0.092685  -0.30263    0.16565   -0.10863   -0.29565\n",
      "  0.26143    0.13369   -0.090181   0.021989  -0.093353  -0.20325\n",
      " -0.2008     0.20721    0.17208   -0.20199    0.043315   0.17768\n",
      "  0.57448   -0.45917   -0.077197   0.12051    0.07209   -0.095313\n",
      "  0.10973    0.22375    0.045804  -0.13573    0.14041   -0.11364\n",
      " -0.46605   -0.43262   -0.058678   0.19043   -0.40867    0.30509\n",
      "  0.18542    0.095309  -0.42329   -0.15225   -0.13827    0.18119\n",
      "  0.14755   -0.053628   0.031298   0.65695   -0.1717     0.23649\n",
      " -0.34742   -0.17438   -0.085304   0.37687    0.21322   -0.13184\n",
      " -0.35197   -0.14072    0.2332     0.21014   -0.14763    0.047515\n",
      " -0.27979    0.090331  -0.15565    0.42803   -0.019297   0.012198\n",
      "  0.036031  -0.10396    0.11014    0.13458    0.2775     0.36225\n",
      " -0.35591   -0.16877   -0.41201    0.070133  -0.27769    0.13739\n",
      " -0.057831   0.19277    0.11131    0.53696    0.0093424 -0.26107\n",
      " -0.38663    0.040653   0.18617    0.26312    0.12212   -0.030012\n",
      "  0.096286   0.47376   -0.21633    0.10798   -0.17703    0.22116\n",
      "  0.6726     0.065036  -0.017414  -0.048585  -0.090863   0.28591  ]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load model and create Doc object\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "doc = nlp('I am happy')\n",
    "\n",
    "# Generate word vectors for each token\n",
    "for token in doc:\n",
    "    print(token.vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happy happy 1.0\n",
      "happy joyous 0.5333031\n",
      "happy sad 0.64389884\n",
      "joyous happy 0.5333031\n",
      "joyous joyous 1.0\n",
      "joyous sad 0.43832767\n",
      "sad happy 0.64389884\n",
      "sad joyous 0.43832767\n",
      "sad sad 1.0\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"happy joyous sad\")\n",
    "for token1 in doc:\n",
    "    for token2 in doc:\n",
    "        print(token1.text, token2.text, token1.similarity(token2))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Document similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9492464724721577"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate doc objects\n",
    "sent1 = nlp(\"I am happy\")\n",
    "sent2 = nlp(\"I am sad\")\n",
    "sent3 = nlp(\"I am joyous\")\n",
    "\n",
    "# Compute similarity between sent1 and sent2\n",
    "sent1.similarity(sent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9239675481730458"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent1.similarity(sent3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Generating word vectors\n",
    "In this exercise, we will generate the pairwise similarity scores of all the words in a sentence. The sentence is available as sent and has been printed to the console for your convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I I 1.0\n",
      "I like 0.55549127\n",
      "I apples 0.20442723\n",
      "I and 0.31607857\n",
      "I oranges 0.18824081\n",
      "like I 0.55549127\n",
      "like like 1.0\n",
      "like apples 0.32987145\n",
      "like and 0.5267485\n",
      "like oranges 0.27717474\n",
      "apples I 0.20442723\n",
      "apples like 0.32987145\n",
      "apples apples 1.0\n",
      "apples and 0.2409773\n",
      "apples oranges 0.77809423\n",
      "and I 0.31607857\n",
      "and like 0.5267485\n",
      "and apples 0.2409773\n",
      "and and 1.0\n",
      "and oranges 0.19245945\n",
      "oranges I 0.18824081\n",
      "oranges like 0.27717474\n",
      "oranges apples 0.77809423\n",
      "oranges and 0.19245945\n",
      "oranges oranges 1.0\n"
     ]
    }
   ],
   "source": [
    "sent = 'I like apples and oranges'\n",
    "\n",
    "# Create the doc object\n",
    "doc = nlp(sent)\n",
    "\n",
    "# Compute pairwise similarity scores\n",
    "for token1 in doc:\n",
    "  for token2 in doc:\n",
    "    print(token1.text, token2.text, token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Computing similarity of Pink Floyd songs\n",
    "In this final exercise, you have been given lyrics of three songs by the British band Pink Floyd, namely 'High Hopes', 'Hey You' and 'Mother'. The lyrics to these songs are available as hopes, hey and mother respectively.\n",
    "\n",
    "Your task is to compute the pairwise similarity between mother and hopes, and mother and hey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8653562508450858\n",
      "0.9595267703981097\n"
     ]
    }
   ],
   "source": [
    "mother = \"\\nMother do you think they'll drop the bomb?\\nMother do you think they'll like this song?\\nMother do you think they'll try to break my balls?\\nOoh, ah\\nMother should I build the wall?\\nMother should I run for President?\\nMother should I trust the government?\\nMother will they put me in the firing mine?\\nOoh ah,\\nIs it just a waste of time?\\nHush now baby, baby, don't you cry.\\nMama's gonna make all your nightmares come true.\\nMama's gonna put all her fears into you.\\nMama's gonna keep you right here under her wing.\\nShe won't let you fly, but she might let you sing.\\nMama's gonna keep baby cozy and warm.\\nOoh baby, ooh baby, ooh baby,\\nOf course mama's gonna help build the wall.\\nMother do you think she's good enough, for me?\\nMother do you think she's dangerous, to me?\\nMother will she tear your little boy apart?\\nOoh ah,\\nMother will she break my heart?\\nHush now baby, baby don't you cry.\\nMama's gonna check out all your girlfriends for you.\\nMama won't let anyone dirty get through.\\nMama's gonna wait up until you get in.\\nMama will always find out where you've been.\\nMama's gonna keep baby healthy and clean.\\nOoh baby, ooh baby, ooh baby,\\nYou'll always be baby to me.\\nMother, did it need to be so high?\\n\"\n",
    "hopes = \"\\nBeyond the horizon of the place we lived when we were young\\nIn a world of magnets and miracles\\nOur thoughts strayed constantly and without boundary\\nThe ringing of the division bell had begun\\nAlong the Long Road and on down the Causeway\\nDo they still meet there by the Cut\\nThere was a ragged band that followed in our footsteps\\nRunning before times took our dreams away\\nLeaving the myriad small creatures trying to tie us to the ground\\nTo a life consumed by slow decay\\nThe grass was greener\\nThe light was brighter\\nWhen friends surrounded\\nThe nights of wonder\\nLooking beyond the embers of bridges glowing behind us\\nTo a glimpse of how green it was on the other side\\nSteps taken forwards but sleepwalking back again\\nDragged by the force of some in a tide\\nAt a higher altitude with flag unfurled\\nWe reached the dizzy heights of that dreamed of world\\nEncumbered forever by desire and ambition\\nThere's a hunger still unsatisfied\\nOur weary eyes still stray to the horizon\\nThough down this road we've been so many times\\nThe grass was greener\\nThe light was brighter\\nThe taste was sweeter\\nThe nights of wonder\\nWith friends surrounded\\nThe dawn mist glowing\\nThe water flowing\\nThe endless river\\nForever and ever\\n\"\n",
    "hey = \"\\nHey you, out there in the cold\\nGetting lonely, getting old\\nCan you feel me?\\nHey you, standing in the aisles\\nWith itchy feet and fading smiles\\nCan you feel me?\\nHey you, don't help them to bury the light\\nDon't give in without a fight\\nHey you out there on your own\\nSitting naked by the phone\\nWould you touch me?\\nHey you with you ear against the wall\\nWaiting for someone to call out\\nWould you touch me?\\nHey you, would you help me to carry the stone?\\nOpen your heart, I'm coming home\\nBut it was only fantasy\\nThe wall was too high\\nAs you can see\\nNo matter how he tried\\nHe could not break free\\nAnd the worms ate into his brain\\nHey you, out there on the road\\nAlways doing what you're told\\nCan you help me?\\nHey you, out there beyond the wall\\nBreaking bottles in the hall\\nCan you help me?\\nHey you, don't tell me there's no hope at all\\nTogether we stand, divided we fall\\n\"\n",
    "\n",
    "# Create Doc objects\n",
    "mother_doc = nlp(mother)\n",
    "hopes_doc = nlp(hopes)\n",
    "hey_doc = nlp(hey)\n",
    "\n",
    "# Print similarity between mother and hopes\n",
    "print(mother_doc.similarity(hopes_doc))\n",
    "\n",
    "# Print similarity between mother and hey\n",
    "print(mother_doc.similarity(hey_doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review\n",
    "- Basic features (characters, words, mentions, etc.)\n",
    "- Readability scores\n",
    "- Tokenization and lemmitization\n",
    "- Text cleaning\n",
    "- Part-of-speach tagging & named entity recording\n",
    "- n-gram modeling\n",
    "- tf-idf\n",
    "- Cosine similarity\n",
    "- Word embedding\n",
    "\n",
    "#### Futher resources\n",
    "- Advanced NLP with spaCy\n",
    "- Deep Learning in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
