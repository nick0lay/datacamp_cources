{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced NLP with spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Finding words, phrases, names and concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "world\n",
      "!\n",
      "world!\n"
     ]
    }
   ],
   "source": [
    "# Import the English language class\n",
    "from spacy.lang.en import English\n",
    "\n",
    "# Create the nlp object\n",
    "nlp = English()\n",
    "\n",
    "# Create by processing a string of text with the nlp object\n",
    "doc = nlp(\"Hello world!\")\n",
    "type(doc)\n",
    "\n",
    "# Itearate over tokens in a Doc\n",
    "for token in doc:\n",
    "    print(token.text)\n",
    "    \n",
    "# Index into the Doc to get a single Token\n",
    "token = doc[1]\n",
    "type(token)\n",
    "\n",
    "# A slice from the Doc is a Span object\n",
    "span = doc[1:4]\n",
    "type(span)\n",
    "\n",
    "# Get span text\n",
    "print(span.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token lexical attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index:  [0, 1, 2, 3, 4]\n",
      "Text:  ['It', 'costs', ',', '$', '5']\n",
      "is_alpha:  [True, True, False, False, False]\n",
      "is_punct:  [False, False, True, False, False]\n",
      "like_num:  [False, False, False, False, True]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"It costs, $5\")\n",
    "print('Index: ', [token.i for token in doc])\n",
    "print('Text: ', [token.text for token in doc])\n",
    "print('is_alpha: ', [token.is_alpha for token in doc])\n",
    "print('is_punct: ', [token.is_punct for token in doc])\n",
    "print('like_num: ', [token.like_num for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### German example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liebe Grüße!\n"
     ]
    }
   ],
   "source": [
    "# Import the German language class\n",
    "from spacy.lang.de import German\n",
    "\n",
    "# Create the nlp object\n",
    "nlp = German()\n",
    "\n",
    "# Process a text (this is German for: \"Kind regards!\")\n",
    "doc = nlp(\"Liebe Grüße!\")\n",
    "\n",
    "# Print the document text\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexical attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage found: 60\n",
      "Percentage found: 4\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(\"In 1990, more than 60% of people in East Asia were in extreme poverty. Now less than 4% are.\")\n",
    "\n",
    "# Iterate over the tokens in the doc\n",
    "for token in doc:\n",
    "    # Check if the token resembles a number\n",
    "    if token.like_num:\n",
    "        # Get the next token in the document\n",
    "        next_token = doc[token.i + 1]\n",
    "        # Check if the next token's text equals '%'\n",
    "        if next_token.text == '%':\n",
    "            print('Percentage found:', token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Enable spaCy to predict linguistic attributes in context\n",
    "  - Part-of-speach tags\n",
    "  - Syntatic dependencies\n",
    "  - Named entities\n",
    "- Trained on labeled example texts\n",
    "- Can be updated with more examples to fine-tune predictions\n",
    "\n",
    "### Pretrained model packages\n",
    "- en_core_web_sm (trained on web text)\n",
    "  - Binary weights\n",
    "  - Vocabulary\n",
    "  - Meta information (language, pipeline)\n",
    "- de_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict part of speach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She PRON\n",
      "She 95\n",
      "ate VERB\n",
      "ate 100\n",
      "the DET\n",
      "the 90\n",
      "pizza NOUN\n",
      "pizza 92\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load small Eanglish model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Process text\n",
    "doc = nlp(\"She ate the pizza\")\n",
    "\n",
    "# Iterate over the tokens\n",
    "for token in doc:\n",
    "    \n",
    "    # Pring the text and the predicted part-of-speach tag\n",
    "    print(token.text, token.pos_)\n",
    "    \n",
    "    # In this case return id of element\n",
    "    print(token.text, token.pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Syntatic Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She PRON nsubj ate\n",
      "ate VERB ROOT ate\n",
      "the DET det pizza\n",
      "pizza NOUN dobj ate\n"
     ]
    }
   ],
   "source": [
    "# Process text\n",
    "doc = nlp(\"She ate the pizza\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_, token.head.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Label defenitions*\n",
    "*nsubj* -> nominal subject - She\n",
    "*dobj* -> direct object - pizza\n",
    "*det* -> determiner (article) - the"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Named Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG 383\n",
      "U.K. GPE 384\n",
      "$1 billion MONEY 394\n"
     ]
    }
   ],
   "source": [
    "# Process a text\n",
    "doc = nlp(u\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "# Iterate over the predicted entities\n",
    "for ent in doc.ents:\n",
    "    \n",
    "    # Print entity text and its label\n",
    "    print(ent.text, ent.label_, ent.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPE - geo political entity\n",
    "To provide quick definitions to tags and labels used following method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'modifier of quantifier'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SYM, relcl, quantmod etc.\n",
    "spacy.explain('quantmod')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting linguistic annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It          PRON      nsubj     \n",
      "’s          VERB      compound  \n",
      "official    NOUN      ROOT      \n",
      ":           PUNCT     punct     \n",
      "Apple       PROPN     nsubj     \n",
      "is          AUX       ROOT      \n",
      "the         DET       det       \n",
      "first       ADJ       amod      \n",
      "U.S.        PROPN     nmod      \n",
      "public      ADJ       amod      \n",
      "company     NOUN      attr      \n",
      "to          PART      aux       \n",
      "reach       VERB      relcl     \n",
      "a           DET       det       \n",
      "$           SYM       quantmod  \n",
      "1           NUM       compound  \n",
      "trillion    NUM       nummod    \n",
      "market      NOUN      compound  \n",
      "value       NOUN      dobj      \n"
     ]
    }
   ],
   "source": [
    "text = \"It’s official: Apple is the first U.S. public company to reach a $1 trillion market value\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "for token in doc:\n",
    "    # Get the token text, part-of-speech tag and dependency label\n",
    "    token_text = token.text\n",
    "    token_pos = token.pos_\n",
    "    token_dep = token.dep_\n",
    "    # This is for formatting only\n",
    "    print('{:<12}{:<10}{:<10}'.format(token_text, token_pos, token_dep))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rule based matcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Matcher on Doc objects, not just strings\n",
    "- Matcher on tokens and token attributes\n",
    "- Use the model's predictions\n",
    "- Example: find \"duck\" (verb) not \"duck\" (noun)\n",
    "\n",
    "#### Match patterns\n",
    "- List of dictionaries, one per token\n",
    "- Match exact token texts\n",
    "In this example looking for two tokens in the text:\n",
    "```\n",
    "[{'ORTH': 'iPhone'}, {'ORTH': 'X'}]\n",
    "```\n",
    "- Match lexical attributes\n",
    "```\n",
    "[{'LOWER': 'iphone'}, {'LOWER': 'x'}]\n",
    "```\n",
    "- Match any token attributes\n",
    "Lemma is a base form it match:\n",
    "  - buying a milk\n",
    "  - drinking a coffe\n",
    "```\n",
    "[{'LEMMA': 'buy'}, {'POS': 'NOUN'}]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iPhone X\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Import matcher\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# Load a model and create the nlp object\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# initialize the matcher with the shared vocab\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Add the pattern to the matcher\n",
    "pattern = [{'ORTH': 'iPhone'}, {'ORTH': 'X'}]\n",
    "matcher.add('IPHONE_PATTERN', None, pattern)\n",
    "\n",
    "# Process some text\n",
    "doc = nlp(\"New iPhone X release date leaked\")\n",
    "\n",
    "# Call the matcher on the doc\n",
    "matches = matcher(doc)\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    # match_id: hash value of the pattern name\n",
    "    # start: start index of matched span\n",
    "    # end: end index of matched span\n",
    "    \n",
    "    # Get the matched span\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018 FIFA World Cup:\n"
     ]
    }
   ],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [\n",
    "    {'IS_DIGIT': True},\n",
    "    {'LOWER': 'fifa'},\n",
    "    {'LOWER': 'world'},\n",
    "    {'LOWER': 'cup'},\n",
    "    {'IS_PUNCT': True}\n",
    "]\n",
    "matcher.add('WORLD_CUP_PATTERN', None, pattern)\n",
    "\n",
    "doc = nlp(\"2018 FIFA World Cup: France won!\")\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    # Get the matched span\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loved dogs\n",
      "love cats\n"
     ]
    }
   ],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [\n",
    "    {'LEMMA': 'love', 'POS': 'VERB'},\n",
    "    {'POS': 'NOUN'}\n",
    "]\n",
    "matcher.add('LOVE', None, pattern)\n",
    "doc = nlp(\"I loved dogs but now I love cats more.\")\n",
    "\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    # Get the matched span\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bought a smartphone\n",
      "buying apps\n"
     ]
    }
   ],
   "source": [
    "# Matching operators and quantifiers\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [\n",
    "    {'LEMMA': 'buy'},\n",
    "    {'POS': 'DET', 'OP': '?'}, # optional: match 0 or 1 times\n",
    "    {'POS': 'NOUN'}\n",
    "]\n",
    "matcher.add('BUY_THINGS', None, pattern)\n",
    "\n",
    "doc = nlp(\"I bought a smartphone. Now I'm buying apps\")\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    # Get the matched span\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{'OP': '!'}``` - negation: match 0 times\n",
    "```{'OP': '?'}``` - optional: match 0 or 1 times\n",
    "```{'OP': '+'}``` - match 1 or more times\n",
    "```{'OP': '*'}``` - match 0 or more times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches: ['iPhone X']\n"
     ]
    }
   ],
   "source": [
    "# Using the Matcher\n",
    "# Import the Matcher and initialize it with the shared vocabulary\n",
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Create a pattern matching two tokens: \"iPhone\" and \"X\"\n",
    "pattern = [{'TEXT': 'iPhone'}, {'TEXT': 'X'}]\n",
    "\n",
    "# Add the pattern to the matcher\n",
    "matcher.add('IPHONE_X_PATTERN', None, pattern)\n",
    "\n",
    "doc = nlp(\"New iPhone X release date leaked as Apple reveals pre-orders by mistake\")\n",
    "\n",
    "# Use the matcher on the doc\n",
    "matches = matcher(doc)\n",
    "print('Matches:', [doc[start:end].text for match_id, start, end in matches])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total matches found: 3\n",
      "Match found: downloaded Fortnite\n",
      "Match found: downloading Minecraft\n",
      "Match found: download Winzip\n"
     ]
    }
   ],
   "source": [
    "# Writing match patterns\n",
    "matcher = Matcher(nlp.vocab)\n",
    "doc = nlp(\"i downloaded Fortnite on my laptop and can't open the game at all. Help? so when I was downloading Minecraft, I got the Windows version where it is the '.zip' folder and I used the default program to unpack it... do I also need to download Winzip?\")\n",
    "\n",
    "# Write a pattern that matches a form of \"download\" plus proper noun\n",
    "pattern = [{'LEMMA': 'download'}, {'POS': 'PROPN'}]\n",
    "\n",
    "# Add the pattern to the matcher and apply the matcher to the doc\n",
    "matcher.add('DOWNLOAD_THINGS_PATTERN', None, pattern)\n",
    "matches = matcher(doc)\n",
    "print('Total matches found:', len(matches))\n",
    "\n",
    "# Iterate over the matches and print the span text\n",
    "for match_id, start, end in matches:\n",
    "    print('Match found:', doc[start:end].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total matches found: 5\n",
      "Match found: beautiful design\n",
      "Match found: smart search\n",
      "Match found: automatic labels\n",
      "Match found: optional voice\n",
      "Match found: optional voice responses\n"
     ]
    }
   ],
   "source": [
    "# Writing match patterns\n",
    "matcher = Matcher(nlp.vocab)\n",
    "doc = nlp(\"Features of the app include a beautiful design, smart search, automatic labels and optional voice responses.\")\n",
    "\n",
    "# Write a pattern for adjective plus one or two nouns\n",
    "pattern = [{'POS': 'ADJ'}, {'POS': 'NOUN'}, {'POS': 'NOUN', 'OP': '?'}]\n",
    "\n",
    "# Add the pattern to the matcher and apply the matcher to the doc\n",
    "matcher.add('ADJ_NOUN_PATTERN', None, pattern)\n",
    "matches = matcher(doc)\n",
    "print('Total matches found:', len(matches))\n",
    "\n",
    "# Iterate over the matches and print the span text\n",
    "for match_id, start, end in matches:\n",
    "    print('Match found:', doc[start:end].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Large-scale data analysis with spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data structures: Vocab, Lexemes and StringStore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shared vocab and string store\n",
    " - Vocab: stores data shared accross multiple documents\n",
    " - To save memory, spyCy encodes all strings to hash values\n",
    " - Strings are only stored once in the StringStore via npl.vocab.strings\n",
    " - String store: lookup table in both directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3197928453018144401\n",
      "12035782083212280080\n"
     ]
    }
   ],
   "source": [
    "coffee_hash = nlp.vocab.strings['coffee']\n",
    "coffee_string = nlp.vocab.strings['coffee_hash']\n",
    "print(coffee_hash)\n",
    "print(coffee_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - hashes can't be reversed - that's why we need to provice the shared vocab\n",
    " - lookup the string and hash in nlp.vocab.strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hash value: 3197928453018144401\n",
      "string value:  coffee\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I love coffee\")\n",
    "\n",
    "hash_value = nlp.vocab.strings['coffee']\n",
    "print('hash value:', hash_value)\n",
    "\n",
    "# Rise an error if we haven't seen the string before\n",
    "string = nlp.vocab.strings[hash_value]\n",
    "print('string value: ', string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - the doc also exposes the vocab and strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hash value:  3197928453018144401\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I love coffee\")\n",
    "print('hash value: ', doc.vocab.strings['coffee'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lexemes: entries in the vocabulary\n",
    " - a Lexeme object is an entry in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coffee 3197928453018144401 True\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I love coffee\")\n",
    "lexeme = nlp.vocab['coffee']\n",
    "# print the lexical attributes\n",
    "print(lexeme.text, lexeme.orth, lexeme.is_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - contains the context-independent information about word\n",
    "    - word text: lexeme.text and lexeme.orth (the hash)\n",
    "    - lexical attributes like lexeme.is_alpha\n",
    "    - not context-dependent part-of-speach tags, dependencies or entity labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![id](images/vocab_hashes_lexemes.png \"Vocab, hashes and lexemes\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data structures: Doc, Span and Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc text:  Hello world!\n",
      "Span text:  Hello world\n",
      "Span with label text:  Hello world\n",
      "New doc text:  Hello world!\n"
     ]
    }
   ],
   "source": [
    "# Create an nlp ogject\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "\n",
    "# Import the Doc class\n",
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "# The words and spaces to create the doc from\n",
    "words = ['Hello', 'world', '!']\n",
    "spaces = [True, False, False]\n",
    "\n",
    "# Create a doc manually\n",
    "doc = Doc(nlp.vocab, words = words, spaces = spaces)\n",
    "print(\"Doc text: \", doc.text)\n",
    "\n",
    "# Create a span manually\n",
    "span = Span(doc, 0, 2)\n",
    "print(\"Span text: \", span.text)\n",
    "\n",
    "# Create a span with a label\n",
    "span_with_label = Span(doc, 0, 2, label = \"GREETING\")\n",
    "print(\"Span with label text: \", span_with_label.text)\n",
    "\n",
    "# Add span to the doc.ents\n",
    "doc.ents = [span_with_label]\n",
    "print(\"New doc text: \", doc.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![id](images/span_object.png \"The Span object\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best practices\n",
    " - Doc and Span are very poverfull and hod reference and relationships of words and sentences\n",
    "   - Convert result to strings as late as possible\n",
    "   - Use token attributes if available - for example, token.i for the token index\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Excercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('David Bowie', 'PERSON')]\n"
     ]
    }
   ],
   "source": [
    "# Excersise\n",
    "# Import the Doc and Span classes\n",
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "# Create a doc from the words and spaces\n",
    "doc = Doc(nlp.vocab, words=['I', 'like', 'David', 'Bowie'], spaces=[True, True, True, False])\n",
    "\n",
    "# Create a span for \"David Bowie\" from the doc and assign it the label \"PERSON\"\n",
    "span = Span(doc, 2, 4, label='PERSON')\n",
    "\n",
    "# Add the span to the doc's entities\n",
    "doc.ents = [span]\n",
    "\n",
    "# Print entities' text and labels\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found a verb after a proper noun!\n"
     ]
    }
   ],
   "source": [
    "# Excersise\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(\"Berlin is a nice city\")\n",
    "# Get all tokens and part-of-speech tags\n",
    "pos_tags = [token.pos_ for token in doc]\n",
    "\n",
    "for token in doc:\n",
    "    # Check if the current token is a proper noun\n",
    "    if token.pos_ == 'PROPN':\n",
    "        # Check if the next token is a verb\n",
    "        if doc[token.i + 1].pos_ == 'AUX':\n",
    "            print('Found a verb after a proper noun!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word vector and semantic similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing semantic similarity\n",
    "- spaCy can compare two objects and predict similarity (Span, Doc, Token)\n",
    "- Doc.similarity(), Span.similarity() and Token.similarity()\n",
    "- Take another object and return a similarity score (0 to 1)\n",
    "- Important: needs a model that has word vector included, for example:\n",
    "  - YES: en_core_web_md (medium model)\n",
    "  - YES: en_core_web_lg (large model)\n",
    "  - YES: en_core_web_sm (small model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similarity examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like fast food\n",
      "0.8627203210548107\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load a large model with vectors\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "# Compare two documents\n",
    "doc1 = nlp(\"I like fast food\")\n",
    "doc2 = nlp(\"I like pizza\")\n",
    "print(doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7369546\n"
     ]
    }
   ],
   "source": [
    "# Compare two tokens\n",
    "doc = nlp(\"I like pizza and pasta\")\n",
    "token1 = doc[2]\n",
    "token2 = doc[4]\n",
    "print(token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3253198600655889\n"
     ]
    }
   ],
   "source": [
    "# Compare a document with a token\n",
    "doc = nlp(\"I like pizza\")\n",
    "token=nlp(\"soap\")[0]\n",
    "\n",
    "print(doc.similarity(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6199092090831612\n"
     ]
    }
   ],
   "source": [
    "# Compare a span with a document\n",
    "span = nlp(\"I like pizza and pasta\")[2:5]\n",
    "doc = nlp(\"McDonalds sells burgers\")\n",
    "\n",
    "print(span.similarity(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How does spaCy predict similarity?\n",
    "- Similarity is determined using word vectors\n",
    "- Multy-dimensional meaning representations of words\n",
    "- Generated using an algorithm like Word2Vec and lots of text\n",
    "- Can be added to spaCy's statistical model\n",
    "- Default: consine similarity, but can be adjusted\n",
    "- Doc and Span vectors default to average of token vectors\n",
    "- Short phrases are better than long documents with many irrelevant words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word vector in spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.0228e-01 -7.6618e-02  3.7032e-01  3.2845e-02 -4.1957e-01  7.2069e-02\n",
      " -3.7476e-01  5.7460e-02 -1.2401e-02  5.2949e-01 -5.2380e-01 -1.9771e-01\n",
      " -3.4147e-01  5.3317e-01 -2.5331e-02  1.7380e-01  1.6772e-01  8.3984e-01\n",
      "  5.5107e-02  1.0547e-01  3.7872e-01  2.4275e-01  1.4745e-02  5.5951e-01\n",
      "  1.2521e-01 -6.7596e-01  3.5842e-01 -4.0028e-02  9.5949e-02 -5.0690e-01\n",
      " -8.5318e-02  1.7980e-01  3.3867e-01  1.3230e-01  3.1021e-01  2.1878e-01\n",
      "  1.6853e-01  1.9874e-01 -5.7385e-01 -1.0649e-01  2.6669e-01  1.2838e-01\n",
      " -1.2803e-01 -1.3284e-01  1.2657e-01  8.6723e-01  9.6721e-02  4.8306e-01\n",
      "  2.1271e-01 -5.4990e-02 -8.2425e-02  2.2408e-01  2.3975e-01 -6.2260e-02\n",
      "  6.2194e-01 -5.9900e-01  4.3201e-01  2.8143e-01  3.3842e-02 -4.8815e-01\n",
      " -2.1359e-01  2.7401e-01  2.4095e-01  4.5950e-01 -1.8605e-01 -1.0497e+00\n",
      " -9.7305e-02 -1.8908e-01 -7.0929e-01  4.0195e-01 -1.8768e-01  5.1687e-01\n",
      "  1.2520e-01  8.4150e-01  1.2097e-01  8.8239e-02 -2.9196e-02  1.2151e-03\n",
      "  5.6825e-02 -2.7421e-01  2.5564e-01  6.9793e-02 -2.2258e-01 -3.6006e-01\n",
      " -2.2402e-01 -5.3699e-02  1.2022e+00  5.4535e-01 -5.7998e-01  1.0905e-01\n",
      "  4.2167e-01  2.0662e-01  1.2936e-01 -4.1457e-02 -6.6777e-01  4.0467e-01\n",
      " -1.5218e-02 -2.7640e-01 -1.5611e-01 -7.9198e-02  4.0037e-02 -1.2944e-01\n",
      " -2.4090e-04 -2.6785e-01 -3.8115e-01 -9.7245e-01  3.1726e-01 -4.3951e-01\n",
      "  4.1934e-01  1.8353e-01 -1.5260e-01 -1.0808e-01 -1.0358e+00  7.6217e-02\n",
      "  1.6519e-01  2.6526e-04  1.6616e-01 -1.5281e-01  1.8123e-01  7.0274e-01\n",
      "  5.7956e-03  5.1664e-02 -5.9745e-02 -2.7551e-01 -3.9049e-01  6.1132e-02\n",
      "  5.5430e-01 -8.7997e-02 -4.1681e-01  3.2826e-01 -5.2549e-01 -4.4288e-01\n",
      "  8.2183e-03  2.4486e-01 -2.2982e-01 -3.4981e-01  2.6894e-01  3.9166e-01\n",
      " -4.1904e-01  1.6191e-01 -2.6263e+00  6.4134e-01  3.9743e-01 -1.2868e-01\n",
      " -3.1946e-01 -2.5633e-01 -1.2220e-01  3.2275e-01 -7.9933e-02 -1.5348e-01\n",
      "  3.1505e-01  3.0591e-01  2.6012e-01  1.8553e-01 -2.4043e-01  4.2886e-02\n",
      "  4.0622e-01 -2.4256e-01  6.3870e-01  6.9983e-01 -1.4043e-01  2.5209e-01\n",
      "  4.8984e-01 -6.1067e-02 -3.6766e-01 -5.5089e-01 -3.8265e-01 -2.0843e-01\n",
      "  2.2832e-01  5.1218e-01  2.7868e-01  4.7652e-01  4.7951e-02 -3.4008e-01\n",
      " -3.2873e-01 -4.1967e-01 -7.5499e-02 -3.8954e-01 -2.9622e-02 -3.4070e-01\n",
      "  2.2170e-01 -6.2856e-02 -5.1903e-01 -3.7774e-01 -4.3477e-03 -5.8301e-01\n",
      " -8.7546e-02 -2.3929e-01 -2.4711e-01 -2.5887e-01 -2.9894e-01  1.3715e-01\n",
      "  2.9892e-02  3.6544e-02 -4.9665e-01 -1.8160e-01  5.2939e-01  2.1992e-01\n",
      " -4.4514e-01  3.7798e-01 -5.7062e-01 -4.6946e-02  8.1806e-02  1.9279e-02\n",
      "  3.3246e-01 -1.4620e-01  1.7156e-01  3.9981e-01  3.6217e-01  1.2816e-01\n",
      "  3.1644e-01  3.7569e-01 -7.4690e-02 -4.8480e-02 -3.1401e-01 -1.9286e-01\n",
      " -3.1294e-01 -1.7553e-02 -1.7514e-01 -2.7587e-02 -1.0000e+00  1.8387e-01\n",
      "  8.1434e-01 -1.8913e-01  5.0999e-01 -9.1960e-03 -1.9295e-03  2.8189e-01\n",
      "  2.7247e-02  4.3409e-01 -5.4967e-01 -9.7426e-02 -2.4540e-01 -1.7203e-01\n",
      " -8.8650e-02 -3.0298e-01 -1.3591e-01 -2.7765e-01  3.1286e-03  2.0556e-01\n",
      " -1.5772e-01 -5.2308e-01 -6.4701e-01 -3.7014e-01  6.9393e-02  1.1401e-01\n",
      "  2.7594e-01 -1.3875e-01 -2.7268e-01  6.6891e-01 -5.6454e-02  2.4017e-01\n",
      " -2.6730e-01  2.9860e-01  1.0083e-01  5.5592e-01  3.2849e-01  7.6858e-02\n",
      "  1.5528e-01  2.5636e-01 -1.0772e-01 -1.2359e-01  1.1827e-01 -9.9029e-02\n",
      " -3.4328e-01  1.1502e-01 -3.7808e-01 -3.9012e-02 -3.4593e-01 -1.9404e-01\n",
      " -3.3580e-01 -6.2334e-02  2.8919e-01  2.8032e-01 -5.3741e-01  6.2794e-01\n",
      "  5.6955e-02  6.2147e-01 -2.5282e-01  4.1670e-01 -1.0108e-02 -2.5434e-01\n",
      "  4.0003e-01  4.2432e-01  2.2672e-01  1.7553e-01  2.3049e-01  2.8323e-01\n",
      "  1.3882e-01  3.1218e-03  1.7057e-01  3.6685e-01  2.5247e-03 -6.4009e-01\n",
      " -2.9765e-01  7.8943e-01  3.3168e-01 -1.1966e+00 -4.7156e-02  5.3175e-01]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I have a banana\")\n",
    "# Access the vector via the token.vector attribute\n",
    "print(doc[3].vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similarity depends on the application context\n",
    "- Useful for many applications: recommendation system, flagging duplicates etc.\n",
    "- There's no objective definition of \"similarity\"\n",
    "- Depends on the context and what application needs to do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9501447503553421\n"
     ]
    }
   ],
   "source": [
    "# Texts is similar bcs both of them about cats\n",
    "doc1 = nlp(\"I like cats\")\n",
    "doc2 = nlp(\"I hate cats\")\n",
    "print(doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6574650996652592\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(\"like\")\n",
    "doc2 = nlp(\"hate\")\n",
    "print(doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining models and rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|                     | Statistical models                                          | Rule-based systems                                     |   |   |\n",
    "|---------------------|-------------------------------------------------------------|--------------------------------------------------------|---|---|\n",
    "| Use cases           | applications needs to generalize based on examples          | dictionary with finite number of examples              |   |   |\n",
    "| Real-world examples | product names, person names, subject/object relationships   | countries of the world, cities, drug names, dog breeds |   |   |\n",
    "| spaCy features      | entity recognizer, dependency parser, part-of-speech tagger | tokenizer, Matcher, PhraseMatcher                      |   |   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recap: Rule-based Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches: ['very happy', 'very very happy']\n"
     ]
    }
   ],
   "source": [
    "# Initialize with the shared vocab\n",
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Patterns are lists of dictionaries describing the tokens\n",
    "# pattern = [{'LEMMA': 'love', 'POS': 'VERB'}, {'LOWER', 'cats'}]\n",
    "# matcher.add('LOVE_CATS', None, pattern)\n",
    "\n",
    "# Operators can specify how often a token should be matched\n",
    "pattern = [{'TEXT': 'very', 'OP': '+'}, {'TEXT': 'happy'}]\n",
    "matcher.add('HAPPY', None, pattern)\n",
    "\n",
    "# Calling matcher on doc returns list of (match_id, start, end) tuples\n",
    "doc = nlp(\"I love cats and I'm very very happy\")\n",
    "matches = matcher(doc)\n",
    "\n",
    "print('Matches:', [doc[start:end].text for match_id, start, end in matches])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding statistical predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matcher span: Golden Retriver\n",
      "Root token: Retriver\n",
      "Root head token: have\n",
      "Previous token:  a DET\n"
     ]
    }
   ],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add('DOG', None, [{'LOWER': 'golden'}, {'LOWER': 'retriver'}])\n",
    "doc = nlp(\"I have a Golden Retriver\")\n",
    "\n",
    "for match_id, start, end in matcher(doc):\n",
    "    span = doc[start:end]\n",
    "    print('Matcher span:', span.text)\n",
    "    \n",
    "    # Get the span's root token and root head token\n",
    "    print('Root token:', span.root.text)\n",
    "    print('Root head token:', span.root.head.text)\n",
    "    \n",
    "    # Get the previous token and its POS tag\n",
    "    print('Previous token: ', doc[start - 1].text, doc[start - 1].pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Efficient phrase matching\n",
    "- PhraseMatcher like regular expressions or keyword search - but access to the tokens!\n",
    "- Takes Doc object as patterns\n",
    "- More efficient and faster than the Matcher\n",
    "- Great for matching large word lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matcher span: Golden Retriver\n"
     ]
    }
   ],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "doc = nlp(\"I have a Golden Retriver\")\n",
    "\n",
    "for match_id, start, end in matcher(doc):\n",
    "    span = doc[start:end]\n",
    "    print('Matcher span:', span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Debugging patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATTERN1 Amazon Prime\n",
      "PATTERN2 ad-free viewing\n",
      "PATTERN1 Amazon Prime\n",
      "PATTERN2 ad-free viewing\n",
      "PATTERN2 ad-free viewing\n",
      "PATTERN2 ad-free viewing\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Twitch Prime, the perks program for Amazon Prime members offering free loot, games and other benefits, is ditching one of its best features: ad-free viewing. According to an email sent out to Amazon Prime members today, ad-free viewing will no longer be included as a part of Twitch Prime for new members, beginning on September 14. However, members with existing annual subscriptions will be able to continue to enjoy ad-free viewing until their subscription comes up for renewal. Those with monthly subscriptions will have access to ad-free viewing until October 15.\")\n",
    "\n",
    "# Create the match patterns\n",
    "pattern1 = [{'LOWER': 'amazon'}, {'IS_TITLE': True, 'POS': 'PROPN'}]\n",
    "pattern2 = [{'LOWER': 'ad'}, {'LOWER': '-'}, {'LOWER': 'free'}, {'POS': 'NOUN'}]\n",
    "\n",
    "# Initialize the Matcher and add the patterns\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add('PATTERN1', None, pattern1)\n",
    "matcher.add('PATTERN2', None, pattern2)\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Print pattern string name and text of matched span\n",
    "    print(doc.vocab.strings[match_id], doc[start:end].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ad', '-', 'free', 'viewing']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token.text for token in nlp(\"ad-free viewing\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Czech Republic, Slovakia]\n"
     ]
    }
   ],
   "source": [
    "COUNTRIES = ['Afghanistan',\n",
    " 'Åland Islands',\n",
    " 'Albania',\n",
    " 'Algeria',\n",
    " 'American Samoa',\n",
    " 'Andorra',\n",
    " 'Angola',\n",
    " 'Anguilla',\n",
    " 'Antarctica',\n",
    " 'Antigua and Barbuda',\n",
    " 'Argentina',\n",
    " 'Armenia',\n",
    " 'Aruba',\n",
    " 'Australia',\n",
    " 'Austria',\n",
    " 'Azerbaijan',\n",
    " 'Bahamas',\n",
    " 'Bahrain',\n",
    " 'Bangladesh',\n",
    " 'Barbados',\n",
    " 'Belarus',\n",
    " 'Belgium',\n",
    " 'Belize',\n",
    " 'Benin',\n",
    " 'Bermuda',\n",
    " 'Bhutan',\n",
    " 'Bolivia (Plurinational State of)',\n",
    " 'Bonaire, Sint Eustatius and Saba',\n",
    " 'Bosnia and Herzegovina',\n",
    " 'Botswana',\n",
    " 'Bouvet Island',\n",
    " 'Brazil',\n",
    " 'British Indian Ocean Territory',\n",
    " 'United States Minor Outlying Islands',\n",
    " 'Virgin Islands (British)',\n",
    " 'Virgin Islands (U.S.)',\n",
    " 'Brunei Darussalam',\n",
    " 'Bulgaria',\n",
    " 'Burkina Faso',\n",
    " 'Burundi',\n",
    " 'Cambodia',\n",
    " 'Cameroon',\n",
    " 'Canada',\n",
    " 'Cabo Verde',\n",
    " 'Cayman Islands',\n",
    " 'Central African Republic',\n",
    " 'Chad',\n",
    " 'Chile',\n",
    " 'China',\n",
    " 'Christmas Island',\n",
    " 'Cocos (Keeling) Islands',\n",
    " 'Colombia',\n",
    " 'Comoros',\n",
    " 'Congo',\n",
    " 'Congo (Democratic Republic of the)',\n",
    " 'Cook Islands',\n",
    " 'Costa Rica',\n",
    " 'Croatia',\n",
    " 'Cuba',\n",
    " 'Curaçao',\n",
    " 'Cyprus',\n",
    " 'Czech Republic',\n",
    " 'Denmark',\n",
    " 'Djibouti',\n",
    " 'Dominica',\n",
    " 'Dominican Republic',\n",
    " 'Ecuador',\n",
    " 'Egypt',\n",
    " 'El Salvador',\n",
    " 'Equatorial Guinea',\n",
    " 'Eritrea',\n",
    " 'Estonia',\n",
    " 'Ethiopia',\n",
    " 'Falkland Islands (Malvinas)',\n",
    " 'Faroe Islands',\n",
    " 'Fiji',\n",
    " 'Finland',\n",
    " 'France',\n",
    " 'French Guiana',\n",
    " 'French Polynesia',\n",
    " 'French Southern Territories',\n",
    " 'Gabon',\n",
    " 'Gambia',\n",
    " 'Georgia',\n",
    " 'Germany',\n",
    " 'Ghana',\n",
    " 'Gibraltar',\n",
    " 'Greece',\n",
    " 'Greenland',\n",
    " 'Grenada',\n",
    " 'Guadeloupe',\n",
    " 'Guam',\n",
    " 'Guatemala',\n",
    " 'Guernsey',\n",
    " 'Guinea',\n",
    " 'Guinea-Bissau',\n",
    " 'Guyana',\n",
    " 'Haiti',\n",
    " 'Heard Island and McDonald Islands',\n",
    " 'Holy See',\n",
    " 'Honduras',\n",
    " 'Hong Kong',\n",
    " 'Hungary',\n",
    " 'Iceland',\n",
    " 'India',\n",
    " 'Indonesia',\n",
    " \"Côte d'Ivoire\",\n",
    " 'Iran (Islamic Republic of)',\n",
    " 'Iraq',\n",
    " 'Ireland',\n",
    " 'Isle of Man',\n",
    " 'Israel',\n",
    " 'Italy',\n",
    " 'Jamaica',\n",
    " 'Japan',\n",
    " 'Jersey',\n",
    " 'Jordan',\n",
    " 'Kazakhstan',\n",
    " 'Kenya',\n",
    " 'Kiribati',\n",
    " 'Kuwait',\n",
    " 'Kyrgyzstan',\n",
    " \"Lao People's Democratic Republic\",\n",
    " 'Latvia',\n",
    " 'Lebanon',\n",
    " 'Lesotho',\n",
    " 'Liberia',\n",
    " 'Libya',\n",
    " 'Liechtenstein',\n",
    " 'Lithuania',\n",
    " 'Luxembourg',\n",
    " 'Macao',\n",
    " 'Macedonia (the former Yugoslav Republic of)',\n",
    " 'Madagascar',\n",
    " 'Malawi',\n",
    " 'Malaysia',\n",
    " 'Maldives',\n",
    " 'Mali',\n",
    " 'Malta',\n",
    " 'Marshall Islands',\n",
    " 'Martinique',\n",
    " 'Mauritania',\n",
    " 'Mauritius',\n",
    " 'Mayotte',\n",
    " 'Mexico',\n",
    " 'Micronesia (Federated States of)',\n",
    " 'Moldova (Republic of)',\n",
    " 'Monaco',\n",
    " 'Mongolia',\n",
    " 'Montenegro',\n",
    " 'Montserrat',\n",
    " 'Morocco',\n",
    " 'Mozambique',\n",
    " 'Myanmar',\n",
    " 'Namibia',\n",
    " 'Nauru',\n",
    " 'Nepal',\n",
    " 'Netherlands',\n",
    " 'New Caledonia',\n",
    " 'New Zealand',\n",
    " 'Nicaragua',\n",
    " 'Niger',\n",
    " 'Nigeria',\n",
    " 'Niue',\n",
    " 'Norfolk Island',\n",
    " \"Korea (Democratic People's Republic of)\",\n",
    " 'Northern Mariana Islands',\n",
    " 'Norway',\n",
    " 'Oman',\n",
    " 'Pakistan',\n",
    " 'Palau',\n",
    " 'Palestine, State of',\n",
    " 'Panama',\n",
    " 'Papua New Guinea',\n",
    " 'Paraguay',\n",
    " 'Peru',\n",
    " 'Philippines',\n",
    " 'Pitcairn',\n",
    " 'Poland',\n",
    " 'Portugal',\n",
    " 'Puerto Rico',\n",
    " 'Qatar',\n",
    " 'Republic of Kosovo',\n",
    " 'Réunion',\n",
    " 'Romania',\n",
    " 'Russian Federation',\n",
    " 'Rwanda',\n",
    " 'Saint Barthélemy',\n",
    " 'Saint Helena, Ascension and Tristan da Cunha',\n",
    " 'Saint Kitts and Nevis',\n",
    " 'Saint Lucia',\n",
    " 'Saint Martin (French part)',\n",
    " 'Saint Pierre and Miquelon',\n",
    " 'Saint Vincent and the Grenadines',\n",
    " 'Samoa',\n",
    " 'San Marino',\n",
    " 'Sao Tome and Principe',\n",
    " 'Saudi Arabia',\n",
    " 'Senegal',\n",
    " 'Serbia',\n",
    " 'Seychelles',\n",
    " 'Sierra Leone',\n",
    " 'Singapore',\n",
    " 'Sint Maarten (Dutch part)',\n",
    " 'Slovakia',\n",
    " 'Slovenia',\n",
    " 'Solomon Islands',\n",
    " 'Somalia',\n",
    " 'South Africa',\n",
    " 'South Georgia and the South Sandwich Islands',\n",
    " 'Korea (Republic of)',\n",
    " 'South Sudan',\n",
    " 'Spain',\n",
    " 'Sri Lanka',\n",
    " 'Sudan',\n",
    " 'Suriname',\n",
    " 'Svalbard and Jan Mayen',\n",
    " 'Swaziland',\n",
    " 'Sweden',\n",
    " 'Switzerland',\n",
    " 'Syrian Arab Republic',\n",
    " 'Taiwan',\n",
    " 'Tajikistan',\n",
    " 'Tanzania, United Republic of',\n",
    " 'Thailand',\n",
    " 'Timor-Leste',\n",
    " 'Togo',\n",
    " 'Tokelau',\n",
    " 'Tonga',\n",
    " 'Trinidad and Tobago',\n",
    " 'Tunisia',\n",
    " 'Turkey',\n",
    " 'Turkmenistan',\n",
    " 'Turks and Caicos Islands',\n",
    " 'Tuvalu',\n",
    " 'Uganda',\n",
    " 'Ukraine',\n",
    " 'United Arab Emirates',\n",
    " 'United Kingdom of Great Britain and Northern Ireland',\n",
    " 'United States of America',\n",
    " 'Uruguay',\n",
    " 'Uzbekistan',\n",
    " 'Vanuatu',\n",
    " 'Venezuela (Bolivarian Republic of)',\n",
    " 'Viet Nam',\n",
    " 'Wallis and Futuna',\n",
    " 'Western Sahara',\n",
    " 'Yemen',\n",
    " 'Zambia',\n",
    " 'Zimbabwe']\n",
    "\n",
    "# Import the PhraseMatcher and initialize it\n",
    "from spacy.matcher import PhraseMatcher\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "# Create pattern Doc objects and add them to the matcher\n",
    "# This is the faster version of: [nlp(country) for country in COUNTRIES]\n",
    "patterns = list(nlp.pipe(COUNTRIES))\n",
    "matcher.add('COUNTRY', None, *patterns)\n",
    "doc = nlp(\"Czech Republic may help Slovakia protect its airspace\")\n",
    "# Call the matcher on the test document and print the result\n",
    "matches = matcher(doc)\n",
    "print([doc[start:end] for match_id, start, end in matches])\n",
    "\n",
    "# Result:\n",
    "# [Czech Republic, Slovakia]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namibia\n",
      "in --> Namibia\n",
      "South Africa\n",
      "in --> South Africa\n",
      "Cambodia\n",
      "Africa --> Cambodia\n",
      "Kuwait\n",
      "of --> Kuwait\n",
      "Somalia\n",
      "as --> Somalia\n",
      "Haiti\n",
      "Somalia --> Haiti\n",
      "Mozambique\n",
      "Haiti --> Mozambique\n",
      "Somalia\n",
      "in --> Somalia\n",
      "Rwanda\n",
      "for --> Rwanda\n",
      "Singapore\n",
      "Britain --> Singapore\n",
      "Sierra Leone\n",
      "War --> Sierra Leone\n",
      "Afghanistan\n",
      "of --> Afghanistan\n",
      "Iraq\n",
      "invaded --> Iraq\n",
      "Sudan\n",
      "in --> Sudan\n",
      "Congo\n",
      "of --> Congo\n",
      "Haiti\n",
      "earthquake --> Haiti\n",
      "[('Namibia', 'GPE'), ('South Africa', 'GPE'), ('US', 'GPE'), ('Kuwait', 'GPE'), ('Somalia', 'GPE'), ('Haiti', 'GPE'), ('Mozambique', 'GPE'), ('Yugoslavia', 'GPE'), ('Somalia', 'GPE'), ('US', 'GPE'), ('Mogadishu', 'GPE'), ('Bosnia', 'GPE'), ('Rwanda', 'GPE'), ('US', 'GPE'), ('Britain', 'GPE'), ('Singapore', 'GPE'), ('the United States', 'GPE'), ('Afghanistan', 'GPE'), ('the United States', 'GPE'), ('Iraq', 'GPE'), ('Darfur', 'GPE'), ('Sudan', 'GPE'), ('Kivu', 'GPE'), ('the Democratic Republic of Congo', 'GPE'), ('Haiti', 'GPE')]\n"
     ]
    }
   ],
   "source": [
    "# Extracting countries and relationship\n",
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "COUNTRIES = ['Afghanistan',\n",
    " 'Åland Islands',\n",
    " 'Albania',\n",
    " 'Algeria',\n",
    " 'American Samoa',\n",
    " 'Andorra',\n",
    " 'Angola',\n",
    " 'Anguilla',\n",
    " 'Antarctica',\n",
    " 'Antigua and Barbuda',\n",
    " 'Argentina',\n",
    " 'Armenia',\n",
    " 'Aruba',\n",
    " 'Australia',\n",
    " 'Austria',\n",
    " 'Azerbaijan',\n",
    " 'Bahamas',\n",
    " 'Bahrain',\n",
    " 'Bangladesh',\n",
    " 'Barbados',\n",
    " 'Belarus',\n",
    " 'Belgium',\n",
    " 'Belize',\n",
    " 'Benin',\n",
    " 'Bermuda',\n",
    " 'Bhutan',\n",
    " 'Bolivia (Plurinational State of)',\n",
    " 'Bonaire, Sint Eustatius and Saba',\n",
    " 'Bosnia and Herzegovina',\n",
    " 'Botswana',\n",
    " 'Bouvet Island',\n",
    " 'Brazil',\n",
    " 'British Indian Ocean Territory',\n",
    " 'United States Minor Outlying Islands',\n",
    " 'Virgin Islands (British)',\n",
    " 'Virgin Islands (U.S.)',\n",
    " 'Brunei Darussalam',\n",
    " 'Bulgaria',\n",
    " 'Burkina Faso',\n",
    " 'Burundi',\n",
    " 'Cambodia',\n",
    " 'Cameroon',\n",
    " 'Canada',\n",
    " 'Cabo Verde',\n",
    " 'Cayman Islands',\n",
    " 'Central African Republic',\n",
    " 'Chad',\n",
    " 'Chile',\n",
    " 'China',\n",
    " 'Christmas Island',\n",
    " 'Cocos (Keeling) Islands',\n",
    " 'Colombia',\n",
    " 'Comoros',\n",
    " 'Congo',\n",
    " 'Congo (Democratic Republic of the)',\n",
    " 'Cook Islands',\n",
    " 'Costa Rica',\n",
    " 'Croatia',\n",
    " 'Cuba',\n",
    " 'Curaçao',\n",
    " 'Cyprus',\n",
    " 'Czech Republic',\n",
    " 'Denmark',\n",
    " 'Djibouti',\n",
    " 'Dominica',\n",
    " 'Dominican Republic',\n",
    " 'Ecuador',\n",
    " 'Egypt',\n",
    " 'El Salvador',\n",
    " 'Equatorial Guinea',\n",
    " 'Eritrea',\n",
    " 'Estonia',\n",
    " 'Ethiopia',\n",
    " 'Falkland Islands (Malvinas)',\n",
    " 'Faroe Islands',\n",
    " 'Fiji',\n",
    " 'Finland',\n",
    " 'France',\n",
    " 'French Guiana',\n",
    " 'French Polynesia',\n",
    " 'French Southern Territories',\n",
    " 'Gabon',\n",
    " 'Gambia',\n",
    " 'Georgia',\n",
    " 'Germany',\n",
    " 'Ghana',\n",
    " 'Gibraltar',\n",
    " 'Greece',\n",
    " 'Greenland',\n",
    " 'Grenada',\n",
    " 'Guadeloupe',\n",
    " 'Guam',\n",
    " 'Guatemala',\n",
    " 'Guernsey',\n",
    " 'Guinea',\n",
    " 'Guinea-Bissau',\n",
    " 'Guyana',\n",
    " 'Haiti',\n",
    " 'Heard Island and McDonald Islands',\n",
    " 'Holy See',\n",
    " 'Honduras',\n",
    " 'Hong Kong',\n",
    " 'Hungary',\n",
    " 'Iceland',\n",
    " 'India',\n",
    " 'Indonesia',\n",
    " \"Côte d'Ivoire\",\n",
    " 'Iran (Islamic Republic of)',\n",
    " 'Iraq',\n",
    " 'Ireland',\n",
    " 'Isle of Man',\n",
    " 'Israel',\n",
    " 'Italy',\n",
    " 'Jamaica',\n",
    " 'Japan',\n",
    " 'Jersey',\n",
    " 'Jordan',\n",
    " 'Kazakhstan',\n",
    " 'Kenya',\n",
    " 'Kiribati',\n",
    " 'Kuwait',\n",
    " 'Kyrgyzstan',\n",
    " \"Lao People's Democratic Republic\",\n",
    " 'Latvia',\n",
    " 'Lebanon',\n",
    " 'Lesotho',\n",
    " 'Liberia',\n",
    " 'Libya',\n",
    " 'Liechtenstein',\n",
    " 'Lithuania',\n",
    " 'Luxembourg',\n",
    " 'Macao',\n",
    " 'Macedonia (the former Yugoslav Republic of)',\n",
    " 'Madagascar',\n",
    " 'Malawi',\n",
    " 'Malaysia',\n",
    " 'Maldives',\n",
    " 'Mali',\n",
    " 'Malta',\n",
    " 'Marshall Islands',\n",
    " 'Martinique',\n",
    " 'Mauritania',\n",
    " 'Mauritius',\n",
    " 'Mayotte',\n",
    " 'Mexico',\n",
    " 'Micronesia (Federated States of)',\n",
    " 'Moldova (Republic of)',\n",
    " 'Monaco',\n",
    " 'Mongolia',\n",
    " 'Montenegro',\n",
    " 'Montserrat',\n",
    " 'Morocco',\n",
    " 'Mozambique',\n",
    " 'Myanmar',\n",
    " 'Namibia',\n",
    " 'Nauru',\n",
    " 'Nepal',\n",
    " 'Netherlands',\n",
    " 'New Caledonia',\n",
    " 'New Zealand',\n",
    " 'Nicaragua',\n",
    " 'Niger',\n",
    " 'Nigeria',\n",
    " 'Niue',\n",
    " 'Norfolk Island',\n",
    " \"Korea (Democratic People's Republic of)\",\n",
    " 'Northern Mariana Islands',\n",
    " 'Norway',\n",
    " 'Oman',\n",
    " 'Pakistan',\n",
    " 'Palau',\n",
    " 'Palestine, State of',\n",
    " 'Panama',\n",
    " 'Papua New Guinea',\n",
    " 'Paraguay',\n",
    " 'Peru',\n",
    " 'Philippines',\n",
    " 'Pitcairn',\n",
    " 'Poland',\n",
    " 'Portugal',\n",
    " 'Puerto Rico',\n",
    " 'Qatar',\n",
    " 'Republic of Kosovo',\n",
    " 'Réunion',\n",
    " 'Romania',\n",
    " 'Russian Federation',\n",
    " 'Rwanda',\n",
    " 'Saint Barthélemy',\n",
    " 'Saint Helena, Ascension and Tristan da Cunha',\n",
    " 'Saint Kitts and Nevis',\n",
    " 'Saint Lucia',\n",
    " 'Saint Martin (French part)',\n",
    " 'Saint Pierre and Miquelon',\n",
    " 'Saint Vincent and the Grenadines',\n",
    " 'Samoa',\n",
    " 'San Marino',\n",
    " 'Sao Tome and Principe',\n",
    " 'Saudi Arabia',\n",
    " 'Senegal',\n",
    " 'Serbia',\n",
    " 'Seychelles',\n",
    " 'Sierra Leone',\n",
    " 'Singapore',\n",
    " 'Sint Maarten (Dutch part)',\n",
    " 'Slovakia',\n",
    " 'Slovenia',\n",
    " 'Solomon Islands',\n",
    " 'Somalia',\n",
    " 'South Africa',\n",
    " 'South Georgia and the South Sandwich Islands',\n",
    " 'Korea (Republic of)',\n",
    " 'South Sudan',\n",
    " 'Spain',\n",
    " 'Sri Lanka',\n",
    " 'Sudan',\n",
    " 'Suriname',\n",
    " 'Svalbard and Jan Mayen',\n",
    " 'Swaziland',\n",
    " 'Sweden',\n",
    " 'Switzerland',\n",
    " 'Syrian Arab Republic',\n",
    " 'Taiwan',\n",
    " 'Tajikistan',\n",
    " 'Tanzania, United Republic of',\n",
    " 'Thailand',\n",
    " 'Timor-Leste',\n",
    " 'Togo',\n",
    " 'Tokelau',\n",
    " 'Tonga',\n",
    " 'Trinidad and Tobago',\n",
    " 'Tunisia',\n",
    " 'Turkey',\n",
    " 'Turkmenistan',\n",
    " 'Turks and Caicos Islands',\n",
    " 'Tuvalu',\n",
    " 'Uganda',\n",
    " 'Ukraine',\n",
    " 'United Arab Emirates',\n",
    " 'United Kingdom of Great Britain and Northern Ireland',\n",
    " 'United States of America',\n",
    " 'Uruguay',\n",
    " 'Uzbekistan',\n",
    " 'Vanuatu',\n",
    " 'Venezuela (Bolivarian Republic of)',\n",
    " 'Viet Nam',\n",
    " 'Wallis and Futuna',\n",
    " 'Western Sahara',\n",
    " 'Yemen',\n",
    " 'Zambia',\n",
    " 'Zimbabwe']\n",
    "\n",
    "# Import the PhraseMatcher and initialize it\n",
    "from spacy.matcher import PhraseMatcher\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "# Create pattern Doc objects and add them to the matcher\n",
    "# This is the faster version of: [nlp(country) for country in COUNTRIES]\n",
    "patterns = list(nlp.pipe(COUNTRIES))\n",
    "matcher.add('COUNTRY', None, *patterns)\n",
    "\n",
    "# Create a doc and find matches in it\n",
    "text = \"After the Cold War, the UN saw a radical expansion in its peacekeeping duties, taking on more missions in ten years than it had in the previous four decades.Between 1988 and 2000, the number of adopted Security Council resolutions more than doubled, and the peacekeeping budget increased more than tenfold. The UN negotiated an end to the Salvadoran Civil War, launched a successful peacekeeping mission in Namibia, and oversaw democratic elections in post-apartheid South Africa and post-Khmer Rouge Cambodia. In 1991, the UN authorized a US-led coalition that repulsed the Iraqi invasion of Kuwait. Brian Urquhart, Under-Secretary-General from 1971 to 1985, later described the hopes raised by these successes as a \\\"false renaissance\\\" for the organization, given the more troubled missions that followed. Though the UN Charter had been written primarily to prevent aggression by one nation against another, in the early 1990s the UN faced a number of simultaneous, serious crises within nations such as Somalia, Haiti, Mozambique, and the former Yugoslavia. The UN mission in Somalia was widely viewed as a failure after the US withdrawal following casualties in the Battle of Mogadishu, and the UN mission to Bosnia faced \\\"worldwide ridicule\\\" for its indecisive and confused mission in the face of ethnic cleansing. In 1994, the UN Assistance Mission for Rwanda failed to intervene in the Rwandan genocide amid indecision in the Security Council. Beginning in the last decades of the Cold War, American and European critics of the UN condemned the organization for perceived mismanagement and corruption. In 1984, the US President, Ronald Reagan, withdrew his nation's funding from UNESCO (the United Nations Educational, Scientific and Cultural Organization, founded 1946) over allegations of mismanagement, followed by Britain and Singapore. Boutros Boutros-Ghali, Secretary-General from 1992 to 1996, initiated a reform of the Secretariat, reducing the size of the organization somewhat. His successor, Kofi Annan (1997–2006), initiated further management reforms in the face of threats from the United States to withhold its UN dues. In the late 1990s and 2000s, international interventions authorized by the UN took a wider variety of forms. The UN mission in the Sierra Leone Civil War of 1991–2002 was supplemented by British Royal Marines, and the invasion of Afghanistan in 2001 was overseen by NATO. In 2003, the United States invaded Iraq despite failing to pass a UN Security Council resolution for authorization, prompting a new round of questioning of the organization's effectiveness. Under the eighth Secretary-General, Ban Ki-moon, the UN has intervened with peacekeepers in crises including the War in Darfur in Sudan and the Kivu conflict in the Democratic Republic of Congo and sent observers and chemical weapons inspectors to the Syrian Civil War. In 2013, an internal review of UN actions in the final battles of the Sri Lankan Civil War in 2009 concluded that the organization had suffered \\\"systemic failure\\\". One hundred and one UN personnel died in the 2010 Haiti earthquake, the worst loss of life in the organization's history. The Millennium Summit was held in 2000 to discuss the UN's role in the 21st century. The three day meeting was the largest gathering of world leaders in history, and culminated in the adoption by all member states of the Millennium Development Goals (MDGs), a commitment to achieve international development in areas such as poverty reduction, gender equality, and public health. Progress towards these goals, which were to be met by 2015, was ultimately uneven. The 2005 World Summit reaffirmed the UN's focus on promoting development, peacekeeping, human rights, and global security. The Sustainable Development Goals were launched in 2015 to succeed the Millennium Development Goals. In addition to addressing global challenges, the UN has sought to improve its accountability and democratic legitimacy by engaging more with civil society and fostering a global constituency. In an effort to enhance transparency, in 2016 the organization held its first public debate between candidates for Secretary-General. On 1 January 2017, Portuguese diplomat António Guterres, who previously served as UN High Commissioner for Refugees, became the ninth Secretary-General. Guterres has highlighted several key goals for his administration, including an emphasis on diplomacy for preventing conflicts, more effective peacekeeping efforts, and streamlining the organization to be more responsive and versatile to global needs.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Create a Span with the label for \"GPE\"\n",
    "    span = Span(doc, start, end, label=\"GPE\")\n",
    "    print(span)\n",
    "\n",
    "    # Overwrite the doc.ents and add the span\n",
    "    doc.ents = list(doc.ents) + #[span] # overlapping issue here that need to be fixed\n",
    "\n",
    "    # Get the span's root head token\n",
    "    span_root_head = span.root.head\n",
    "    # Print the text of the span root's head token and the span text\n",
    "    print(span_root_head.text, '-->', span.text)\n",
    "# Print the entities in the document\n",
    "print([(ent.text, ent.label_) for ent in doc.ents if ent.label_ == 'GPE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Processing Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![id](images/nlp_processing_process.png \"What happens when you call hlp?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"This is a sentence.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build-in pipeline components\n",
    "\n",
    "| Name    | Description             | Creates                                           |\n",
    "|---------|-------------------------|---------------------------------------------------|\n",
    "| tagger  | Part-of-speech tagger   | Token.tag                                         |\n",
    "| parser  | Dependency parser       | Token.dep, Token.head, Doc.sents, Doc.noun_chunks |\n",
    "| ner     | Named entity recognizer | Doc.ents, Token.ent_iob, Token.ent_type           |\n",
    "| textcat | Text classifier         | Doc.cats                                          |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text classifier not included in any pretrained model bcs it's very specific for for any particular text. But it can be used to train own model with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Under the hood"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "meta.json\n",
    "/ner\n",
    "/parser\n",
    "/tagger\n",
    "/vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pipeline defined in model's meta.json in order\n",
    "- Built-in components need binary data to make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ```nlp.pipe_names```: list of pipeline component names\n",
    "- ```nlp.pipeline```: list of (name, component) names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Import the English language class\n",
    "from spacy.lang.en import English\n",
    "\n",
    "# Create the nlp object\n",
    "nlp = English()\n",
    "doc = nlp(\"This is a sentence.\")\n",
    "print(nlp.pipe_names)\n",
    "print(nlp.pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Inspecting the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tagger', 'parser', 'ner']\n",
      "[('tagger', <spacy.pipeline.pipes.Tagger object at 0x7f8d63597048>), ('parser', <spacy.pipeline.pipes.DependencyParser object at 0x7f8d5d799768>), ('ner', <spacy.pipeline.pipes.EntityRecognizer object at 0x7f8d5d7997c8>)]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "print(nlp.pipe_names)\n",
    "print(nlp.pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom pipeline components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Make a function execute automatically when you call nlp\n",
    "- Add your own metadata to documents and tokens\n",
    "- Updating built-in attributes like doc.ents\n",
    "\n",
    "#### Anatomy of component\n",
    "- Function that takes a doc, modifies it and return it\n",
    "- Can be added using the nlp.add_pipe method\n",
    "\n",
    "```\n",
    "def custom_component(doc):\n",
    "    # Do something to the doc here\n",
    "    return doc\n",
    "\n",
    "nlp.add_pipe(custom_component)\n",
    "```\n",
    "\n",
    "To specify where to add component following argument used\n",
    "\n",
    "| Argument | Description          | Example                                 |\n",
    "|----------|----------------------|-----------------------------------------|\n",
    "| last     | If True, add last    | nlp.add_pipe(component, last=True)      |\n",
    "| first    | If True, all first   | nlp.add_pipe(component, first=True)     |\n",
    "| before   | Add before component | nlp.add_pipe(component, before='ner')   |\n",
    "| after    | Add after component  | nlp.add_pipe(component, after='tagger') |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: a simple component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline: ['custom_component', 'tagger', 'parser', 'ner']\n",
      "Doc length: 3\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def custom_component(doc):\n",
    "    print('Doc length:', len(doc))\n",
    "    # Return modified doc\n",
    "    return doc\n",
    "\n",
    "# Add the component first in the pipeline\n",
    "nlp.add_pipe(custom_component, first=True)\n",
    "# Pring the pipe;ome component names\n",
    "print('Pipeline:', nlp.pipe_names)\n",
    "\n",
    "# Process doc\n",
    "doc = nlp(\"Hello world!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Complex component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you'll be writing a custom component that uses the PhraseMatcher to find animal names in the document and adds the matched spans to the doc.ents.\n",
    "\n",
    "A PhraseMatcher with the animal patterns has already been created as the variable matcher. The small English model is available as the variable nlp. The Span object has already been imported for you.\n",
    "\n",
    "animal_patterns: \\[Golden Retriever, cat, turtle, Rattus norvegicus\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tagger', 'parser', 'ner', 'animal_component']\n",
      "[('cat', 'ANIMAL'), ('Golden Retriever', 'ANIMAL')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add('DOG', None, [{'LOWER': 'golden'}, {'LOWER': 'retriever'}])\n",
    "matcher.add('CAT', None, [{'LOWER': 'cat'}])\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Define the custom component\n",
    "def animal_component(doc):\n",
    "    # Apply the matcher to the doc\n",
    "    matches = matcher(doc)\n",
    "    # Create a Span for each match and assign the label 'ANIMAL'\n",
    "    spans = [Span(doc, start, end, label='ANIMAL')\n",
    "             for match_id, start, end in matches]\n",
    "    # Overwrite the doc.ents with the matched spans\n",
    "    doc.ents = spans\n",
    "    return doc\n",
    "\n",
    "nlp.add_pipe(animal_component, after='ner')\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Process the text and print the text and label for the doc.ents\n",
    "doc = nlp(\"I have a cat and a Golden Retriever\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extension attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extension attributes\n",
    "##### Setting custom attributes\n",
    "- add custom metadata to document, tokens and spans, data can be added once or computed dynamically\n",
    "- accessible via the ._ property\n",
    "\n",
    "```\n",
    "doc._.title = 'My documents'\n",
    "tokens._.is_color = True\n",
    "span._.has_color = False\n",
    "```\n",
    "\n",
    "- register on the global Doc, Token or Span using set_extension method\n",
    "\n",
    "##### Extension attribute types\n",
    "1. Attribute extensions\n",
    "2. Property extensions\n",
    "3. Method extensions\n",
    "\n",
    "##### Attribute extensions\n",
    "- set a default value that can be overwritten\n",
    "\n",
    "##### Property extensions\n",
    "- Define a getter and an optional setter function\n",
    "- Getter only called when you retrive the attribute value\n",
    "- Span extensions should almost always use a getter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import global clsasses\n",
    "from spacy.tokens import Doc, Token, Span\n",
    "\n",
    "# Set extesions on the Doc, Token, Span\n",
    "Doc.set_extension('Title', default = None)\n",
    "Token.set_extension('is_color', default = False)\n",
    "Span.set_extension('has_color', default = False)\n",
    "\n",
    "# Set extension with default value\n",
    "Doc.set_extension('Title', default = None)\n",
    "\n",
    "# Overwrite extension with default value\n",
    "doc[3]._.is_color = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True - blue\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Token\n",
    "\n",
    "# Define getter function\n",
    "def get_is_color(token):\n",
    "    colors = ['red', 'yellow', 'blue']\n",
    "    return token.text in colors\n",
    "    \n",
    "# Set extension on the Token with getter\n",
    "Token.set_extension('is_color', getter = get_is_color, force = True)\n",
    "\n",
    "doc=nlp(\"The sky is blue.\")\n",
    "print(doc[3]._.is_color, '-', doc[3].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True - sky is blue\n",
      "False - The sky\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Span\n",
    "\n",
    "# Define getter function\n",
    "def get_has_color(span):\n",
    "    colors = ['red', 'yellow', 'blue']\n",
    "    return any(token.text in colors for token in span)\n",
    "    \n",
    "# Set extension on the Span with getter\n",
    "Span.set_extension('has_color', getter=get_has_color, force = True)\n",
    "\n",
    "doc=nlp(\"The sky is blue.\")\n",
    "print(doc[1:4]._.has_color, '-', doc[1:4].text)\n",
    "print(doc[0:2]._.has_color, '-', doc[0:2].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Method extensions\n",
    "- assign a function that becomes available as an object method\n",
    "- let you pass arguments to the extesion function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True - blue\n",
      "False - cloud\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Doc\n",
    "\n",
    "# Define method with arguments\n",
    "def has_token(doc, token_text):\n",
    "    in_doc = token_text in [token.text for token in doc]\n",
    "    return in_doc\n",
    "\n",
    "# Set extension on the Doc with method\n",
    "Doc.set_extension('has_token', method = has_token, force = True)\n",
    "doc = nlp(\"The sky is blue.\")\n",
    "\n",
    "print(doc._.has_token('blue'), '- blue')\n",
    "print(doc._.has_token('cloud'), '- cloud')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example: set extension attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reversed: llA\n",
      "reversed: snoitazilareneg\n",
      "reversed: era\n",
      "reversed: eslaf\n",
      "reversed: ,\n",
      "reversed: gnidulcni\n",
      "reversed: siht\n",
      "reversed: eno\n",
      "reversed: .\n"
     ]
    }
   ],
   "source": [
    "# Define the getter function that takes a token and returns its reversed text\n",
    "def get_reversed(token):\n",
    "    return token.text[::-1]\n",
    "  \n",
    "# Register the Token property extension 'reversed' with the getter get_reversed\n",
    "Token.set_extension('reversed', getter=get_reversed, force=True)\n",
    "\n",
    "# Process the text and print the reversed attribute for each token\n",
    "doc = nlp(\"All generalizations are false, including this one.\")\n",
    "for token in doc:\n",
    "    print('reversed:', token._.reversed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "has_number: True\n"
     ]
    }
   ],
   "source": [
    "# Define the getter function\n",
    "def get_has_number(doc):\n",
    "    # Return if any of the tokens in the doc return True for token.like_num\n",
    "    return any(token.like_num for token in doc)\n",
    "\n",
    "# Register the Doc property extension 'has_number' with the getter get_has_number\n",
    "Doc.set_extension('has_number', getter = get_has_number, force=True)\n",
    "\n",
    "# Process the text and check the custom has_number attribute \n",
    "doc = nlp(\"The museum closed for five years in 2012.\")\n",
    "print('has_number:', doc._.has_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<strong>Hello world</strong>\n"
     ]
    }
   ],
   "source": [
    "# Define the method\n",
    "def to_html(span, tag):\n",
    "    # Wrap the span text in a HTML tag and return it\n",
    "    return '<{tag}>{text}</{tag}>'.format(tag=tag, text=span.text)\n",
    "\n",
    "# Register the Span property extension 'to_html' with the method to_html\n",
    "Span.set_extension('to_html', method=to_html, force = True)\n",
    "\n",
    "# Process the text and call the to_html method on the span with the tag name 'strong'\n",
    "doc = nlp(\"Hello world, this is a sentence.\")\n",
    "span = doc[0:2]\n",
    "print(span._.to_html('strong'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example: entities and extensions\n",
    "In this exercise, you'll combine custom extension attributes with the model's predictions and create an attribute getter that returns a Wikipedia search URL if the span is a person, organization, or location.\n",
    "\n",
    "The Span class is already imported and the nlp object has been created for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fifty years None\n",
      "first None\n",
      "David Bowie https://en.wikipedia.org/w/index.php?search=David_Bowie\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def get_wikipedia_url(span):\n",
    "    # Get a Wikipedia URL if the span has one of the labels\n",
    "    if span.label_ in ('PERSON', 'ORG', 'GPE', 'LOCATION'):\n",
    "        entity_text = span.text.replace(' ', '_')\n",
    "        return \"https://en.wikipedia.org/w/index.php?search=\" + entity_text\n",
    "\n",
    "# Set the Span extension wikipedia_url using get getter get_wikipedia_url\n",
    "Span.set_extension('wikipedia_url', getter=get_wikipedia_url, force = True)\n",
    "\n",
    "doc = nlp(\"In over fifty years from his very first recordings right through to his last album, David Bowie was at the vanguard of contemporary culture.\")\n",
    "for ent in doc.ents:\n",
    "    # Print the text and Wikipedia URL of the entity\n",
    "    print(ent.text, ent._.wikipedia_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example: components with extensions\n",
    "Extension attributes are especially powerful if they're combined with custom pipeline components. In this exercise, you'll write a pipeline component that finds country names and a custom extension attribute that returns a country's capital, if available.\n",
    "\n",
    "The nlp object has already been created and the Span class is already imported. A phrase matcher with all countries is available as the variable matcher. A dictionary of countries mapped to their capital cities is available as the variable capitals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tagger', 'parser', 'ner', 'countries_component']\n",
      "Ents: Czech Republic may help Slovakia protect its airspace (Czech Republic, Slovakia)\n",
      "[('Czech Republic', 'GPE', 'Prague'), ('Slovakia', 'GPE', 'Bratislava')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "capitals = {'Afghanistan': 'Kabul',\n",
    " 'Albania': 'Tirana',\n",
    " 'Algeria': 'Algiers',\n",
    " 'American Samoa': 'Pago Pago',\n",
    " 'Andorra': 'Andorra la Vella',\n",
    " 'Angola': 'Luanda',\n",
    " 'Anguilla': 'The Valley',\n",
    " 'Antarctica': '',\n",
    " 'Antigua and Barbuda': \"Saint John's\",\n",
    " 'Argentina': 'Buenos Aires',\n",
    " 'Armenia': 'Yerevan',\n",
    " 'Aruba': 'Oranjestad',\n",
    " 'Australia': 'Canberra',\n",
    " 'Austria': 'Vienna',\n",
    " 'Azerbaijan': 'Baku',\n",
    " 'Bahamas': 'Nassau',\n",
    " 'Bahrain': 'Manama',\n",
    " 'Bangladesh': 'Dhaka',\n",
    " 'Barbados': 'Bridgetown',\n",
    " 'Belarus': 'Minsk',\n",
    " 'Belgium': 'Brussels',\n",
    " 'Belize': 'Belmopan',\n",
    " 'Benin': 'Porto-Novo',\n",
    " 'Bermuda': 'Hamilton',\n",
    " 'Bhutan': 'Thimphu',\n",
    " 'Bolivia (Plurinational State of)': 'Sucre',\n",
    " 'Bonaire, Sint Eustatius and Saba': 'Kralendijk',\n",
    " 'Bosnia and Herzegovina': 'Sarajevo',\n",
    " 'Botswana': 'Gaborone',\n",
    " 'Bouvet Island': '',\n",
    " 'Brazil': 'Brasília',\n",
    " 'British Indian Ocean Territory': 'Diego Garcia',\n",
    " 'Brunei Darussalam': 'Bandar Seri Begawan',\n",
    " 'Bulgaria': 'Sofia',\n",
    " 'Burkina Faso': 'Ouagadougou',\n",
    " 'Burundi': 'Bujumbura',\n",
    " 'Cabo Verde': 'Praia',\n",
    " 'Cambodia': 'Phnom Penh',\n",
    " 'Cameroon': 'Yaoundé',\n",
    " 'Canada': 'Ottawa',\n",
    " 'Cayman Islands': 'George Town',\n",
    " 'Central African Republic': 'Bangui',\n",
    " 'Chad': \"N'Djamena\",\n",
    " 'Chile': 'Santiago',\n",
    " 'China': 'Beijing',\n",
    " 'Christmas Island': 'Flying Fish Cove',\n",
    " 'Cocos (Keeling) Islands': 'West Island',\n",
    " 'Colombia': 'Bogotá',\n",
    " 'Comoros': 'Moroni',\n",
    " 'Congo': 'Brazzaville',\n",
    " 'Congo (Democratic Republic of the)': 'Kinshasa',\n",
    " 'Cook Islands': 'Avarua',\n",
    " 'Costa Rica': 'San José',\n",
    " 'Croatia': 'Zagreb',\n",
    " 'Cuba': 'Havana',\n",
    " 'Curaçao': 'Willemstad',\n",
    " 'Cyprus': 'Nicosia',\n",
    " 'Czech Republic': 'Prague',\n",
    " \"Côte d'Ivoire\": 'Yamoussoukro',\n",
    " 'Denmark': 'Copenhagen',\n",
    " 'Djibouti': 'Djibouti',\n",
    " 'Dominica': 'Roseau',\n",
    " 'Dominican Republic': 'Santo Domingo',\n",
    " 'Ecuador': 'Quito',\n",
    " 'Egypt': 'Cairo',\n",
    " 'El Salvador': 'San Salvador',\n",
    " 'Equatorial Guinea': 'Malabo',\n",
    " 'Eritrea': 'Asmara',\n",
    " 'Estonia': 'Tallinn',\n",
    " 'Ethiopia': 'Addis Ababa',\n",
    " 'Falkland Islands (Malvinas)': 'Stanley',\n",
    " 'Faroe Islands': 'Tórshavn',\n",
    " 'Fiji': 'Suva',\n",
    " 'Finland': 'Helsinki',\n",
    " 'France': 'Paris',\n",
    " 'French Guiana': 'Cayenne',\n",
    " 'French Polynesia': 'Papeetē',\n",
    " 'French Southern Territories': 'Port-aux-Français',\n",
    " 'Gabon': 'Libreville',\n",
    " 'Gambia': 'Banjul',\n",
    " 'Georgia': 'Tbilisi',\n",
    " 'Germany': 'Berlin',\n",
    " 'Ghana': 'Accra',\n",
    " 'Gibraltar': 'Gibraltar',\n",
    " 'Greece': 'Athens',\n",
    " 'Greenland': 'Nuuk',\n",
    " 'Grenada': \"St. George's\",\n",
    " 'Guadeloupe': 'Basse-Terre',\n",
    " 'Guam': 'Hagåtña',\n",
    " 'Guatemala': 'Guatemala City',\n",
    " 'Guernsey': 'St. Peter Port',\n",
    " 'Guinea': 'Conakry',\n",
    " 'Guinea-Bissau': 'Bissau',\n",
    " 'Guyana': 'Georgetown',\n",
    " 'Haiti': 'Port-au-Prince',\n",
    " 'Heard Island and McDonald Islands': '',\n",
    " 'Holy See': 'Rome',\n",
    " 'Honduras': 'Tegucigalpa',\n",
    " 'Hong Kong': 'City of Victoria',\n",
    " 'Hungary': 'Budapest',\n",
    " 'Iceland': 'Reykjavík',\n",
    " 'India': 'New Delhi',\n",
    " 'Indonesia': 'Jakarta',\n",
    " 'Iran (Islamic Republic of)': 'Tehran',\n",
    " 'Iraq': 'Baghdad',\n",
    " 'Ireland': 'Dublin',\n",
    " 'Isle of Man': 'Douglas',\n",
    " 'Israel': 'Jerusalem',\n",
    " 'Italy': 'Rome',\n",
    " 'Jamaica': 'Kingston',\n",
    " 'Japan': 'Tokyo',\n",
    " 'Jersey': 'Saint Helier',\n",
    " 'Jordan': 'Amman',\n",
    " 'Kazakhstan': 'Astana',\n",
    " 'Kenya': 'Nairobi',\n",
    " 'Kiribati': 'South Tarawa',\n",
    " \"Korea (Democratic People's Republic of)\": 'Pyongyang',\n",
    " 'Korea (Republic of)': 'Seoul',\n",
    " 'Kuwait': 'Kuwait City',\n",
    " 'Kyrgyzstan': 'Bishkek',\n",
    " \"Lao People's Democratic Republic\": 'Vientiane',\n",
    " 'Latvia': 'Riga',\n",
    " 'Lebanon': 'Beirut',\n",
    " 'Lesotho': 'Maseru',\n",
    " 'Liberia': 'Monrovia',\n",
    " 'Libya': 'Tripoli',\n",
    " 'Liechtenstein': 'Vaduz',\n",
    " 'Lithuania': 'Vilnius',\n",
    " 'Luxembourg': 'Luxembourg',\n",
    " 'Macao': '',\n",
    " 'Macedonia (the former Yugoslav Republic of)': 'Skopje',\n",
    " 'Madagascar': 'Antananarivo',\n",
    " 'Malawi': 'Lilongwe',\n",
    " 'Malaysia': 'Kuala Lumpur',\n",
    " 'Maldives': 'Malé',\n",
    " 'Mali': 'Bamako',\n",
    " 'Malta': 'Valletta',\n",
    " 'Marshall Islands': 'Majuro',\n",
    " 'Martinique': 'Fort-de-France',\n",
    " 'Mauritania': 'Nouakchott',\n",
    " 'Mauritius': 'Port Louis',\n",
    " 'Mayotte': 'Mamoudzou',\n",
    " 'Mexico': 'Mexico City',\n",
    " 'Micronesia (Federated States of)': 'Palikir',\n",
    " 'Moldova (Republic of)': 'Chișinău',\n",
    " 'Monaco': 'Monaco',\n",
    " 'Mongolia': 'Ulan Bator',\n",
    " 'Montenegro': 'Podgorica',\n",
    " 'Montserrat': 'Plymouth',\n",
    " 'Morocco': 'Rabat',\n",
    " 'Mozambique': 'Maputo',\n",
    " 'Myanmar': 'Naypyidaw',\n",
    " 'Namibia': 'Windhoek',\n",
    " 'Nauru': 'Yaren',\n",
    " 'Nepal': 'Kathmandu',\n",
    " 'Netherlands': 'Amsterdam',\n",
    " 'New Caledonia': 'Nouméa',\n",
    " 'New Zealand': 'Wellington',\n",
    " 'Nicaragua': 'Managua',\n",
    " 'Niger': 'Niamey',\n",
    " 'Nigeria': 'Abuja',\n",
    " 'Niue': 'Alofi',\n",
    " 'Norfolk Island': 'Kingston',\n",
    " 'Northern Mariana Islands': 'Saipan',\n",
    " 'Norway': 'Oslo',\n",
    " 'Oman': 'Muscat',\n",
    " 'Pakistan': 'Islamabad',\n",
    " 'Palau': 'Ngerulmud',\n",
    " 'Palestine, State of': 'Ramallah',\n",
    " 'Panama': 'Panama City',\n",
    " 'Papua New Guinea': 'Port Moresby',\n",
    " 'Paraguay': 'Asunción',\n",
    " 'Peru': 'Lima',\n",
    " 'Philippines': 'Manila',\n",
    " 'Pitcairn': 'Adamstown',\n",
    " 'Poland': 'Warsaw',\n",
    " 'Portugal': 'Lisbon',\n",
    " 'Puerto Rico': 'San Juan',\n",
    " 'Qatar': 'Doha',\n",
    " 'Republic of Kosovo': 'Pristina',\n",
    " 'Romania': 'Bucharest',\n",
    " 'Russian Federation': 'Moscow',\n",
    " 'Rwanda': 'Kigali',\n",
    " 'Réunion': 'Saint-Denis',\n",
    " 'Saint Barthélemy': 'Gustavia',\n",
    " 'Saint Helena, Ascension and Tristan da Cunha': 'Jamestown',\n",
    " 'Saint Kitts and Nevis': 'Basseterre',\n",
    " 'Saint Lucia': 'Castries',\n",
    " 'Saint Martin (French part)': 'Marigot',\n",
    " 'Saint Pierre and Miquelon': 'Saint-Pierre',\n",
    " 'Saint Vincent and the Grenadines': 'Kingstown',\n",
    " 'Samoa': 'Apia',\n",
    " 'San Marino': 'City of San Marino',\n",
    " 'Sao Tome and Principe': 'São Tomé',\n",
    " 'Saudi Arabia': 'Riyadh',\n",
    " 'Senegal': 'Dakar',\n",
    " 'Serbia': 'Belgrade',\n",
    " 'Seychelles': 'Victoria',\n",
    " 'Sierra Leone': 'Freetown',\n",
    " 'Singapore': 'Singapore',\n",
    " 'Sint Maarten (Dutch part)': 'Philipsburg',\n",
    " 'Slovakia': 'Bratislava',\n",
    " 'Slovenia': 'Ljubljana',\n",
    " 'Solomon Islands': 'Honiara',\n",
    " 'Somalia': 'Mogadishu',\n",
    " 'South Africa': 'Pretoria',\n",
    " 'South Georgia and the South Sandwich Islands': 'King Edward Point',\n",
    " 'South Sudan': 'Juba',\n",
    " 'Spain': 'Madrid',\n",
    " 'Sri Lanka': 'Colombo',\n",
    " 'Sudan': 'Khartoum',\n",
    " 'Suriname': 'Paramaribo',\n",
    " 'Svalbard and Jan Mayen': 'Longyearbyen',\n",
    " 'Swaziland': 'Lobamba',\n",
    " 'Sweden': 'Stockholm',\n",
    " 'Switzerland': 'Bern',\n",
    " 'Syrian Arab Republic': 'Damascus',\n",
    " 'Taiwan': 'Taipei',\n",
    " 'Tajikistan': 'Dushanbe',\n",
    " 'Tanzania, United Republic of': 'Dodoma',\n",
    " 'Thailand': 'Bangkok',\n",
    " 'Timor-Leste': 'Dili',\n",
    " 'Togo': 'Lomé',\n",
    " 'Tokelau': 'Fakaofo',\n",
    " 'Tonga': \"Nuku'alofa\",\n",
    " 'Trinidad and Tobago': 'Port of Spain',\n",
    " 'Tunisia': 'Tunis',\n",
    " 'Turkey': 'Ankara',\n",
    " 'Turkmenistan': 'Ashgabat',\n",
    " 'Turks and Caicos Islands': 'Cockburn Town',\n",
    " 'Tuvalu': 'Funafuti',\n",
    " 'Uganda': 'Kampala',\n",
    " 'Ukraine': 'Kiev',\n",
    " 'United Arab Emirates': 'Abu Dhabi',\n",
    " 'United Kingdom of Great Britain and Northern Ireland': 'London',\n",
    " 'United States Minor Outlying Islands': '',\n",
    " 'United States of America': 'Washington, D.C.',\n",
    " 'Uruguay': 'Montevideo',\n",
    " 'Uzbekistan': 'Tashkent',\n",
    " 'Vanuatu': 'Port Vila',\n",
    " 'Venezuela (Bolivarian Republic of)': 'Caracas',\n",
    " 'Viet Nam': 'Hanoi',\n",
    " 'Virgin Islands (British)': 'Road Town',\n",
    " 'Virgin Islands (U.S.)': 'Charlotte Amalie',\n",
    " 'Wallis and Futuna': 'Mata-Utu',\n",
    " 'Western Sahara': 'El Aaiún',\n",
    " 'Yemen': \"Sana'a\",\n",
    " 'Zambia': 'Lusaka',\n",
    " 'Zimbabwe': 'Harare',\n",
    " 'Åland Islands': 'Mariehamn'}\n",
    "\n",
    "from spacy.matcher import PhraseMatcher\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "patterns = list(nlp.pipe(capitals.keys()))\n",
    "matcher.add('COUNTRY', None, *patterns)\n",
    "\n",
    "def countries_component(doc):\n",
    "    # Create an entity Span with the label 'GPE' for all matches\n",
    "    doc.ents = [Span(doc, start, end, label='GPE')\n",
    "                for match_id, start, end in matcher(doc)]\n",
    "    return doc\n",
    "\n",
    "# Add the component to the pipeline\n",
    "nlp.add_pipe(countries_component)\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Getter that looks up the span text in the dictionary of country capitals\n",
    "get_capital = lambda span: capitals.get(span.text)\n",
    "\n",
    "# Register the Span extension attribute 'capital' with the getter get_capital \n",
    "Span.set_extension('capital', getter=get_capital, force = True)\n",
    "\n",
    "# Process the text and print the entity text, label and capital attributes\n",
    "doc = nlp(\"Czech Republic may help Slovakia protect its airspace\")\n",
    "print([(ent.text, ent.label_, ent._.capital) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling and performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing large volumes of text\n",
    "- Use nlp.pipe method\n",
    "- Processes texts as a stream, yields Doc objects\n",
    "- Much faster than calling nlp on each text\\\n",
    "BAD:\\\n",
    "```docs = [nlp(text) for text in LOTS_OF_TEXTS]```\n",
    "GOOD:\\\n",
    "```docs = list(nlp.pipe(LOTS_OF_TEXTS))```\n",
    "\n",
    "#### Passing in context\n",
    "- Setting ```as_tuples = True``` on ```nlp.pipe``` lets you pass in ```(text, context)``` tuples\n",
    "- Yields ```(doc, context)``` tuples\n",
    "- Useful for associating metadata with the ```doc```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ents: This is a text ()\n",
      "This is a text 15\n",
      "Ents: And another text ()\n",
      "And another text 16\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    ('This is a text', {'id': 1, 'page_number': 15}),\n",
    "    ('And another text', {'id': 2, 'page_number': 16})\n",
    "]\n",
    "for doc, context in nlp.pipe(data, as_tuples = True):\n",
    "    print(doc.text, context['page_number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ents: This is a text ()\n",
      "{'id': 1, 'page_number': 15}\n",
      "Ents: And another text ()\n",
      "{'id': 2, 'page_number': 16}\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Doc\n",
    "\n",
    "Doc.set_extension('id', default = None, force = True)\n",
    "Doc.set_extension('page_number', default = None, force = True)\n",
    "\n",
    "data = [\n",
    "    ('This is a text', {'id': 1, 'page_number': 15}),\n",
    "    ('And another text', {'id': 2, 'page_number': 16})\n",
    "]\n",
    "for doc, context in nlp.pipe(data, as_tuples = True):\n",
    "    print(context)\n",
    "    doc._.id = context['id']\n",
    "    doc._.page_number = context['page_number']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using only the tokenizer\n",
    "- don't run the whole pipeline!\n",
    "- use ```nlp.make_doc``` to tuen a text into a Doc object\\\n",
    "BAD:\n",
    "```doc = nlp(\"Hello world\")```\n",
    "GOOD:\n",
    "```doc = nlp.make_doc(\"Hello world\")```\n",
    "\n",
    "#### Disable pipeline components\n",
    "- use ```nlp.disable_pipes``` to temporarily disable one or more pipes\n",
    "\n",
    "```\n",
    "# Disable tagger and parser\n",
    "with nlp.disable_pipes('tagger', 'parser'):\n",
    "   # Process the text and print the entities\n",
    "   doc = nlp(text)\n",
    "   print(doc.ents)\n",
    "```\n",
    "- restores them after the ```with``` block\n",
    "- only runs the remaining components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: processing strams\n",
    "In this exercise, you'll be using nlp.pipe for more efficient text processing. The nlp object has already been created for you. A list of tweets about a popular American fast food chain are available as the variable TEXTS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['favorite']\n",
      "['sick']\n",
      "[]\n",
      "['happy']\n",
      "['delicious', 'fast']\n",
      "['open', 'BAD']\n",
      "['terrible']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "TEXTS = ['McDonalds is my favorite restaurant.',\n",
    " 'Here I thought @McDonalds only had precooked burgers but it seems they only have not cooked ones?? I have no time to get sick..',\n",
    " 'People really still eat McDonalds :(',\n",
    " 'The McDonalds in Spain has chicken wings. My heart is so happy ',\n",
    " '@McDonalds Please bring back the most delicious fast food sandwich of all times!!....The Arch Deluxe :P',\n",
    " 'please hurry and open. I WANT A #McRib SANDWICH SO BAD! :D',\n",
    " 'This morning i made a terrible decision by gettin mcdonalds and now my stomach is payin for it']\n",
    "# Process the texts and print the adjectives\n",
    "for doc in nlp.pipe(TEXTS):\n",
    "    print([token.text for token in doc if token.pos_ == 'ADJ'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(McDonalds,) () (McDonalds,) (McDonalds, Spain) (The Arch Deluxe,) (hurry,) ()\n"
     ]
    }
   ],
   "source": [
    "# Process the texts and print the entities\n",
    "docs = list(nlp.pipe(TEXTS))\n",
    "entities = [doc.ents for doc in docs]\n",
    "print(*entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "people = ['David Bowie', 'Angela Merkel', 'Lady Gaga']\n",
    "\n",
    "# Create a list of patterns for the PhraseMatcher\n",
    "patterns = [nlp(person) for person in people] # bad\n",
    "patterns = list(nlp.pipe(people)) # good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: processing data with context\n",
    "In this exercise, you'll be using custom attributes to add author and book meta information to quotes.\n",
    "\n",
    "A list of (text, context) examples is available as the variable DATA. The texts are quotes from famous books, and the contexts dictionaries with the keys 'author' and 'book'. The nlp object has already been created for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin. \n",
      " — 'Metamorphosis' by Franz Kafka \n",
      "\n",
      "I know not all that may be coming, but be it what it will, I'll go to it laughing. \n",
      " — 'Moby-Dick or, The Whale' by Herman Melville \n",
      "\n",
      "It was the best of times, it was the worst of times. \n",
      " — 'A Tale of Two Cities' by Charles Dickens \n",
      "\n",
      "The only people for me are the mad ones, the ones who are mad to live, mad to talk, mad to be saved, desirous of everything at the same time, the ones who never yawn or say a commonplace thing, but burn, burn, burn like fabulous yellow roman candles exploding like spiders across the stars. \n",
      " — 'On the Road' by Jack Kerouac \n",
      "\n",
      "It was a bright cold day in April, and the clocks were striking thirteen. \n",
      " — '1984' by George Orwell \n",
      "\n",
      "Nowadays people know the price of everything and the value of nothing. \n",
      " — 'The Picture Of Dorian Gray' by Oscar Wilde \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the Doc class\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "DATA = [('One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin.',\n",
    "  {'author': 'Franz Kafka', 'book': 'Metamorphosis'}),\n",
    " (\"I know not all that may be coming, but be it what it will, I'll go to it laughing.\",\n",
    "  {'author': 'Herman Melville', 'book': 'Moby-Dick or, The Whale'}),\n",
    " ('It was the best of times, it was the worst of times.',\n",
    "  {'author': 'Charles Dickens', 'book': 'A Tale of Two Cities'}),\n",
    " ('The only people for me are the mad ones, the ones who are mad to live, mad to talk, mad to be saved, desirous of everything at the same time, the ones who never yawn or say a commonplace thing, but burn, burn, burn like fabulous yellow roman candles exploding like spiders across the stars.',\n",
    "  {'author': 'Jack Kerouac', 'book': 'On the Road'}),\n",
    " ('It was a bright cold day in April, and the clocks were striking thirteen.',\n",
    "  {'author': 'George Orwell', 'book': '1984'}),\n",
    " ('Nowadays people know the price of everything and the value of nothing.',\n",
    "  {'author': 'Oscar Wilde', 'book': 'The Picture Of Dorian Gray'})]\n",
    "\n",
    "# Register the Doc extension 'author' (default None)\n",
    "Doc.set_extension('author', default = None, force = True)\n",
    "\n",
    "# Register the Doc extension 'book' (default None)\n",
    "Doc.set_extension('book', default = None, force = True)\n",
    "\n",
    "for doc, context in nlp.pipe(DATA, as_tuples = True):\n",
    "    # Set the doc._.book and doc._.author attributes from the context\n",
    "    doc._.book = context['book']\n",
    "    doc._.author = context['author']\n",
    "    \n",
    "    # Print the text and custom attribute data\n",
    "    print(doc.text, '\\n', \"— '{}' by {}\".format(doc._.book, doc._.author), '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: selective processing\n",
    "In this exercise, you'll use the nlp.make_doc and nlp.disable_pipes methods to only run selected components when processing a text. The small English model is already loaded in as the nlp object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Chick', '-', 'fil', '-', 'A', 'is', 'an', 'American', 'fast', 'food', 'restaurant', 'chain', 'headquartered', 'in', 'the', 'city', 'of', 'College', 'Park', ',', 'Georgia', ',', 'specializing', 'in', 'chicken', 'sandwiches', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"Chick-fil-A is an American fast food restaurant chain headquartered in the city of College Park, Georgia, specializing in chicken sandwiches.\"\n",
    "\n",
    "# Only tokenize the text\n",
    "doc = nlp.make_doc(text)\n",
    "\n",
    "print([token.text for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(American, College Park, Georgia)\n"
     ]
    }
   ],
   "source": [
    "text = \"Chick-fil-A is an American fast food restaurant chain headquartered in the city of College Park, Georgia, specializing in chicken sandwiches.\"\n",
    "\n",
    "# Disable the tagger and parser\n",
    "with nlp.disable_pipes('tagger', 'parser'):\n",
    "    # Process the text\n",
    "    doc = nlp(text)\n",
    "    # Print the entities in the doc\n",
    "    print(doc.ents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Training a neural network model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and update models\n",
    "#### Why updating the model?\n",
    "- Better resuls on your specific domain\n",
    "- Learn classification schemes specifically your problem\n",
    "- Essential for text classifiction\n",
    "- Very useful for named entity recognition\n",
    "- Less critical for part-of-speach tagging and dependency parsing\n",
    "\n",
    "#### How training works\n",
    "1. **Initialize** the model weights rendomly with ```nlp.begin_training```\n",
    "2. **Predict** a few examples with the current weights by calling ```nlp.update```\n",
    "3. **Compare** predictions with true labels\n",
    "4. **Calculate** how to change wiights to improve predictions\n",
    "5. **Update** weights slightly\n",
    "6. Go back t0 2\n",
    "\n",
    "![id](images/model_training.png \"How training works?\")\n",
    "\n",
    "- **Training data**: Examples and their annnotations.\n",
    "- **Text**: The input text the model should predict a label for.\n",
    "- **Label**: The label the model should predict.\n",
    "- **Gradient**: How to change the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Training the entity recognizer\n",
    "- The entity recognizer tags words and phrases in context\n",
    "- Each token can be only the part of one entity\n",
    "- Examples need to come with context\n",
    "```(\"iPhone X is coming\", {'entities': [(0, 8, 'GADGET')])```\n",
    "- Texts with no entities are also important\n",
    "```(\"I need a new phone! Any tips?\", {'entities': []})```\n",
    "- **Goal**: teach the model to generalize\n",
    "\n",
    "#### The training data\n",
    "- Examples of what we want the model to predict in context\n",
    "- Update an **existing model**: a few hundred to a few thousand examples\n",
    "- Train a **new category**: a few thousand to a million examples\n",
    "  - spaCy's English models: trained on 2 million words\n",
    "- Usually created manually by human annotators\n",
    "- Can be semi-automated - for example, using spaCy's ```Matcher```!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating training data\n",
    "\n",
    "spaCy's rule-based Matcher is a great way to quickly create training data for named entity models. A list of sentences is available as the variable TEXTS. You can print it the IPython shell to inspect it. We want to find all mentions of different iPhone models, so we can create training data to teach a model to recognize them as 'GADGET'.\n",
    "\n",
    "The nlp object has already been created for you and the Matcher is available as the variable matcher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('How to preorder the iPhone X', {'entities': [(20, 28, 'GADGET')]})\n",
      "('iPhone X is coming', {'entities': [(0, 8, 'GADGET')]})\n",
      "('Should I pay $1,000 for the iPhone X?', {'entities': [(28, 36, 'GADGET')]})\n",
      "('The iPhone 8 reviews are here', {'entities': [(4, 12, 'GADGET')]})\n",
      "('Your iPhone goes up to 11 today', {'entities': []})\n",
      "('I need a new phone! Any tips?', {'entities': []})\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "TEXTS = ['How to preorder the iPhone X',\n",
    " 'iPhone X is coming',\n",
    " 'Should I pay $1,000 for the iPhone X?',\n",
    " 'The iPhone 8 reviews are here',\n",
    " 'Your iPhone goes up to 11 today',\n",
    " 'I need a new phone! Any tips?']\n",
    "\n",
    "# Two tokens whose lowercase forms match 'iphone' and 'x'\n",
    "pattern1 = [{'LOWER': 'iphone'}, {'LOWER': 'x'}]\n",
    "\n",
    "# Token whose lowercase form matches 'iphone' and an optional digit\n",
    "pattern2 = [{'LOWER': 'iphone'}, {'IS_DIGIT': True, 'OP': '1'}]\n",
    "\n",
    "# Add patterns to the matcher\n",
    "matcher.add('GADGET', None, pattern1, pattern2)\n",
    "\n",
    "TRAINING_DATA = []\n",
    "\n",
    "# Create a Doc object for each text in TEXTS\n",
    "for doc in nlp.pipe(TEXTS):\n",
    "    # Match on the doc and create a list of matched spans\n",
    "    spans = [doc[start:end] for match_id, start, end in matcher(doc)]\n",
    "    # Get (start character, end character, label) tuples of matches\n",
    "    entities = [(span.start_char, span.end_char, 'GADGET') for span in spans]\n",
    "    \n",
    "    # Format the matches as a (doc.text, entities) tuple\n",
    "    training_example = (doc.text, {'entities': entities})\n",
    "    # Append the example to the training data\n",
    "    TRAINING_DATA.append(training_example)\n",
    "    \n",
    "print(*TRAINING_DATA, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The training loop\n",
    "#### The steps of a training loop\n",
    "- **Loop** for a number of times\n",
    "- **Shuffle** the training data\n",
    "- **Divide** the data into batches\n",
    "- **Update** the model for each batch\n",
    "\n",
    "#### Example loop"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "TRAINING_DATA = [\n",
    "    (\"How to preorder iPhone X\", {'entities': [(20, 28, 'GADGET')]})\n",
    "    # And many more examples ...\n",
    "]\n",
    "\n",
    "for i in range(10):\n",
    "    # Shuffle training data\n",
    "    random.shuffle(TRAINING_DATA)\n",
    "    \n",
    "    # Create batches and iterate over them\n",
    "    for batch in spacy.util.minibatch(TRAINING_DATA):\n",
    "        # Split the batch in texts and annotations\n",
    "        annotations = [annotation for text, annotation in batch]\n",
    "        \n",
    "        # Update the model\n",
    "        nlp.update(text)\n",
    "# Save the model\n",
    "nlp.to_disk(path_to_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update an existing model\n",
    "- Improve prediction on new data\n",
    "- Especiallly useful to improve existing categories, like `PERSON`\n",
    "- Also possible to add new categories\n",
    "- Be carefull and make sure the model doesn't \"forget\" the old ones\n",
    "\n",
    "#### Settings up a new pipeline from scratch\n",
    "Start from blank english model contains only a language data and tokinizations rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import random\n",
    "\n",
    "examples = [\n",
    "    (\"How to preorder iPhone X\", {'entities': [(20, 28, 'GADGET')]})\n",
    "    # And many more examples ...\n",
    "]\n",
    "\n",
    "# Start with blank English model\n",
    "nlp = spacy.blank('en')\n",
    "\n",
    "# Create blank entity recognizer and add it to the pipeline\n",
    "ner = nlp.create_pipe('ner')\n",
    "nlp.add_pipe(ner)\n",
    "\n",
    "# Add a new label\n",
    "ner.add_label('GADGET')\n",
    "\n",
    "# Start the training\n",
    "nlp.begin_training()\n",
    "\n",
    "# Train for 10 iterations\n",
    "for itn in range(10):\n",
    "    random.shuffle(examples)\n",
    "    \n",
    "    # Divide examples into batches\n",
    "    for batch in spacy.util.minibatch(examples, size=2):\n",
    "        texts = [text for text, annotations in batch]\n",
    "        annotations = [annotation for text, annotation in batch]\n",
    "        # Update the model\n",
    "        nlp.update(texts, annotations)\n",
    "# Save model to disk\n",
    "# nlp.to_disk(path_to_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Setting up the pipeline\n",
    "In this exercise, you'll prepare a spaCy pipeline to train the entity recognizer to recognize 'GADGET' entities in a text – for exampe, \"iPhone X\".\n",
    "\n",
    "spacy has already been imported for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Create a blank 'en' model\n",
    "nlp = spacy.blank('en')\n",
    "\n",
    "# Create a new entity recognizer and add it to the pipeline\n",
    "ner = nlp.create_pipe('ner')\n",
    "nlp.add_pipe(ner)\n",
    "\n",
    "# Add the label 'GADGET' to the entity recognizer\n",
    "ner.add_label('GADGET')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Building a training loop\n",
    "Let's write a simple training loop from scratch!\n",
    "\n",
    "The pipeline you've created in the previous exercise is available as the nlp object. It already contains the entity recognizer with the added label 'GADGET'.\n",
    "\n",
    "The small set of labelled examples that you've created previously is available as the global variable TRAINING_DATA. To see the examples, you can print them in your script or in the IPython shell. spacy and random have already been imported for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 10.833332419395447}\n",
      "{'ner': 23.59615170955658}\n",
      "{'ner': 33.35446310043335}\n",
      "{'ner': 6.554355084896088}\n",
      "{'ner': 14.063887894153595}\n",
      "{'ner': 19.46621137857437}\n",
      "{'ner': 2.6135728657245636}\n",
      "{'ner': 5.711166794411838}\n",
      "{'ner': 9.176530804252252}\n",
      "{'ner': 1.5220626677037217}\n",
      "{'ner': 3.885590207937639}\n",
      "{'ner': 5.911869316420052}\n",
      "{'ner': 4.600181766785681}\n",
      "{'ner': 8.52852916996926}\n",
      "{'ner': 9.536159337498248}\n",
      "{'ner': 3.132711953483522}\n",
      "{'ner': 4.775071158539504}\n",
      "{'ner': 5.988043637713417}\n",
      "{'ner': 1.3914463706314564}\n",
      "{'ner': 3.8246281114643352}\n",
      "{'ner': 4.771497246832951}\n",
      "{'ner': 0.6008064360357821}\n",
      "{'ner': 3.37357012436496}\n",
      "{'ner': 3.389595909142031}\n",
      "{'ner': 1.343033199444335}\n",
      "{'ner': 1.3456716429002427}\n",
      "{'ner': 1.3481531683949015}\n",
      "{'ner': 0.00028501169049377495}\n",
      "{'ner': 0.6737396408173026}\n",
      "{'ner': 0.6739416004238947}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "TRAINING_DATA = [('How to preorder the iPhone X', {'entities': [(20, 28, 'GADGET')]}),\n",
    " ('iPhone X is coming', {'entities': [(0, 8, 'GADGET')]}),\n",
    " ('Should I pay $1,000 for the iPhone X?', {'entities': [(28, 36, 'GADGET')]}),\n",
    " ('The iPhone 8 reviews are here', {'entities': [(4, 12, 'GADGET')]}),\n",
    " ('Your iPhone goes up to 11 today', {'entities': [(5, 11, 'GADGET')]}),\n",
    " ('I need a new phone! Any tips?', {'entities': []})]\n",
    "\n",
    "# Create a blank 'en' model\n",
    "nlp = spacy.blank('en')\n",
    "\n",
    "# Create a new entity recognizer and add it to the pipeline\n",
    "ner = nlp.create_pipe('ner')\n",
    "nlp.add_pipe(ner)\n",
    "\n",
    "# Add the label 'GADGET' to the entity recognizer\n",
    "ner.add_label('GADGET')\n",
    "\n",
    "# Start the training\n",
    "nlp.begin_training()\n",
    "\n",
    "# Loop for 10 iterations\n",
    "for itn in range(10):\n",
    "    # Shuffle the training data\n",
    "    random.shuffle(TRAINING_DATA)\n",
    "    losses = {}\n",
    "    # Batch the examples and iterate over them\n",
    "    for batch in spacy.util.minibatch(TRAINING_DATA, size=2):\n",
    "        texts = [text for text, annotation in batch]\n",
    "        annotations = [annotation for text, annotation in batch]\n",
    "        # Update the model\n",
    "        nlp.update(texts, annotations, losses=losses)\n",
    "        print(losses)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Exploring the model\n",
    "Let's see how the model performs on unseen data! To speed things up a little, here's a trained model for the label 'GADGET', using the examples from the previous exercise, plus a few hundred more. The loaded model is already available as the nlp object. A list of test texts is available as TEST_DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple is slowing down the iPhone 8 and iPhone X - how to stop it\n",
      "(iPhone 8, iPhone X) \n",
      "\n",
      "\n",
      "I finally understand what the iPhone X 'notch' is for\n",
      "(iPhone X,) \n",
      "\n",
      "\n",
      "Everything you need to know about the Samsung Galaxy S9\n",
      "() \n",
      "\n",
      "\n",
      "Looking to compare iPad models? Here’s how the 2018 lineup stacks up\n",
      "() \n",
      "\n",
      "\n",
      "The iPhone 8 and iPhone 8 Plus are smartphones designed, developed, and marketed by Apple\n",
      "(iPhone 8, iPhone 8) \n",
      "\n",
      "\n",
      "what is the cheapest ipad, especially ipad pro???\n",
      "() \n",
      "\n",
      "\n",
      "Samsung Galaxy is a series of mobile computing devices designed, manufactured and marketed by Samsung Electronics\n",
      "() \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run previous example before start this one\n",
    "TEST_DATA = ['Apple is slowing down the iPhone 8 and iPhone X - how to stop it',\n",
    " \"I finally understand what the iPhone X 'notch' is for\",\n",
    " 'Everything you need to know about the Samsung Galaxy S9',\n",
    " 'Looking to compare iPad models? Here’s how the 2018 lineup stacks up',\n",
    " 'The iPhone 8 and iPhone 8 Plus are smartphones designed, developed, and marketed by Apple',\n",
    " 'what is the cheapest ipad, especially ipad pro???',\n",
    " 'Samsung Galaxy is a series of mobile computing devices designed, manufactured and marketed by Samsung Electronics']\n",
    "\n",
    "for doc in nlp.pipe(TEST_DATA):\n",
    "    # Print the document text and entitites\n",
    "    print(doc.text)\n",
    "    print(doc.ents, '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training best preactices\n",
    "#### Problem 1: Models can \"forget\" things\n",
    "- Existing model can overfit on new data\n",
    "  - e.g.: if you only update it with `WEBSITE`, it can \"unlearn\" what a `PERSON` is\n",
    "- Also known as \"catastrophic forgetting\" problem\n",
    "\n",
    "#### Solution 1: Mix in previously correct predictions\n",
    "- For example, if you're training `WEBSITE`, also include examples of `PERSON`\n",
    "- Run existing spaCy model over data and extract all other relevant entities\n",
    "\n",
    "**BAD:**\n",
    "```\n",
    "TRAINING_DATA = [\n",
    "    ('Reddit is a webstite', {'entities': [(0, 6, 'WEBSITE')]})\n",
    "]\n",
    "```\n",
    "\n",
    "**GOOD:**\n",
    "```\n",
    "TRAINING_DATA = [\n",
    "    ('Reddit is a webstite', {'entities': [(0, 6, 'WEBSITE')]}),\n",
    "    ('Obama is a webstite', {'entities': [(0, 6, 'PERSON')]})\n",
    "]\n",
    "```\n",
    "\n",
    "#### Problem 2: Models can't learn everything\n",
    "- spaCy's models make predictions based on **local context**\n",
    "- Model can struggle to learn if decision is difficult to make based on context\n",
    "- Label scheme needs to be consistent and not too specific\n",
    "  - For example: `CLOTHING` is better than `ADULT_CLOTHING` and `CHILDRENS_CLOTHING`\n",
    "\n",
    "#### Solution 2: Plan your label scheme carefully\n",
    "- Pick categories that are reflected in local context\n",
    "- More generic is better than too specific\n",
    "- Use rules to go from generic labels to specific categories\n",
    "\n",
    "**BAD:**\n",
    "```\n",
    "LABELS = ['ADULT_SHOES', 'CHILDRENS_SHOES', 'BANDS_I_LIKE']\n",
    "```\n",
    "\n",
    "**GOOD:**\n",
    "```\n",
    "LABELS = ['CLOTHINGS', 'BAND']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Good data vs. bad data\n",
    "Here's an excerpt from a training set that labels the entity type TOURIST_DESTINATION in traveler reviews.\n",
    "\n",
    "**Solution**\n",
    "Fix TOURIST_DESTINATION with GPE\n",
    "Great work! Once the model achieves good results on detecting GPE entities in the traveler reviews, you could add a rule-based component to determine whether the entity is a tourist destination in this context. For example, you could resolve the entities types back to a knowledge base or look them up in a travel wiki.\n",
    "\n",
    "#### Training multiple labels\n",
    "Here's a small sample of a dataset created to train a new entity type WEBSITE. The original dataset contains a few thousand sentences. In this exercise, you'll be doing the labeling by hand. In real life, you probably want to automate this and use an annotation tool – for example, Brat, a popular open-source solution, or Prodigy, our own annotation tool that integrates with spaCy.\n",
    "\n",
    "After this exercise you will be nearly done with the course! If you enjoyed it, feel free to send Ines a thank you via Twitter - she'll appreciate it! Tweet to Ines\n",
    "\n",
    "Fixed by adding person labels in text\n",
    "```\n",
    "TRAINING_DATA = [\n",
    "    (\"Reddit partners with Patreon to help creators build communities\", \n",
    "     {'entities': [(0, 6, 'WEBSITE'), (21, 28, 'WEBSITE')]}),\n",
    "  \n",
    "    (\"PewDiePie smashes YouTube record\", \n",
    "     {'entities': [(0, 9, 'PERSON'), (18, 25, 'WEBSITE')]}),\n",
    "  \n",
    "    (\"Reddit founder Alexis Ohanian gave away two Metallica tickets to fans\", \n",
    "     {'entities': [(0, 6, 'WEBSITE'), (15, 29, 'PERSON')]}),\n",
    "    # And so on...\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapping up\n",
    "#### Your new spaCy skills\n",
    "- Extracting linguistic features: part-of-speach tags, dependencies. named entities\n",
    "- Work with pre-trained **statistical models**\n",
    "- Find words and phrases using `Matcher` and `PhraseMatcher` **match rules**\n",
    "- Best practices for working with **data structures** `Doc`, `Token`, `Span`, `Vocab`, `Lexeme`\n",
    "- Find **semantic similarity** using **word vectors**\n",
    "- Write custom **pipeline components** with **extension attributes**\n",
    "- **Scale up** your spaCy pipeline and make them fast\n",
    "- Create **training data** for spaCy' statistical models\n",
    "\n",
    "#### More things to do with spaCy\n",
    "- **Training and updating** other pipeline components\n",
    "  - Part-of-speach tagger\n",
    "  - Dependency parser\n",
    "  - Text classifier\n",
    "- **Customize the tokinizer**\n",
    "  - Adding rules and exceptions to split text differently\n",
    "- **Adding or improve support for other languages**\n",
    "  - 45+ lanuges currently\n",
    "  - Lots of room for improvement and more languages\n",
    "  - Allow training models for other languages\n",
    "\n",
    "For more examples use:\n",
    " [spacy.io](https://spacy.io)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
